{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ACGAN_task_handout_Ananya_Neogi.ipynb","provenance":[{"file_id":"1kzZdKbf-224zkBmOFLX77QuU86f2I5Jp","timestamp":1583484795079}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8aPeeOCJ6NKu"},"source":["# GAN - first tries\n","\n","We are implementing a simple feed-forward GAN architecture on MNIST - just to get the feeling right."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9ToOlMYj6NK7"},"source":["## Loading data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s4z2pdw06NK_","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"2f07d656-5d14-4bad-8d84-038e4be88bff","executionInfo":{"status":"ok","timestamp":1583500573446,"user_tz":-60,"elapsed":1710,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["from tensorflow.keras.datasets import mnist\n","import numpy as np"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W4lLQUX26NLS","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"688a54dc-5849-4584-b439-3bee3a17e3da","executionInfo":{"status":"ok","timestamp":1583500575302,"user_tz":-60,"elapsed":1462,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["data = mnist.load_data()\n","(train_X, train_y),(test_X,test_y) = data"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"POtCtKQm6NLf","colab":{}},"source":["# The more the merrier :-)\n","# Remember, this is unsupervised learning, so \"holdout\" and such makes less sense\n","X = np.concatenate((train_X,test_X),axis=0)\n","y = np.concatenate((train_y,test_y),axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K_0M86x06NLu","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7f6fb232-d261-4028-d3c7-42b1b16fefc9","executionInfo":{"status":"ok","timestamp":1583500577302,"user_tz":-60,"elapsed":561,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["X.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0Iakww8woGSQ","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"19c188c9-1c18-4160-c3a0-1f45a7a00dea","executionInfo":{"status":"ok","timestamp":1583500579645,"user_tz":-60,"elapsed":550,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["y.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70000,)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PJvkyD2J6NMD","colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"16261516-3a37-4100-b457-66891a31ee41","executionInfo":{"status":"ok","timestamp":1583501956977,"user_tz":-60,"elapsed":933,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["import matplotlib.pyplot as plt\n","\n","example_count = 5\n","\n","for ex in range(example_count):\n","    plt.subplot(5, example_count//5, ex+1)\n","    plt.imshow(train_X[ex], interpolation=\"nearest\", cmap=\"gray\")\n","    plt.axis(\"off\")\n","\n","plt.show()"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAADMAAADnCAYAAACzDm4TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 5 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BOYjr23B6NMP","colab":{}},"source":["# Normalization betwenn -1 and 1 !!!!\n","quasi_mean = X.max()/2 # Max is always 255, so this works ok.\n","X = (X.astype(np.float32)-quasi_mean)/quasi_mean"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yPqwfkZw6NMp","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4f458393-8d29-4419-a6cb-4f0b2cf45c69","executionInfo":{"status":"ok","timestamp":1583500584641,"user_tz":-60,"elapsed":521,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["# NOT Flattening of the image vectors!!!!\n","# This is a convolutional model, so it works well with 2D data\n","# X = X.reshape(X.shape[0],-1)\n","\n","# Instead, we add a new \"channel\" axis to the data. \n","# Since this is grayscale, that is only 1 channel.\n","\n","X = X[:,np.newaxis]\n","\n","X.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70000, 1, 28, 28)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HpZFqr6c6NM0"},"source":["## Parameters"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x675qgOy6NM4"},"source":["### Training parameters\n","\n","We will be forced to do manual batching here, so we have to calcculate the number of batches manually, and iterate on a per batch basis."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J7WqMOVw6NM8","colab":{}},"source":["EPOCHS = 30\n","BATCH_SIZE = 200\n","HALF_BATCH = BATCH_SIZE // 2\n","BATCH_NUM = (X.shape[0] // BATCH_SIZE)\n","if X.shape[0] % BATCH_SIZE:\n","    BATCH_NUM+=1\n","Z_DIM = 100\n","\n","NUM_CLASSES = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kpjkPEdS6NNU"},"source":["### Model parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"snAXlrvr6NNY","colab":{}},"source":["GENERATOR_INITIAL_IMAGE_SIZE = 7\n","GENERATOR_INITIAL_IMAGE_CHANNELS = 128\n","GENERATOR_L1_DIM = GENERATOR_INITIAL_IMAGE_SIZE*GENERATOR_INITIAL_IMAGE_SIZE*GENERATOR_INITIAL_IMAGE_CHANNELS \n","# eg. 7*7 image, 128 channels it will be, and we go DOWN with the channels from there\n","\n","# We have to tkae care, that the final shape of all generator convolutions results in 28*28*1, \n","# so it is a kind of balancing act\n","GENERATOR_L2_DIM = 64\n","GENERATOR_L2_KERNEL_SIZE = (5,5)\n","GENERATOR_OUTPUT_DIM = 1 # Nuber of output CHANNELS!!!!\n","GENERATOR_OUTPUT_KERNEL_SIZE = (5,5)\n","\n","GENERATOR_L3_DIM = 1024\n","\n","DISCRIMINATOR_L1_DIM = 64\n","DISCRIMINATOR_L1_KERNEL_SIZE = (5,5)\n","DISCRIMINATOR_L2_DIM = 128\n","DISCRIMINATOR_L2_KERNEL_SIZE = (5,5)\n","\n","LEAKY_ALPHA = 0.2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L7E4_OMC6NNj"},"source":["## Model building\n","\n","Please remember th ACGAN architecture:\n","\n","<img src=\"https://programming.vip/images/doc/7337a1370ac02af82912e32d852fabeb.jpg\" width=25%>\n","\n","Which is convolutional in this case, so DCGAN is relevant here, for the convolutions:\n","\n","<img src=\"https://miro.medium.com/max/760/1*B7y91tLgeWE-EuuFP-1XwA.png\" width=65%>\n","\n","As well as the calculation of the resulting image sizes in case of a convolution:\n","\n","<img src=\"https://qph.fs.quoracdn.net/main-qimg-d4023fe66cac95238a76ea1b5bc21d84\" wudth=45%>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dNWBYT8o6NNn","colab":{}},"source":["from tensorflow.keras.optimizers import Adam\n","\n","# Some empirically set values. \n","# It might well be worth experimenting with newer optimizers / settings\n","optimizer = Adam(lr=2e-4, beta_1=0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7FDJpwtE6NNw"},"source":["### Generator"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-0oh5Av66NNz","colab":{}},"source":["from tensorflow.keras.layers import Input, Dense, LeakyReLU, Conv2D, Flatten, Reshape\n","from tensorflow.keras.layers import BatchNormalization, UpSampling2D, Embedding, multiply\n","from tensorflow.keras.models import Model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X-TPOT6D6NN5","colab":{}},"source":["### Define the generator!\n","#########################\n","\n","# We use FUNCTIONAL API!\n","\n","# The generator always gets a noise vector as input\n","noise_input = Input(shape=(Z_DIM,))\n","# AND a real as a single value. (Please observe, even a single value should be an 1 long vector!)\n","label_input = Input(shape=(1,),dtype='int32')\n","\n","# Now we have to make sure, that the labels are embedded to a space which has the same dimensions as the noise input\n","# For this, ve use an embedding, that transforms the class input (which has a range defined in a comnstant above)\n","# to something, that matches the dimensions of the noise input.\n","embedded_label_input = Embedding()\n","# This is a technical step to ensure, it is a vector\n","embedded_label_input = Flatten()(embedded_label_input)\n","\n","# And then we simply multiply the noise with the embedded class \n","# For this, we already imported the function above.\n","# Please observe, that the function takes in a LIST of things\n","combined_input = ....\n","# Note, other operations can make sense here, it is an empirical thing to try...\n","\n","# Define the first layer of the fully connected network, without activation!\n","# Use the parameters defined with capital letter constants in the cells above for node counts!\n","g_layer_1 = ....\n","\n","# But we do have to ensure that the input for anything convolutional is 2D + channel, so reshape is in order\n","# remeber, it is image szie * image size * channels\n","# ant this is the initial \"image\"\n","# USE THE DEFINED CONSTANTS FROM ABOVE, and define a reshape layer!\n","reshaped_layer = ....\n","\n","# Define a non-linearity, namely leaky relu on this layer!\n","# We use LeakyReLU for avoiding sparsity - other options are viable also, just not normal relu\n","# use the alpha value defined in constants above!\n","g_layer_1_nonlin = ....\n","\n","# For stability, we add a batch normalization layer - no extra settings.\n","g_layer_1_batchnorm = ....\n","\n","# Now we use Upsampling to gradually get the image resolution up, by doubling\n","# upsampling layer does this without any extra parameters.\n","g_layer_2_upsample = ....\n","# As a result we have 14*14*128 - with the above default settings\n","\n","# Furthermore we use convolutions to get the number of channels down\n","# Define a convolutional layer to get down to the layer 2 dimension of generator (number of filters)\n","# Use the defined constant from above for layer 2 kernel size, and use _\"same\"_ padding\n","g_layer_2_conv = ....\n","# As a result we get 14*14*64 - with the above default settings\n","\n","# And again a non linearity as above, please...\n","g_layer_2_nonlin = ....\n","# And a batch normalization, as above, please...\n","g_layer_2_batchnorm = ....\n","\n","# Now we again use Upsampling to gradually get the image resolution up, by doubling\n","g_layer_3_upsample = ....\n","# As a result we have 28*28*64 - with the above default settings\n","\n","# Furthermore we use Conv2D to get the number of channels down\n","# Define a convolutional layer to get down to the output layer dimension of generator (number of filters)\n","# Use the defined constant from above for output layer kernel size, and use _\"same\"_ padding\n","# USE A NON-LINEARITY TO PROJECT BETWEEN -1 and 1!!! \n","# Remember, the images are normalized!\n","g_output_layer = ....\n","# As a result we get 28*28*1 - with the above default settings\n","# And please notice, that we used an activation, so our pixels get between -1 and 1 again\n","# This should now be indistinguishable from an input image - hopefully\n","\n","# Please instantiate the model!\n","generator = ....\n","\n","# Please remeber, that the loss for the discriminator will be a binary loss, so this applies here also\n","# Use the appropriate loss measure!\n","generator.compile(loss=...., optimizer=optimizer)\n","# Think about this carefully, please!"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jDS7ZWQs6NOA"},"source":["### Discriminator"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0BsR4uc-6NOB","colab":{}},"source":["### Define the discriminator!\n","#############################\n","\n","# We still use FUNCTIONAL API!\n","\n","# The discriminator always gets original sized images, so 28x28x1 (channel) as inputs\n","image_input = ....\n","\n","# Define the first convolutional layer of the discriminator!\n","# Use the above defined constants for  filter number and kernel size!\n","# We use strides of 2,2 instead of pooling, which is a sparse operator, and _\"same\"_ padding\n","d_layer_1 = ....\n","# And add a non-linearity, as in the discriminator, please...\n","d_layer_1_nonlin = ....\n","\n","# Repet the block again, please!\n","d_layer_2 = ....\n","d_layer_2_nonlin = ....\n","\n","# Please use an appropriate operation \n","# to make the output of the previos conv compatible with a fully connected layer!\n","d_layer_2_flattened = ....\n","\n","# Please implement ONE OF the output layers of the discriminator!\n","# One output of the discriminator is a single binary decision, \n","# so one use an appropriate activation and dimensionality!\n","validity = \n","\n","# Please implement THE OTHER output layer of the discriminator!\n","# The other output of the discriminator is the predicted class of the input (+1 for the fake class!!!!)\n","# Please observe, thisis a multiclass classification! Use appropriate nonlinearity!\n","predicted_class = ....\n","\n","# Please instantiate the model!\n","discriminator = Model(inputs=...., outputs=[....])\n","# Observe syntax for multi output!!!\n","\n","# Please remeber, that ONE loss for the discriminator will be a binary loss (validity)\n","# And the other will be something for the multi-class classification (including +1 class for fake)\n","# This is NOT one hot encoded class label, so only one kind of builtin lossfunction works here!\n","# Think and ask, if unsure!\n","discriminator.compile(loss=[....], optimizer=optimizer)\n","# Observe syntax for multi output!!!"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j0ZgKpmC6NOL"},"source":["### Joint model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nhgWdhGl6NON","colab":{}},"source":["### Define the GAN itself!\n","##########################\n","\n","\n","# STOP!!!!!!\n","# This is a crucial line, since in the joint model, discriminator will be frozen, so no weight update!\n","discriminator.trainable = False\n","\n","....\n","# Remove this .... if you understand, why the above line is here!\n","\n","# What is the input for the whole GAN?\n","# For one, there is noise:\n","noise_input = ....\n","# And for two: there is the class label, as a single number (vector)\n","label_input = ....\n","\n","# Use the generator as a function on the input!\n","# Please remeber, there is a list of TWO things that has to go into the generator!\n","generated_image = ....\n","\n","# Use the discriminator as a function on the fake images!\n","validity, target_label = ....\n","# Observe, that two things come out of the discriminator!\n","\n","# Instantiate the joint model, appropriate inputS and outputS! (Plural is not an accident! :-)\n","# Use lists!\n","joint_model = ....\n","\n","# Please think about, why need TWO losses!\n","joint_model.compile(loss=[....], optimizer=optimizer)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0bxzVNkh6NOb"},"source":["## Helper functions for visualization"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ickSx5Ed6NOd","colab":{}},"source":["# Nothing to see here! :-P\n","\n","from matplotlib import pyplot as plt\n","import matplotlib.image as mpimg\n","\n","\n","def get_example_images(epoch, example_count=25, num_classes=10):\n","    input_noise = np.random.normal(0,1, size=(example_count,Z_DIM))\n","    input_classes = []\n","    class_num = 0\n","    while len(input_classes)<example_count:\n","        input_classes.append(class_num)\n","        class_num+=1\n","        if class_num > num_classes:\n","            class_num = 0\n","    input_classes=np.array(input_classes).reshape(-1,1)\n","    \n","    generated_images = generator.predict([input_noise,input_classes])\n","    generated_images = generated_images.reshape(example_count, 28, 28) #,_\n","    \n","    plt.figure(figsize = (5, example_count // 5))\n","    for ex in range(example_count):\n","        plt.subplot(5, example_count//5, ex+1)\n","        plt.imshow(generated_images[ex], interpolation=\"nearest\", cmap=\"gray\")\n","        plt.axis(\"off\")\n","    plt.tight_layout()\n","    plt.savefig(\"ACGAN_example_images_epoch_num_{0}.png\".format(epoch))\n","\n","def show_image_for_epoch(epoch):\n","    imgname = \"ACGAN_example_images_epoch_num_\"+str(epoch)+\".png\"\n","    img = mpimg.imread(imgname)\n","    imgplot = plt.imshow(img)\n","    plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1tfFKGOR6NOn"},"source":["## Training\n","\n","Sadly, we can not use simple `fit()`, but have to construct the main training loop ourselves."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Wt2FjLP86NOq","colab":{}},"source":["# To see some progress, we use tqdm as a progress bar\n","from tqdm import tqdm\n","\n","# Since we do NOT use fit\n","# sadly, we have to do this ourselves manually\n","history = {\"discriminator\":[],\"generator\":[]}\n","\n","# Main trainign loop\n","for epoch_num in range(EPOCHS):\n","    epoch_discriminator_loss = 0\n","    epoch_generator_loss = 0\n","    \n","    for batch in tqdm(range(BATCH_NUM)):\n","        # We select randomly a half batch amount of real images from MNIST\n","        # Use Numpy to choose, no replacement!\n","        # To ensure, that we have image and class PAIRS, and don't mess up, we use a binary mask.\n","        # Use choice from numpy to generate a mask for the whole dataset length!\n","        choice_mask = ....\n","        real_images = X[choice_mask]\n","        real_labels = y[choice_mask]\n","        \n","        \n","        # We generate a half batch amount of fake images\n","        # By first generating a half batch worth of Gaussian noise with zero mean, unit variance\n","        # and appropriate noise dimensions\n","        input_noise = ....\n","        \n","        # And we don't forget to generate the fake labels for them!\n","        # Which are random integers between 0 and the class number, reshaped to be 1D matrices\n","        generator_labels = ....\n","        \n","        # And then using the fixed generator, to output some images from it\n","        # Using the predict method of the generator!\n","        # Warning, the generator needs a list of two things!\n","        generated_images = ....\n","        \n","        ....\n","        # STOP, and thik through, WHY predict?!\n","        # Then you can remove the ....\n","      \n","        # We generate our \"validity\" class\n","        # Remember one sided label smoothing for the positive class!\n","        # Let's say with 0.9...\n","        # So please, generate a half batch sized, one dimensional matrix with ones, using numpy\n","        # and multiuply it by 0.9\n","        real_validity = ....\n","        # And generate a half batch worth of zeroes, again one dimensional matrix\n","        generated_validity = ....\n","\n","        # And we are making available a vector, with the new \"fake\" class for the discriminator to find\n","        # Typically, if we had classes 0-9, we use 10 as the new class label that the discriminator has to find.\n","        # Happens to be, that NUM_CLASSES is 10 (0-9), so we can use it as the 11th class label\n","        # Practically: generate a half batch of ones, reshape it to be 1D matrix \n","        # and multiply by the number of clases\n","        labeled_as_fake = \n","        \n","        ### Do the actual training!\n","        \n","        # First for the discriminator on the real data\n","        discriminator_loss_real = discriminator.train_on_batch(real_images, [real_validity, real_labels])\n","        \n","        # Then on the fake data\n","        discriminator_loss_generated = discriminator.train_on_batch(generated_images, [generated_validity, labeled_as_fake])\n","        \n","        # We have now two losses for real and two for fake\n","        # We take the mean of them se two again\n","        discriminator_loss = 0.5 * (np.array(discriminator_loss_real) + np.array(discriminator_loss_generated))\n","        epoch_discriminator_loss += np.mean(discriminator_loss)\n","        \n","        ### We then update the generator\n","        # We use the discriminator that was trained a line above, and is frozen, as defined in the joint model\n","        \n","        # Please generate a new set of input noise, notice, it is a full batch!\n","        # Again, using numpy, normal distribution, zero mean, unit variance\n","        new_input_noise = ....\n","        \n","        # And we generate the new random labels for the generator again\n","        # Between 0 and number of classes, a full batch worth, reshaped as an 1D matrix\n","        generator_labels = ....\n"," \n","        # We try to convince the discriminator, that this is real data - which is not\n","        # So please generate a batch worth of one dimensional matrix filled with ones \n","        convincing_y = .... \n","        # Notice, no label smoothing!\n","\n","        # Remember, the joint model takes in noise plus target labels, \n","        # does the generation, the discrimination, whereby outputting validity and a predicted label \n","        # (10, if it finds out, that the sample is fake) then computes loss\n","        # But the discriminator is frozen, so only the generator will get updated\n","        # It is \"successful\" if the discriminator predicts \"real\" - hence the convincing_y\n","        # and some label differnt from 10 - ideally the one we gave in as input for the generator\n","        generator_loss = joint_model.train_on_batch([new_input_noise, generator_labels], [convincing_y, generator_labels])\n","        # Same as above, we take the mean of the losses\n","        epoch_generator_loss += np.mean(generator_loss)\n","        \n","    # Loss printout in every epoch, averaged over the batches\n","    print(\"Epoch number:\",epoch_num,\"discriminator_loss:\",epoch_discriminator_loss / BATCH_NUM, \"generator_loss:\", epoch_generator_loss / BATCH_NUM)\n","    \n","    # Save it for the future\n","    history[\"discriminator\"].append(epoch_discriminator_loss / BATCH_NUM)\n","    history[\"generator\"].append(epoch_generator_loss / BATCH_NUM)\n","    \n","    #Save model - optional\n","    #generator.save(\"ACGAN_generator.h5\")\n","    \n","    #Save images\n","    get_example_images(epoch_num)\n","    \n","    # Show epoch example\n","    show_image_for_epoch(epoch_num)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qo7-eaJf6NO0"},"source":["## Visualization of training progress"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pkY1ixWj6NO2","colab":{}},"source":["plt.plot(history[\"discriminator\"], color='blue', linewidth=2, label=\"Discriminator\")\n","plt.plot(history[\"generator\"],  color='red', linewidth=2, label=\"Generator\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend();\n"],"execution_count":0,"outputs":[]}]}