{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Assignment_1_handout_Ananya_Neogi.ipynb","provenance":[{"file_id":"1DIsX4IINJ-2gTBJq2PAtY1UPNmT52EzW","timestamp":1583404692803}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"tDEnfPD1LOed","colab_type":"text"},"source":["# Importing libraries"]},{"cell_type":"code","metadata":{"id":"8mVRUQreMkgc","colab_type":"code","colab":{}},"source":["%load_ext rpy2.ipython"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gGjI1cMNsCX","colab_type":"code","outputId":"12dafd66-60d6-4fea-bc6e-10563e688c2e","executionInfo":{"status":"ok","timestamp":1584385897002,"user_tz":-60,"elapsed":182448,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%R\n","install.packages(\"grf\")\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Installing package into ‘/usr/local/lib/R/site-library’\n","(as ‘lib’ is unspecified)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: also installing the dependencies ‘zoo’, ‘DiceKriging’, ‘lmtest’, ‘sandwich’, ‘RcppEigen’\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/zoo_1.8-7.tar.gz'\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Content type 'application/x-gzip'\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 861309 bytes (841 KB)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: =\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 841 KB\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/DiceKriging_1.5.6.tar.gz'\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 95925 bytes (93 KB)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 93 KB\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/lmtest_0.9-37.tar.gz'\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 184426 bytes (180 KB)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 180 KB\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/sandwich_2.5-1.tar.gz'\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 1349536 bytes (1.3 MB)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 1.3 MB\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/RcppEigen_0.3.3.7.0.tar.gz'\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 1643103 bytes (1.6 MB)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 1.6 MB\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/grf_1.1.0.tar.gz'\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 129152 bytes (126 KB)\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 126 KB\n","\n","\n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n","  warnings.warn(x, RRuntimeWarning)\n","/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: The downloaded source packages are in\n","\t‘/tmp/RtmpW9epD2/downloaded_packages’\n","  warnings.warn(x, RRuntimeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"M2WB2i7qbusi","colab_type":"code","outputId":"bf466174-1750-4cce-b360-ad1d93686c5d","executionInfo":{"status":"ok","timestamp":1584385915575,"user_tz":-60,"elapsed":201010,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install justcause"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting justcause\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/ec/767d453b101a39a719167408e5687863fde44cee57b1a844b4ee157f707d/JustCause-0.3.2-py2.py3-none-any.whl (50kB)\n","\r\u001b[K     |██████▌                         | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 518kB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause) (0.25.3)\n","Collecting pygam\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/be/775033ef08a8945bec6ad7973b161ca909f852442e0d7cfb8d1a214de1ac/pygam-0.8.0-py2.py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause) (1.17.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause) (0.22.1)\n","Requirement already satisfied: rpy2 in /usr/local/lib/python3.6/dist-packages (from justcause) (2.9.5)\n","Collecting causalml\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/ac/663245906867cb41e8f246efc27e8fa1b11e4a411bc32c9bb7ef8c6dcc55/causalml-0.7.0.tar.gz (3.5MB)\n","\u001b[K     |████████████████████████████████| 3.5MB 49.6MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from justcause) (0.14.1)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause) (2.6.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause) (2018.9)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from pygam->justcause) (3.38.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pygam->justcause) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pygam->justcause) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->justcause) (0.14.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2->justcause) (2.11.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rpy2->justcause) (1.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (45.2.0)\n","Requirement already satisfied: pip>=10.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (19.3.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (3.1.3)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (0.10.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (0.10.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (0.29.15)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (0.90)\n","Requirement already satisfied: pydotplus in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (4.28.1)\n","Collecting shap\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/77/b504e43e21a2ba543a1ac4696718beb500cfa708af2fb57cb54ce299045c/shap-0.35.0.tar.gz (273kB)\n","\u001b[K     |████████████████████████████████| 276kB 59.5MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (0.3.1.1)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (2.2.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (20.1)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (2.2.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from causalml->justcause) (1.15.0)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam->justcause) (2.3.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2->justcause) (1.1.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause) (2.4.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause) (0.10.0)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->causalml->justcause) (0.5.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause) (1.0.8)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (0.9.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (3.1.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (1.15.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (1.27.1)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (0.2.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (0.34.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (0.1.8)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (3.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (1.1.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (1.11.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->causalml->justcause) (0.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->causalml->justcause) (3.2.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->causalml->justcause) (1.0.0)\n","Building wheels for collected packages: causalml, shap\n","  Building wheel for causalml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for causalml: filename=causalml-0.7.0-cp36-cp36m-linux_x86_64.whl size=471483 sha256=0f17787516727440541fe91a98f23e1a72e32cf5f92b0c5eb9057d3ced7a3b70\n","  Stored in directory: /root/.cache/pip/wheels/9a/b4/77/8ab9acc1734668cc9a607b8078702fa213bd138097e5cbc7b2\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.35.0-cp36-cp36m-linux_x86_64.whl size=394153 sha256=1849b1a0f9c5e980950dbb26c664fbe969761d530ee66f9986ddbcf3ea334119\n","  Stored in directory: /root/.cache/pip/wheels/e7/f7/0f/b57055080cf8894906b3bd3616d2fc2bfd0b12d5161bcb24ac\n","Successfully built causalml shap\n","Installing collected packages: pygam, shap, causalml, justcause\n","Successfully installed causalml-0.7.0 justcause-0.3.2 pygam-0.8.0 shap-0.35.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lPzAPlfebvbX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4UpBS6mNOCW","colab_type":"code","colab":{}},"source":["from justcause.data import Col\n","from justcause.data.sets import load_ihdp\n","from justcause.metrics import pehe_score, mean_absolute\n","from justcause.evaluation import calc_scores, summarize_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CfvwSmoLOe-","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","\n","%autoreload 2\n","\n","# Loading all required packages \n","import itertools\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jufTu9k5LOfL","colab_type":"text"},"source":["#### Infant Health Development Program Data-Set used in this exercise\n","\n","- Original study constructed to study the effect of special child care for low birthweight, premature infants.\n","- In total, six continuous and 19 binary pretreatment variables\n","- Using the covariates of all instances in both treatment groups, the potential outcomes are generated synthetically\n","- Finally, manipulation of observational study by omitting a non-random set of samples from the treatment group.\n","- The way the subset is generated from the experimental data does not ensure complete overlap - latent confounder\n","- Specifically, the observational subset is created by throwing away the set of all children with nonwhite mothers from the treatment group\n","- Following data generation process used for potentia outcomes\n","- After the adaptions from Hill, we are left with 139 instances in the treated group and 608 instances in the control group."]},{"cell_type":"markdown","metadata":{"id":"yLQd0vO_LOfP","colab_type":"text"},"source":["# 1. Running the causal models"]},{"cell_type":"code","metadata":{"id":"yg5b4PoxLOfU","colab_type":"code","colab":{}},"source":["# We Import the IHDP data-set \n","# There are 1000 replications in this data-set, each with a different individual treament effect\n","# produced from an underlying generative function. \n","# Check out https://justcause.readthedocs.io/en/latest/\n","\n","\n","# We load 100 of the 1000 data-sets\n","replications = load_ihdp(select_rep=np.arange(100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyTH9JV_LOfh","colab_type":"code","colab":{}},"source":["# Defining global parameters\n","train_size = 0.8        # Size of the training data-set \n","random_state = 42        # Setting the random state\n","\n","n= 0       # number of the data-sets we look at \n","\n","metrics = [pehe_score, mean_absolute]    ## Defining the metrics that will be calculated below"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQWUbE6HLOfw","colab_type":"text"},"source":["## 1.1 S-Learner Linear Regression"]},{"cell_type":"code","metadata":{"id":"rT9UjEovLOf0","colab_type":"code","colab":{}},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import SLearner\n","\n","\n","# Defining the S-Learner function that returns the ITE\n","# We define a function that takes the data, splits it up and returns individual treatment effect accuracies for the train and the test data-set\n","# The function takes each, the train and test data separately and selects the relevant variales and coverts them into np arrays\n","# The relevant variables have the followings names: x (the covariates), t (the treatment), y (the outcome)\n","# Note that the treatment needs to be explicityl defined\n","\n","\n","\n","def basic_slearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    slearner = model     # Select linear regression as a method to find the ITE for the S-Learner\n","    slearner.fit(train_X, train_t, train_y)      # Fitting the s-learner with linear regression\n","    return (\n","        slearner.predict_ite(train_X, train_t, train_y),   # Returning the predicting values for ITE for train\n","        slearner.predict_ite(test_X, test_t, test_y)       # Returning the prediction values for ITE for test\n","    )\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxwuoQxwLOgD","colab_type":"code","colab":{}},"source":["results_df = list()     # We define the list that contains the results\n","test_scores = list()    # Storing the test scores in a list\n","train_scores = list()   # Storing the train scores in a list\n","\n","\n","# Here we define the model that is going to be used for the S-learner\n","# Please instantiate linear regression for the simple learner\n","\n","\n","##-----------------Question------------------###\n","# Pass a LinearRegression Model into the S-Learner\n","# No particular parameter-settings necessary\n","from sklearn.linear_model import LinearRegression\n","\n","lreg =  LinearRegression()\n","\n","model=SLearner(lreg)\n","\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_slearner(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'S-Learner LR', 'train': True})\n","test_result.update({'method': 'S-Learner LR', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Gb37xoDuLOgQ","colab_type":"code","outputId":"e38de103-7361-4da9-de50-66c70ab0f1bd","executionInfo":{"status":"ok","timestamp":1584385919364,"user_tz":-60,"elapsed":204775,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_S_learner_LR=pd.DataFrame([train_result, test_result])\n","df_S_learner_LR"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.633660</td>\n","      <td>2.623297</td>\n","      <td>8.362125</td>\n","      <td>0.732443</td>\n","      <td>0.238185</td>\n","      <td>1.493276</td>\n","      <td>S-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.625971</td>\n","      <td>2.635993</td>\n","      <td>8.213626</td>\n","      <td>1.292668</td>\n","      <td>0.396246</td>\n","      <td>2.474603</td>\n","      <td>S-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         5.633660           2.623297  ...  S-Learner LR   True\n","1         5.625971           2.635993  ...  S-Learner LR  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"yfhFVRd4LOgk","colab_type":"text"},"source":["### 1.1.1 S-Learner Visualization"]},{"cell_type":"code","metadata":{"id":"xbbouLxaLOgo","colab_type":"code","colab":{}},"source":["# We run the same analysis again but only on an indvidual run of the data\n","# The reason is that the data generating process is varied every time,... \n","# ...so we can only look at individual runs of the ITE effect\n","results_df = list()    # We define the list that contains the results\n","test_scores = list()   # Storing the test scores in a list\n","train_scores = list()  # Storing the train scores in a list\n","\n","\n","#for rep in replications:\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state     # Use train_test_split  to split the data-set (replications[n]) \n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = basic_slearner(train, test, model)         # using the pre-defined basic learner function to retunr train, test\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))    # Using the just cause API to calcualte the scores from the estimate ITE for the training data\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))     # Using the just cause API to calcualte the scores from the estimate ITE for the test data\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)   #summary of the scores \n","train_result.update({'method': 'S-Learner LR', 'train': True})\n","test_result.update({'method': 'S-Learner LR', 'train': False})\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AsxWY_9mLOg2","colab_type":"code","outputId":"a1deff3a-9e00-47b7-827d-2145184301dc","executionInfo":{"status":"ok","timestamp":1584385919749,"user_tz":-60,"elapsed":205149,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["# Importing Matplotlib \n","import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()\n","\n","# If the treatment effect is perfectly represented there should be a 45 degree line!"],"execution_count":12,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df5xWdZ3//8eTYYYZBAaQ0RAExNaM\n1NAmw/qYRmu5u2WK7mdry8228rufst0y+mF128o0rXW3Nm23NTV1t/xYRq760dQUNSvUQRHxB65K\nomiKCQwIDMPM6/vHeV/D5cXMNeeCuWauYZ732+26ca73eb/P+3UOcF7XOe/zQxGBmZlZXqOGOgAz\nMxtenDjMzKwiThxmZlYRJw4zM6uIE4eZmVXEicPMzCrixGFWRNI5kl6S9IehjmU4knSSpGckbZJ0\nuKTXSVomaaOkvx/q+GxgOHFYxST9L0m/lbRB0suSfiPpzbu5zNMk3V1Sdrmkc3Yv2opimAF8FpgT\nEa/pZf6xkp6tcgyDus4lfc+SFJJG78ZiLgDOiIhxEfEA8HlgcUSMj4jv7UZsd0j62G7EZQPIicMq\nImkCcANwITAZmAZ8HegYyrh6sws7wBnAHyPixUHsc08zE3i4zHfbE0SEP/7k/gCtwPp+6nwceBTY\nCDwCHJHKvwg8WVR+Uip/PbAV6AI2AeuB04FOYFsquz7V3Q/4ObAWWAX8fVG/XwOuAf4LaAc+1kts\nzcCVqf3TwFfIfkD9KbAF6E79XV7Sbq+S+ZtSLDv1mZZXWNc/Aj8FJhct62fAH4ANwF3AG1J5X+v8\ne+BzwHLgFeBSYF/gprQtfwVMKlr+POC3aTs+CBxbNO8O4BvAb1LbW4Apad5qIIrW76hetl+v6waM\nSW0ixfgkcHv6O92a5h2U6l2Q+noB+AHQVLT89wHL0rZ8EjgeOLdkORcBAr4DvJjqPgQcMtT/P0bK\nZ8gD8Gd4fYAJaYdxBfBnxTusNP8vgTXAm9N/7tcCM4vm7Zd2Pn+VdjBT07zTgLtLlnU5cE7R91HA\nUuAfgQZgNvAU8O40/2tpx3tiqtvUS/xXAv8NjAdmAY8DH03zjgWeLbPuO83vrU/gH4AlwPS0o/wP\n4KqiNn+b+h8DfBdY1tc6p7Lfp+XtS3aE9yJwP3A40Jh20F9Ndaelv58/T/Ecl763pPl3pB3yQSnW\nO4Dz07xZZDv+0WW2QX/rFsBri77fQVECJ9vZX0eWbMYD1wPnpXlHkiXT41Ls04CD+1jOu9O/hYlk\n/85eT/q35E/1Pz5VZRWJiHbgf5HtIH4IrJV0naR9U5WPAd+OiPsi80REPJ3a/iwinouI7oi4Gvgf\nsp1FXm8m2wGeHRHbIuKpFMP7i+r8LiKuTX1sKW4sqS7VPSsiNkbE74F/Bk6tdDuUKO3z74AvR8Sz\nEdFBllxOKZzGiojLUv+FeW+U1NxPHxdGxAsRsQb4NXBPRDwQEVuBX5AlEYAPATdGxI0pnluBNrJE\nUvCjiHg8xfpTYG4F61p23cqRJLKjqs9ExMsRsRH4Jjv+/j4KXBYRt6bY10TEY30srpMs8RwMKCIe\njYjnK1gP2w0j/Xys7YKIeJTsCAFJB5Odpvku8AFgf7JftDuR9DfAmWS/bAHGAVMq6HomsJ+k9UVl\ndWQ70oJnyrSfAtSTnaIqeJrsl+3uKO1zJvALSd1FZV3AvulqrXPJjr5ayE59FWLbUKaPF4qmt/Ty\nfVxR338p6b1F8+uBxUXfi68Y21zUNo8+143sSLOcFmAssDTLIUB2tFCXpvcHbswTRETcLuki4PvA\nTEmLgIXph41VmY84bLekX4SXA4ekomeAA0vrSZpJdnRwBrB3REwEVpDtOCA7gtlp8SXfnwFWRcTE\nos/4iPjzMm2KvUT2S3VmUdkM+t/h9bfs3uL8s5I4G9PRwl+Tncf/U7LxllmpTbntUIlngP8s6Xuv\niDg/R9s8fZdbt/68RJbk3lDUtjkiComr1387fcUWEd+LiDcBc8hOvX0uRww2AJw4rCKSDpb0WUnT\n0/f9yY40lqQqlwALJb1JmdempLEX2X/+tandR9iRbCD7BT1dUkNJ2eyi7/cCGyV9QVKTpDpJh+S9\nFDgiushOzZwraXyK60yyI6Y8XgD2znFa6Qepj5kAklokvS/NG092BdofyX59f7OXPmaz6/4LeK+k\nd6ft05guI56eo+1asiOgcv2XW7eyIqKb7MfDdyTtk9pPk/TuVOVS4COS3ilpVJp3cJr3qu0i6c2S\n3iKpnmysbCs7jt6sypw4rFIbgbcA90h6hSxhrCC7/4GI+BnZqZifpLrXkl1R9AjZeMLvyHYCh5Jd\n2VNwO9llm3+Q9FIquxSYI2m9pGvTjv89ZOfkV5H9gr2E7Jd7Xp8i29E8Bdyd4rwsT8N0dHUV8FSK\nab8+qv4r2QDwLZI2km2jt6R5V5KdHltDdmXZkpK2r1rn3Gu1I8ZnyI5ovkSWCJ4h+yXe7//1iNhM\n9nf3m9T/vArXLY8vAE8ASyS1k10R9rrU/73AR8gG0DcAd7Lj6PBfycZS1kn6HtlFGj8E1pFtzz8C\n/1RBHLYbFOEXOZmZWX4+4jAzs4o4cZiZWUWcOMzMrCJOHGZmVpERcQPglClTYtasWUMdhpnZsLJ0\n6dKXIqKltHxEJI5Zs2bR1tY21GGYmQ0rkp7urdynqszMrCJOHGZmVhEnDjMzq4gTh5mZVcSJw8zM\nKuLEYWZmFXHiMDOzijhxmJlZRZw4zMysIk4cZmZWEScOMzOriBOHmZlVxInDzMwq4sRhZmYVceIw\nM7OKOHGYmVlFnDjMzKwiThxmZlYRJw4zM6uIE4eZmVWkaolDUqOkeyU9KOlhSV/vpc5MSbdJWi7p\nDknTi+Z9S9KK9PmrovIDJN0j6QlJV0tqqNY6mJnZzqp5xNEBzI+INwJzgeMlzSupcwFwZUQcBpwN\nnAcg6S+AI1K7twALJU1Ibb4FfCciXgusAz5axXUwM7MSVUsckdmUvtanT5RUmwPcnqYXA+8rKr8r\nIrZHxCvAcrLEI2A+cE2qdwVwYpVWwczMelHVMQ5JdZKWAS8Ct0bEPSVVHgQWpOmTgPGS9k7lx0sa\nK2kK8A5gf2BvYH1EbE9tngWm9dH36ZLaJLWtXbt2YFfMzGwEq2riiIiuiJgLTAeOlHRISZWFwDGS\nHgCOAdYAXRFxC3Aj8FvgKuB3QFeFfV8cEa0R0drS0rK7q2JmZsmgXFUVEevJTkUdX1L+XEQsiIjD\ngS8X1SUizo2IuRFxHCDgceCPwERJo9MippMlGzMzGyTVvKqqRdLENN0EHAc8VlJniqRCDGcBl6Xy\nunTKCkmHAYcBt0REkCWgU1KbDwP/Xa11MDOznVXziGMqsFjScuA+sjGOGySdLemEVOdYYKWkx4F9\ngXNTeT3wa0mPABcDHyoa1/gCcKakJ8jGPC6t4jqYmVkJZT/i92ytra3R1tY21GGYmQ0rkpZGRGtp\nue8cNzOzijhxmJlZRZw4zMysIk4cZmZWEScOMzOriBOHmZlVxInDzMwq4sRhZmYVceIwM7OKOHGY\nmVlFnDjMzKwiThxmZlaRfhOHpLflKTMzs5EhzxHHhTnLzMxsBBjd1wxJRwFvBVoknVk0awJQV+3A\nzMysNvWZOIAGYFyqM76ovJ0db+AzM7MRps/EERF3AndKujwinh7EmMzMrIblGeO4pPDucABJkyTd\nXMWYzMyshuVJHFMiYn3hS0SsA/apXkhmZlbL8iSObkkzCl8kzQT2/BeVm5lZr8oNjhd8Gbhb0p2A\ngKOB06salZmZ1ax+E0dE/FLSEcC8VPTpiHipumGZmVmtynPnuIDjgSMi4gZgrKQjqx6ZmZnVpDxj\nHP8GHAV8IH3fCHy/ahGZmVlNyzPG8ZaIOELSA5BdVSWpocpxmZlZjcpzxNEpqY50JZWkFqC7qlGZ\nmVnNypM4vgf8AthH0rnA3cA3+2skqVHSvZIelPSwpK/3UmempNskLZd0h6TpRfO+ndo9Kul7aayF\nVG+lpGXp43tKzMwGUbmHHB4QEasi4seSlgLvJLsc98SIeDTHsjuA+RGxSVI92SW9N0XEkqI6FwBX\nRsQVkuYD5wGnSnor8DbgsFTvbuAY4I70/YMR0VbBepqZ2QApN8ZxDfAmSbdFxDuBxypZcEQEsCl9\nrU+f0hsH5wCFJ+8uBq4tNAcayR60qNT2hUr6NzOz6iiXOEZJ+hJwUMlj1QGIiH/pb+FpbGQp8Frg\n+xFxT0mVB4EFwL8CJwHjJe0dEb+TtBh4nixxXFRylPMjSV3Az4FzUpIyM7NBUG6M4/1AFzseq176\n6VdEdEXEXGA6cKSkQ0qqLASOSVdsHQOsAbokvRZ4fWo3DZgv6ejU5oMRcSjZHexHA6f21rek0yW1\nSWpbu3ZtnnDNzCyHckccx0fEtySNiYizd6eTiFifjiCOB1YUlT9HdsSBpHHAyanux4ElEbEpzbuJ\n7F6SX0fEmtR2o6SfAEcCV/bS58XAxQCtra0+IjEzGyDljjg+kv48cVcWLKml8Dh2SU3AcZSMk0ia\nIqkQw1nAZWl6NdmRyOg0sH4M8Gj6PiW1rQfeQ1EiMjOz6it3xPGopP8B9pO0vKhcZGPfh/XRrmAq\ncEUa5xgF/DQibpB0NtAWEdcBxwLnSQrgLuCTqe01wHzgIbKB8l9GxPWS9gJuTkmjDvgV8MMK1tfM\nzHaTyo0rS3oNcDNwQum84fRWwNbW1mhr89W7ZmaVkLQ0IlpLy8s+ciQi/gC8MZ1qmhERK6sVoJmZ\nDQ95no77XmAZ8Mv0fa6k66odmJmZ1aY8jxz5GtmVS+sBImIZcEAVYzIzsxqW6yGHEbGhpMyXt5qZ\njVB5Hqv+sKS/Buok/Qnw98BvqxuWmZnVqjxHHJ8C3kD20MKfABuAT1czKDMzq1153jm+Gfhy+piZ\n2QiX54jDzMyshxOHmZlVJM99HG/LU2ZmZiNDniOOC3OWmZnZCFDu1bFHAW8FWkpe5DSB7AGDZmY2\nApW7qqoBGMeOFzkVtAOnVDMoMzOrXX0mjoi4E7hT0uXD6Um4ZmZWXXnuHB8j6WJgVnH9iJhfraDM\nzKx25UkcPwN+AFxC9g5yMzMbwfIkju0R8e9Vj8TMzIaFPJfjXi/pE5KmSppc+FQ9MjMzq0l5jjg+\nnP78XFFZALMHPhwzM6t1eR5y6Jc2mZlZjzyPHBkr6Svpyiok/Ymk91Q/NDMzq0V5xjh+BGwju4sc\nYA1wTtUiMjOzmpYncRwYEd8GOqHn/RyqalRmZlaz8iSObZKaSO8Zl3Qg2dsAzcxsBMpzVdVXgV8C\n+0v6MfA24LRqBmVmZrUrz1VVt0q6H5hHdorqHyLipapHZmZmNSnvGwCnkT1KvQF4u6QF1QvJzMxq\nWb9HHJIuAw4DHga6U3EAi/pp1wjcBYxJ/VwTEV8tqTMTuAxoAV4GPhQRz6Z53wb+giy53Up2pBOS\n3gRcDjQBNxbK86ysmZntvjxjHPMiYs4uLLsDmB8RmyTVA3dLuikilhTVuQC4MiKukDQfOA84VdJb\nycZSDkv17gaOAe4A/h34OHAPWeI4HrhpF+IzM7NdkOdU1e8kVZw4IrMpfa1Pn9IjgznA7Wl6MfC+\nQnOgkezU2JjU9gVJU4EJEbEkHWVcCZxYaWxmZrbr8iSOK8mSx0pJyyU9JGl5noVLqpO0DHgRuDUi\n7imp8iBQGC85CRgvae+I+B1ZInk+fW6OiEfJxlqeLWr/bCrrre/TJbVJalu7dm2ecM3MLIc8p6ou\nBU4FHmLHGEcuEdEFzJU0EfiFpEMiYkVRlYXARZJOIxsPWQN0SXot8Hpgeqp3q6SjgS0V9H0xcDFA\na2urx0DMzAZInsSxNiKu251OImK9pMVk4xErisqfIx1xSBoHnJzqfhxYUjjVJekm4CjgP9mRTEjT\na3YnNjMzq0yeU1UPSPqJpA9IWlD49NdIUks60iDdeX4c8FhJnSmSCjGcRXaFFcBq4BhJo9PA+jHA\noxHxPNAuaZ4kAX8D/HeeFTUzs4GR54ijiewKqXcVlfV7OS4wFbhCUh1ZgvppRNwg6WygLR3FHAuc\nJynITlV9MrW9BphPdnosgF9GxPVp3ifYcTnuTfiKKjOzQaX+boGQ9LaI+E1/ZbWstbU12trahjoM\nM7NhRdLSiGgtLc9zqurCnGVmZjYC9HmqStJRZO/gaJF0ZtGsCWSPHzEzsxGo3BhHAzAu1RlfVN4O\nnFLNoMzMrHb1mTgi4k7gTkmXR8TTgxiTmZnVsDxXVW2W9E/AG8geAwJARMyvWlQ1Yvny5SxatIjV\nq1czY8YMFixYwGGHHVa1dsNFufVbvnw53/jGN/j1r39NZ2cnBxxwAF/84hc55ZT+D1L39O22O/rb\nNqXzDznkEFasWMHq1atpaGhAEh0dHbnaLliQXW2/aNEili1bxvr165k4cSJz585l/PjxXHbZZTz1\n1FN0dnYCUF9fz7777svJJ5/M5s2bWbJkCRHBvHnz+MQnPtHT1zXXXMP555/PqlWrqK+v5+ijj2bG\njBlceumltLe3A9Dc3MxXvvIVjjvuOBYuXNjz76ihoYFp06bx0ksvsWHDBgD22msvjjrqKGbPns3K\nlStZtWoV3d3dzJ49mzPOOIODDjroVes1fvx4rrrqKp566ikaGho4+uijmTdvHtdffz1r1qyhubmZ\n2bNns23bNtavX89LL73E008/zdatWxkzZgwHHHAAXV1d/OEPf6ChoQGAF154oc+/s6amJiTR2dlJ\nV1dXT1lzczObNm1iy5bsfubu7m66u7vp7UKlUaNGIYlRo0b1bO9KjR49mvPPP5/Pfvazu9S+N3mu\nqroFuJrsLu+/Az5MdlPgFwYsiirblauqli9fzgUXXMCkSZNobm5mw4YNrFu3joULF5bdme1qu+Gi\n3PoBfPKTn2T58uU0NjZSV1fHli1b2Guvvfjud79bNnns6dttd/S3bUrnP/HEEyxZsoSjjjqKcePG\ncddddwHw9re/ncbGxrJtN2zYwJNPPokkxo8fz4oVKxg1ahTd3d00Nzdz3333sX37doCenSFkO8SO\njg4mT57MAQccAMDGjRs58MAD+eY3v8njjz/Opz/9aV555RWampro6upi/fr1bNu2baf1ra+vZ8qU\nKT07ZUmv6qvUxIkT2b59O42NjUhi0qRJvPLKK8yePZvDDz+c5uZm2trauOOOOxg9ejTjxo2jq6uL\njRs30tXVxcyZMxk3bhyrVq2io6ODlpYWOjs7ef7553t22oX1HTVqFJMnT2b9+vU922G4uOCCCypO\nHrtzVdXeEXEp0BkRd0bE35LdY7FHW7RoEZMmTWLSpEmMGjWqZ3rRovK3r+xqu+Gi3PotWrSIp556\nisbGRpqammhoaGDs2LF0dnZy0UUX7fJyR7r+tk3p/Oeee44JEyawZs0aVq5cyYQJE5gwYQIrV67s\nt+2kSZNYu3YtL774Is899xxNTU1MnDiRpqYmHnroIbZv386oUaN22pF3dHQgiQ0bNjB27FjGjh3L\nhAkTWLt2LYsWLeKiiy6is7OTsWPH0tDQQFNT06t+QdfV1VFXV9fzC/35559n1KhRjB49mtGjdz4x\nUle34/qc9vZ2uru72Wuvvaivr6ejo4POzk6efPLJnvV65JFHiAgi4lX9d3V1sW3bNl5++WXGjh3L\nqFGjWLduHS+//DKQJa26ujq6u7OnLXV3d9PZ2TnskgbAhRcO3MWweRJH4W/3eUl/IelwYPKARVCj\nVq9eTXNz86vKmpubWb16dVXaDRfl1m/16tVs2bKFMWPG9Mwr/Kdfs6b8k2H29O22O/rbNqXzN2zY\nwIQJE9iwYQMbNmygsbGRxsbGnlM85dpClgQ6Ojp62gI0NjayZcuWnh1oqcKZi+KE0tjYSEdHB6tX\nr+75+y9OAr2d7cgeCFGZ4phGjx7N1q1bAXpOBUF29CPpVX0W2m3dupWtW7f2xLZt2zY6OztfFUtx\nu+GYNICeZDgQ8iSOcyQ1A58lO111CfCZAYugRs2YMaPnP1rBhg0bmDFjRlXaDRfl1m/GjBk9pywK\nCv/Jpk3r9SHGuZY70vW3bUrnNzc3097eTnNzM83NzT07xkKCKNcWYMyYMYwZM6anLWQ716ampp7T\nNqUKO9niI4HC2MCMGTN6/v6Ld7q9JYldeSdbcUyFU1aQnT4rGD9+PBHxqj4L7QqJtRBbQ0MD9fX1\nr4qluF1vR0DDweTJA/d7v9/EERE3RMSGiFgREe+IiDft7kMPh4MFCxawbt061q1bR3d3d890YeBw\noNsNF+XWb8GCBcyePZutW7eyZcsWtm3bxubNm6mvr+eMM87Y5eWOdP1tm9L5++23H+3t7UybNo3X\nve51tLe3097ezute97p+265bt46Wlhb22Wcf9ttvP7Zs2cL69evZsmULhx56KKNHj6a7u/tVCQKy\nZBMRNDc3s3nzZjZv3kx7ezstLS0sWLCAM844g/r6ejZv3sy2bdvYsmUL9fX1Pe27urro6uoiIqiv\nr2fq1Kl0d3ezffv2Xn/hFx/ZTJgwgVGjRvHKK6/Q2dnJmDFjqK+v58ADD+xZrzlz5iAJSa/qv66u\njoaGBiZPnszmzZvp7u5m0qRJPTvZiOgZ24As2dTX1w/L5PGpT31qwJaVZ3D8ILK37u0bEYdIOgw4\nISLOGbAoqmxXHzniq6p656uqBp+vqvJVVUNxVVVfg+N5EsedwOeA/4iIw1PZiog4pOIohoifVWVm\nVrnduapqbETcW1I2PEeHzMxst+VJHC9JOpD0vnBJp5C9ztXMzEagPCM8nyR7BevBktYAq4APVjUq\nMzOrWWUTR3o7X2tE/KmkvYBREbFxcEIzM7NaVPZUVUR0A59P0684aZiZWZ4xjl9JWihpf0mTC5+q\nR2ZmZjUpzxjHX6U/P1lUFsDsgQ/HzMxqXZ7E8fqI2FpcIKmxr8pmZrZny3Oq6rc5y8zMbAQo987x\n1wDTgKb0RNzCU74mAGMHITYzM6tB5U5VvRs4DZgO/DM7Ekc78KXqhmVmZrWq3DvHrwCukHRyRPx8\nEGMyM7Maluex6k4aZmbWI8/guJmZWQ8nDjMzq0i5q6rKvnotIhaVm5/u9bgLGJP6uSYivlpSZyZw\nGdACvAx8KCKelfQO4DtFVQ8G3h8R10q6HDgGKLzv8rSIWFYuFjMzGzjlrqp6b/pzH+CtwO3p+zvI\n7uMomziADmB+RGySVA/cLemmiFhSVOcC4MqIuELSfOA84NSIWAzMBUiPN3kCuKWo3eci4pr+V8/M\nzAZauauqPgIg6RZgTkQ8n75PBS7vb8GRvVpwU/panz6lrxucA5yZphcD1/ayqFOAmyJic399mplZ\n9eUZ49i/kDSSF4AZeRYuqU7SMuBF4NaIuKekyoNA4ZTYScB4SXuX1Hk/cFVJ2bmSlkv6jqQxffR9\nuqQ2SW1r167NE66ZmeWQJ3HcJulmSadJOg34f8Cv8iw8IroiYi7ZTYRHSip9T/lC4BhJD5CNW6wB\nugoz09HNocDNRW3OIhvzeDMwGfhCH31fHBGtEdHa0tKSJ1wzM8uh34ccRsQZkk4C3p6KLo6IX1TS\nSUSsl7QYOB5YUVT+HOmIQ9I44OSIWF/U9H8Dv4iIzqI2haOfDkk/Iks+ZmY2SPJejns/8P8i4jPA\nzZLG99dAUoukiWm6CTgOeKykzpT0lkHIjiQuK1nMByg5TZWOQpAk4ESKEpGZmVVfv4lD0seBa4D/\nSEXT6H0Qu9RUYLGk5cB9ZGMcN0g6W9IJqc6xwEpJjwP7AucW9TsL2B+4s2S5P5b0EPAQMAU4J0cs\nZmY2QJRd/FSmQja4fSRwT0QcnsoeiohDByG+AdHa2hptbW1DHYaZ2bAiaWlEtJaW5zlV1RER24oW\nNJqdL6s1M7MRIk/iuFPSl8jey3Ec8DPg+uqGZWZmtSpP4vgisJZsTOH/A26MiC9XNSozM6tZed45\n/qmI+Ffgh4UCSf+QyszMbITJc8Tx4V7KThvgOMzMbJgo93TcDwB/DRwg6bqiWePJnmRrZmYjULlT\nVb8Fnie7V+Kfi8o3AsurGZSZmdWuck/HfRp4Gjhq8MIxM7Nal+fO8XmS7pO0SdI2SV2S2gcjODMz\nqz15BscvIntm1P8ATcDHgO9XMygzM6tduR5yGBFPAHXpMek/InvKrZmZjUB57uPYLKkBWCbp22QD\n5nmfqmtmZnuYPAngVKAOOAN4heyJtSdXMygzM6tdeV7k9HSa3AJ8vbrhmJlZrctzVdV7JD0g6WVJ\n7ZI2+qoqM7ORK88Yx3fJXu/6UPT38g4zM9vj5RnjeAZY4aRhZmaQ74jj88CNku4EOgqFEfEvVYvK\nzMxqVp7EcS6wCWgEGqobjpmZ1bo8iWO/iDik6pGYmdmwkGeM40ZJ76p6JGZmNizkSRz/B/ilpC2+\nHNfMzPLcADh+MAIxM7PhodwbAA+OiMckHdHb/Ii4v3phmZlZrSp3xHEmcDqvfvtfQQDzqxKRmZnV\ntHJvADw9Tf5ZRGwtniepsapRmZlZzcozOP7bnGVmZjYC9Jk4JL1G0puAJkmHSzoifY4Fxva3YEmN\nku6V9KCkhyXt9GRdSTMl3SZpuaQ7JE1P5e+QtKzos1XSiWneAZLukfSEpKvTu0LMzGyQlBvjeDdw\nGjCdbJxDqXwj8KUcy+4A5kfEJkn1wN2SboqIJUV1LgCujIgrJM0HzgNOjYjFwFwASZOBJ4BbUptv\nAd+JiP8r6QfAR4F/zxGPmZkNgHJjHFcAV0g6OSJ+XumC00MRN6Wv9elT+qDEOWSD8ACLgWt7WdQp\nwE0RsVmSyAbl/zrNuwL4Gk4cZmaDJs8Yx3RJE5S5RNL9ee8kl1QnaRnwInBrRNxTUuVBske2A5wE\njJe0d0md9wNXpem9gfURsT19fxaY1kffp0tqk9S2du3aPOGamVkOeRLH30ZEO/Aush33qcD5eRYe\nEV0RMZfsdNeRkkqfebUQOEbSA8AxwBqgqzBT0lTgUODmPP2V9H1xRLRGRGtLS0ulzc3MrA95HnJY\nGNv4c7LxiIfTKaPcImK9pMXA8cCKovLnSEccksYBJ0fE+qKm/xv4RUR0pu9/BCZKGp2OOqaTJRsz\nMxskeY44lkq6hSxx3CxpPLs1ZQcAAAsoSURBVNDdXyNJLZImpukm4DjgsZI6UyQVYjgLuKxkMR9g\nx2mqwrjJYrJxD4APA/+dYx3MzGyA5EkcHwW+CLw5IjaTvZPjIznaTQUWS1oO3Ec2xnGDpLMlnZDq\nHAuslPQ4sC/Zuz8AkDQL2B+4s2S5XwDOlPQE2amzS3PEYmZmA0T9vRE2nZb6IDA7Is6WNAN4TUTc\nOxgBDoTW1tZoa2sb6jDMzIYVSUsjorW0PM8Rx78BR5GdNoLsPo7vD2BsZmY2jOQZHH9LRByRrnwi\nItb5bm0zs5ErzxFHp6Q60s17klrIMThuZmZ7pjyJ43vAL4B9JJ0L3A18s6pRmZlZzcrzBsAfS1oK\nvJPsno4TI+LRqkdmZmY1Kc8YBxHxGCX3YJiZ2ciU51SVmZlZDycOMzOriBOHmZlVxInDzMwq4sRh\nZmYVceIwM7OKOHGYmVlFnDjMzKwiThxmZlYRJw4zM6uIE4eZmVXEicPMzCrixGFmZhVx4jAzs4o4\ncZiZWUWcOMzMrCJOHGZmVhEnDjMzq4gTh5mZVcSJw8zMKuLEYWZmFala4pDUKOleSQ9KeljS13up\nM1PSbZKWS7pD0vSieTMk3SLpUUmPSJqVyi+XtErSsvSZW611MDOznY2u4rI7gPkRsUlSPXC3pJsi\nYklRnQuAKyPiCknzgfOAU9O8K4FzI+JWSeOA7qJ2n4uIa6oYu5mZ9aFqRxyR2ZS+1qdPlFSbA9ye\nphcD7wOQNAcYHRG3pmVtiojN1YrVzMzyq+oYh6Q6ScuAF4FbI+KekioPAgvS9EnAeEl7AwcB6yUt\nkvSApH+SVFfU7tx0eus7ksb00ffpktokta1du3aA18zMbOSqauKIiK6ImAtMB46UdEhJlYXAMZIe\nAI4B1gBdZKfQjk7z3wzMBk5Lbc4CDk7lk4Ev9NH3xRHRGhGtLS0tA7peZmYj2aBcVRUR68lORR1f\nUv5cRCyIiMOBLxfVfRZYFhFPRcR24FrgiDT/+XQarAP4EXDkYKyDmZllqnlVVYukiWm6CTgOeKyk\nzhRJhRjOAi5L0/cBEyUVDhXmA4+kNlPTnwJOBFZUax3MzGxn1TzimAoslrScLBHcGhE3SDpb0gmp\nzrHASkmPA/sC50J2iovsNNVtkh4CBPwwtflxKnsImAKcU8V1MDOzEooovdBpz9Pa2hptbW1DHYaZ\n2bAiaWlEtJaW+85xMzOriBOHmZlVxInDzMwq4sRhZmYVceIwM7OKOHGYmVlFnDjMzKwiThxmZlaR\nEXEDoKS1wNNDHMYU4KUhjqEvjm3XOLZd49gqN1RxzYyInZ4SOyISRy2Q1NbbHZi1wLHtGse2axxb\n5WotLp+qMjOzijhxmJlZRZw4Bs/FQx1AGY5t1zi2XePYKldTcXmMw8zMKuIjDjMzq4gTh5mZVcSJ\nYwBJukzSi5J6fZ2tMt+T9ISk5ZKOqKHYjpW0QdKy9PnHQYxtf0mLJT0i6WFJ/9BLnUHfdjnjGsrt\n1ijpXkkPpvi+3kudMZKuTtvtHkmzaiSu0yStLdpuH6t2XCX910l6QNINvcwb9G1WQWxDut16RIQ/\nA/QB3g4cAazoY/6fAzeRvQp3HnBPDcV2LHDDEG23qcARaXo88DgwZ6i3Xc64hnK7CRiXpuuBe4B5\nJXU+AfwgTb8fuLpG4joNuGgotlvq/0zgJ7393Q3FNqsgtiHdboWPjzgGUETcBbxcpsr7gCsjswSY\nKGlqjcQ2ZCLi+Yi4P01vBB4FppVUG/RtlzOuIZO2xab0tT59Sq92eR9wRZq+BninJNVAXENG0nTg\nL4BL+qgy6NusgthqghPH4JoGPFP0/VlqaEcEHJVOL9wk6Q1DEUA6LXA42a/UYkO67crEBUO43dJp\njWXAi8CtEdHndouI7cAGYO8aiAvg5HTa8RpJ+1c7piLfBT4PdPcxf0i2WdJfbDB0262HE4cV3E/2\nXJo3AhcC1w52AJLGAT8HPh0R7YPdf1/6iWtIt1tEdEXEXGA6cKSkQwaz/77kiOt6YFZEHAbcyo5f\n+FUl6T3AixGxdDD6q0TO2IZku5Vy4hhca4DiXwjTU9mQi4j2wumFiLgRqJc0ZbD6l1RPtnP+cUQs\n6qXKkGy7/uIa6u1WFMd6YDFwfMmsnu0maTTQDPxxqOOKiD9GREf6egnwpkEK6W3ACZJ+D/xfYL6k\n/yqpM1TbrN/YhnC7vYoTx+C6DvibdIXQPGBDRDw/1EEBSHpN4TyupCPJ/m0Myg4m9Xsp8GhE/Esf\n1QZ92+WJa4i3W4ukiWm6CTgOeKyk2nXAh9P0KcDtkUZZhzKukvGpE8jGj6ouIs6KiOkRMYts4Pv2\niPhQSbVB32Z5Yxuq7VZq9FB0uqeSdBXZVTZTJD0LfJVsYJCI+AFwI9nVQU8Am4GP1FBspwD/R9J2\nYAvw/sH4z5K8DTgVeCidFwf4EjCjKL6h2HZ54hrK7TYVuEJSHVnC+mlE3CDpbKAtIq4jS3z/KekJ\nsosj3l8jcf29pBOA7Smu0wYhrj7VwDbLG1tNbDc/csTMzCriU1VmZlYRJw4zM6uIE4eZmVXEicPM\nzCrixGFmZhVx4rARR9Llkk7ppfw0SfsNYD/HSnrrQC1voPuRdFV6dMVnJB2cnrb6gKQDB6N/G76c\nOGzYSjcDDuS/4dOAXhNHuiehUscCg7FDrbgfSa8B3hwRh0XEd4ATgWsi4vCIeLLa/dvw5sRhw4qk\nWZJWSroSWAHsL+ldkn4n6X5JP0vPlkLSP0q6T9IKSReXe8JpOgJpBX6cfnk3Sfq9pG9Juh/4S0kH\nSvqlpKWSfi3p4NT2vcre2/CApF9J2lfZQxH/DvhMWt7R6Ujn3yUtkfRU+qV+maRHJV1eFEtf6/N7\nSV9P5Q+lo4Sd+ilZr71SH/em+N6XZt0CTEttvgp8muxGxsWp3YdSm2WS/qOQOCUdn/p/UNJt/fVv\ne6ihfq67P/5U8gFmkT05dF76PgW4C9grff8C8I9penJRu/8E3pumLwdO6WXZdwCtRd9/D3y+6Ptt\nwJ+k6beQPRICYBI7bqb9GPDPafprwMKi9peTPYNIZI/ubgcOJfsBtxSY28/6/B74VJr+BHBJb/2U\nrNM3gQ+l6Ylk7xTZK23HFUX1epYBvJ7sYXr16fu/AX8DtJA9NfaA4u1brn9/9syPHzliw9HTkb2T\nA7KXOs0BfpMOKBqA36V575D0eWAsMBl4mGyHWImroecJuW8FflZ04DIm/TkduDo9R6gBWFVmeddH\nREh6CHghIh5Ky3+YbGc+vcz6ABQetLgUWJAj/neRPThvYfreSPbIlC1l2ryT7OF596UYmsgejz4P\nuCsiVgFERE2+38Wqz4nDhqNXiqZF9r6HDxRXkNRI9ku5NSKekfQ1sp3mrvY1Clgf2aPCS10I/EtE\nXCfpWLJf4H0pPNm0u2i68H000EUv69NL+y7y/f8VcHJErHxVYfnXoQq4IiLOKmnz3hz92QjgMQ4b\n7pYAb5P0Wug5p38QO5LES+loYaerqHqxkewVsTuJ7D0cqyT9ZepHkt6YZjez4xHvHy5q1ufyyuhr\nfXYpbuBm4FOF8R1Jh+eI4TbgFEn7pDaTJc1Msb1d0gGF8hz92x7IicOGtYhYS3Y11FWSlpOd1jk4\nsvdA/JBsAP1m4L4ci7sc+EFhcLyX+R8EPirpQbLTXoWB5q+RncJaCrxUVP964KRKBo37Wp9+mpXr\n5xtkT0Fenk6HfSNHDI8AXwFuSTHcCkxNsZ0OLErb4OpdXU8b3vx0XDMzq4iPOMzMrCJOHGZmVhEn\nDjMzq4gTh5mZVcSJw8zMKuLEYWZmFXHiMDOzivz/p7ci2Sh6zS4AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"HSiTTv7KLOhI","colab_type":"text"},"source":["## QUESTION 1\n","\n","IS THE S-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"4rBASPn-FNSK","colab_type":"text"},"source":["# Answer 1: #\n","\n","The propensity score matching(PSM) which is a popular tool to estimate causal effects in observational studies shows not so good results when used with Linear Regression as a model. The reason for coming to such a conclusion is the lack of heterogenous treatment effects that the model can show as the avergae treatment effect is fixed; which could be determined by the horizontal line in the graph and by the pehe-mean score."]},{"cell_type":"markdown","metadata":{"id":"dJe9Cnj6LOhL","colab_type":"text"},"source":["## 1.2 Propensity Score Weighing with Linear Regression"]},{"cell_type":"code","metadata":{"id":"w7TpRNPdLOhP","colab_type":"code","colab":{}},"source":["# Importing the relevant PSWEstimator\n","\n","from justcause.learners import PSWEstimator\n","\n","\n","#Defining the Propoensity Score weighing function that returns the ITE\n","\n","def propensity_score_weighing(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    pswestimator = model\n","\n","    return (\n","        pswestimator.estimate_ate(train_X, train_t, train_y),\n","        pswestimator.estimate_ate(test_X, test_t, test_y)\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNoIPNfgLOhb","colab_type":"code","colab":{}},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","\n","model = PSWEstimator(propensity_learner=None, delta=0.001)\n","\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = propensity_score_weighing(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'PSW', 'train': True})\n","test_result.update({'method': 'PSW', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"caopoaQrLOhm","colab_type":"code","outputId":"61b14ab2-5dab-46f8-f6f2-8dd2ed9d00a9","executionInfo":{"status":"ok","timestamp":1584385929579,"user_tz":-60,"elapsed":214966,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_PSW_LR=pd.DataFrame([train_result, test_result])\n","df_PSW_LR"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.595322</td>\n","      <td>2.537818</td>\n","      <td>8.244302</td>\n","      <td>0.412006</td>\n","      <td>0.284332</td>\n","      <td>0.457697</td>\n","      <td>PSW</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.837997</td>\n","      <td>3.484394</td>\n","      <td>8.323623</td>\n","      <td>3.783440</td>\n","      <td>2.649187</td>\n","      <td>3.225824</td>\n","      <td>PSW</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...  method  train\n","0         5.595322           2.537818  ...     PSW   True\n","1         6.837997           3.484394  ...     PSW  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"jAcuhZhKLOhy","colab_type":"text"},"source":["## 1.3 S-Learner Random Forest"]},{"cell_type":"code","metadata":{"id":"wtiDtGbyLOh3","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","\n","\n","# Importing the relevant S-Learner estimator\n","from justcause.learners import SLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_slearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    slearner = model  \n","    slearner.fit(train_X, train_t, train_y)\n","    return (\n","        slearner.predict_ite(train_X, train_t, train_y),\n","        slearner.predict_ite(test_X, test_t, test_y)\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XW0XLJ0ELOiG","colab_type":"code","colab":{}},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","#---------------------------Question--------------------------------#\n","# Pass a RandomForestRegressor into the S-learner\n","\n","rreg = RandomForestRegressor()\n","\n","model = SLearner(rreg)\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_slearner(train, test, model )\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'S-Learner RF', 'train': True})\n","test_result.update({'method': 'S-Learner RF', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"QgIkGA9MLOiR","colab_type":"code","outputId":"3760edb3-9171-496e-9b51-54ad743c9e86","executionInfo":{"status":"ok","timestamp":1584385970738,"user_tz":-60,"elapsed":256112,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_S_learner_RF=pd.DataFrame([train_result, test_result])\n","df_S_learner_RF"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.105397</td>\n","      <td>1.050890</td>\n","      <td>4.766095</td>\n","      <td>0.500324</td>\n","      <td>0.133818</td>\n","      <td>0.928113</td>\n","      <td>S-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.319565</td>\n","      <td>1.273795</td>\n","      <td>5.171689</td>\n","      <td>0.446731</td>\n","      <td>0.133490</td>\n","      <td>1.027640</td>\n","      <td>S-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         3.105397           1.050890  ...  S-Learner RF   True\n","1         3.319565           1.273795  ...  S-Learner RF  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"LyU9_8rcLOid","colab_type":"text"},"source":["### 1.3.1 Random Forest Visualization"]},{"cell_type":"code","metadata":{"id":"Meb4UZjELOig","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","\n","\n","# Importing the relevant SLearner module\n","from justcause.learners import SLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_slearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    slearner = model\n","    slearner.fit(train_X, train_t, train_y)\n","    return (\n","        slearner.predict_ite(train_X, train_t, train_y),\n","        slearner.predict_ite(test_X, test_t, test_y)\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"eF2mk_AxLOin","colab_type":"code","outputId":"02d9a81f-5a2c-407b-df65-8699b442e5b8","executionInfo":{"status":"ok","timestamp":1584385971029,"user_tz":-60,"elapsed":256393,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Q0t4K4CfLOit","colab_type":"text"},"source":["## QUESTION 2\n","\n","IS THE S-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"Z-eY_sagNVj0","colab_type":"text"},"source":["# Answer 2 #\n","\n","In comparison to the training of S-Learner with 'Linear Regression' as a model, the S-Learner trained with 'Random Forest' as a model shows better results. The heterogenity of the treatment effect is captured which can be observed in the graph and the pehe scores. The distribution between the real time and estimated treatment effects increases linearly, indicating with a high estimated treatment effect the real-time effect is also high. Hence, the model is not optimum as the S-learner is used on the estimators in the model and the treatment effects are handled as a feature, as all the other features are getting handled the treatment effects feature doesn't get assigned any role."]},{"cell_type":"markdown","metadata":{"id":"vowzj2teLOiw","colab_type":"text"},"source":["## 1.4 T-Learner Linear Regression"]},{"cell_type":"code","metadata":{"id":"811XNGgLLOix","colab_type":"code","colab":{}},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import TLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_tlearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    tlearner = model\n","    tlearner.fit(train_X, train_t, train_y)\n","    return (\n","        tlearner.predict_ite(train_X, train_t, train_y),\n","        tlearner.predict_ite(test_X, test_t, test_y)\n","    )\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_GCgPWtLOi2","colab_type":"code","colab":{}},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","#------------------Question------------------------------#\n","# Pass linear regression into the T-Learner\n","\n","\n","\n","model = TLearner(lreg)\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner LR', 'train': True})\n","test_result.update({'method': 'T-Learner LR', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tMeJsu4zLOi6","colab_type":"code","outputId":"3e833555-1303-47cf-e7c0-32c7b8ca3977","executionInfo":{"status":"ok","timestamp":1584385971956,"user_tz":-60,"elapsed":257306,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_T_learner_LR=pd.DataFrame([train_result, test_result])\n","df_T_learner_LR"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.276868</td>\n","      <td>1.055409</td>\n","      <td>3.319402</td>\n","      <td>0.149960</td>\n","      <td>0.133955</td>\n","      <td>0.129121</td>\n","      <td>T-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.337753</td>\n","      <td>1.122464</td>\n","      <td>3.263915</td>\n","      <td>0.263287</td>\n","      <td>0.195850</td>\n","      <td>0.327846</td>\n","      <td>T-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         2.276868           1.055409  ...  T-Learner LR   True\n","1         2.337753           1.122464  ...  T-Learner LR  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"-k49nGyQLOi9","colab_type":"text"},"source":["### 1.4.1 T-Learner Linear Regression Visualization"]},{"cell_type":"code","metadata":{"id":"9u2DMEWTLOi-","colab_type":"code","colab":{}},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner LR', 'train': True})\n","test_result.update({'method': 'T-Learner LR', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VtFgO5PZLOjC","colab_type":"code","outputId":"db0076d1-c11f-4062-9da7-03fee9265711","executionInfo":{"status":"ok","timestamp":1584385972376,"user_tz":-60,"elapsed":257715,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"61rEzJoLLOjF","colab_type":"text"},"source":["## QUESTION 3\n","\n","IS THE T-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"ddL7zOj3P_D1","colab_type":"text"},"source":["# Answer 3 #\n","\n","The T-Learner trained on Linear Regression is even not the optimum one in estimating the individual treatment effects well as the density of the graph is high around the 4.0-4.5 area of the real treatment effect. Irrespective of the low or high value of the estimated treatment effect the real treatment effect tends to be high."]},{"cell_type":"markdown","metadata":{"id":"RO36FBY1LOjG","colab_type":"text"},"source":["## 1.5 T-Learner Random Forest"]},{"cell_type":"code","metadata":{"id":"pjA-HNpCLOjH","colab_type":"code","colab":{}},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import TLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_tlearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    tlearner = model\n","    tlearner.fit(train_X, train_t, train_y)\n","    return (\n","        tlearner.predict_ite(train_X, train_t, train_y),\n","        tlearner.predict_ite(test_X, test_t, test_y)\n","    )\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMXoNfexLOjL","colab_type":"code","colab":{}},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","#--------------------Question----------------------------------#\n","# Pass a Random Forest into the T-Learner\n","\n","model = TLearner(rreg)\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"psT80zSaLOjP","colab_type":"code","outputId":"857d8aa2-c803-4a83-9e6b-3a3b4613fb03","executionInfo":{"status":"ok","timestamp":1584386021428,"user_tz":-60,"elapsed":306755,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_T_learner_RF=pd.DataFrame([train_result, test_result])\n","df_T_learner_RF"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.779325</td>\n","      <td>0.953937</td>\n","      <td>2.242132</td>\n","      <td>0.123705</td>\n","      <td>0.094111</td>\n","      <td>0.115848</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.509532</td>\n","      <td>1.112424</td>\n","      <td>3.500381</td>\n","      <td>0.205711</td>\n","      <td>0.118718</td>\n","      <td>0.275564</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         1.779325           0.953937  ...  T-Learner RF   True\n","1         2.509532           1.112424  ...  T-Learner RF  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"XSU2Kyj9LOjT","colab_type":"text"},"source":["### 1.5.1 T-Learner with Random Forrest Visualization"]},{"cell_type":"code","metadata":{"id":"BKNP1x2KLOjU","colab_type":"code","colab":{}},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha8GizLtLOjY","colab_type":"code","outputId":"1f1c9bd9-3576-45d2-9edc-58e03885930b","executionInfo":{"status":"ok","timestamp":1584386022184,"user_tz":-60,"elapsed":307500,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":30,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrAAwxpgU9f8Adi3cvze/3TYAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"6x_BryEaLOjd","colab_type":"text"},"source":["## QUESTION 4\n","\n","IS THE T-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"lqbmjqM-ROTm","colab_type":"text"},"source":["# Answer 4 #\n","\n","Alternatively, the T-Learner trained on a Random Forest proves to be a good estimator of the individual treatment effect as the symmetry between the real treatment effects and the estimated treatment effects seems to be in line. Hence, low overfitting. This conclusion is also proven by the low pehe-mean score for the training = True part, indicating people receiving the actual treatment."]},{"cell_type":"markdown","metadata":{"id":"jh0sdhdKLOjf","colab_type":"text"},"source":["## 1.6 Causal Forest"]},{"cell_type":"code","metadata":{"id":"8GWzTUTlLOjg","colab_type":"code","colab":{}},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import CausalForest\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def causal_forest(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    causalforest = CausalForest(random_state=random_state)\n","    causalforest.fit(train_X, train_t, train_y)\n","    return (\n","        causalforest.predict_ite(train_X, train_t, train_y),\n","        causalforest.predict_ite(test_X, test_t, test_y)\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZY-qvwfHLOjk","colab_type":"code","colab":{}},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","model = CausalForest(random_state=random_state)\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = causal_forest(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B78davrGLOjn","colab_type":"code","outputId":"4ec3fff8-ac59-4da6-9762-39f00c2c63f8","executionInfo":{"status":"ok","timestamp":1584386037468,"user_tz":-60,"elapsed":322771,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_causal_forest=pd.DataFrame([train_result, test_result])\n","df_causal_forest"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.172661</td>\n","      <td>1.921242</td>\n","      <td>6.330031</td>\n","      <td>0.441738</td>\n","      <td>0.198733</td>\n","      <td>0.823171</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.378083</td>\n","      <td>1.797608</td>\n","      <td>6.606150</td>\n","      <td>0.683703</td>\n","      <td>0.233695</td>\n","      <td>1.354973</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         4.172661           1.921242  ...  T-Learner RF   True\n","1         4.378083           1.797608  ...  T-Learner RF  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"GfQBHXUTLOjt","colab_type":"text"},"source":["### 1.6.1 Causal Forest Visualization "]},{"cell_type":"code","metadata":{"id":"Z4c3Y-XDLOju","colab_type":"code","colab":{}},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = causal_forest(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3mIANgDLOjw","colab_type":"code","outputId":"6a619e5a-5c69-4b8c-8f67-929aa8627d7e","executionInfo":{"status":"ok","timestamp":1584386037757,"user_tz":-60,"elapsed":323050,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":35,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAmOMyXL/F6/vfNvN7Y1mAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3ingDVHQLOj0","colab_type":"text"},"source":["## QUESTION 5\n","\n","IS THE CAUSAL FOREST WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"Sg2hKgahSSXp","colab_type":"text"},"source":["# Answer 5 #\n","\n","The Causal Forest trained on Linear Regression is not a good estimator for estimating individual treatment effects as the graph shows for 4.2-4.4 estimated effect the real effect is also considerably high. Additionally, the pehe-standard deviation is huge 6.3-6.6(approx) for training true and false respectively. Thus, implying the size of the leaves of both treatment and non-treatment are not sufficient. "]},{"cell_type":"markdown","metadata":{"id":"JnFLlHMKLOj1","colab_type":"text"},"source":["## 1.7 Neural Network"]},{"cell_type":"code","metadata":{"id":"QXE4Usi1LOj2","colab_type":"code","colab":{}},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import DragonNet\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def causal_forest(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    dragonnet = model\n","    dragonnet.fit(train_X, train_t, train_y)\n","    return (\n","        dragonnet.predict_ite(train_X, train_t, train_y),\n","        dragonnet.predict_ite(test_X, test_t, test_y)\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeJ5_9NBLOj5","colab_type":"code","outputId":"02ddce04-a878-460c-fedf-b5179524cbdb","executionInfo":{"status":"ok","timestamp":1584386392953,"user_tz":-60,"elapsed":678235,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","#---------------------------Question----------------------------#\n","# Set the model to the DragonNet neural network from JustCause\n","\n","\n","model = DragonNet()\n","\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = causal_forest(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'Dragonnet', 'train': True})\n","test_result.update({'method': 'Dragonnet', 'train': False})"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 46/50\n","537/537 [==============================] - 0s 68us/step - loss: 1620.1696 - regression_loss: 685.2870 - val_loss: 305.1628 - val_regression_loss: 137.9672\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 1600.2654 - regression_loss: 678.7872 - val_loss: 297.1945 - val_regression_loss: 134.0851\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 1556.1100 - regression_loss: 654.1004 - val_loss: 291.7565 - val_regression_loss: 131.3416\n","Epoch 49/50\n","537/537 [==============================] - 0s 54us/step - loss: 1503.0106 - regression_loss: 629.7872 - val_loss: 287.8929 - val_regression_loss: 129.3147\n","Epoch 50/50\n","537/537 [==============================] - 0s 52us/step - loss: 1497.6454 - regression_loss: 626.4661 - val_loss: 284.1360 - val_regression_loss: 127.3921\n","***************************** elapsed_time is:  3.286308765411377\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 172524.2193 - regression_loss: 86287.5561 - val_loss: 17495.6582 - val_regression_loss: 8748.5674\n","Epoch 2/50\n","537/537 [==============================] - 0s 58us/step - loss: 151751.0993 - regression_loss: 75903.5338 - val_loss: 14726.4629 - val_regression_loss: 7362.3701\n","Epoch 3/50\n","537/537 [==============================] - 0s 49us/step - loss: 127227.1246 - regression_loss: 63633.0704 - val_loss: 11110.7002 - val_regression_loss: 5550.6543\n","Epoch 4/50\n","537/537 [==============================] - 0s 55us/step - loss: 95435.2762 - regression_loss: 47711.4298 - val_loss: 6968.6504 - val_regression_loss: 3471.0769\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 60228.6298 - regression_loss: 30045.1804 - val_loss: 3551.4392 - val_regression_loss: 1743.3494\n","Epoch 6/50\n","537/537 [==============================] - 0s 59us/step - loss: 28266.8753 - regression_loss: 13917.8090 - val_loss: 3097.9463 - val_regression_loss: 1478.7800\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 20695.3242 - regression_loss: 9841.4839 - val_loss: 4589.9888 - val_regression_loss: 2198.2285\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 30919.3309 - regression_loss: 14734.4555 - val_loss: 3843.7949 - val_regression_loss: 1839.0308\n","Epoch 9/50\n","537/537 [==============================] - 0s 57us/step - loss: 26070.7882 - regression_loss: 12425.3052 - val_loss: 2275.1846 - val_regression_loss: 1082.1638\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 15399.7678 - regression_loss: 7309.4261 - val_loss: 1425.4690 - val_regression_loss: 678.4635\n","Epoch 11/50\n","537/537 [==============================] - 0s 53us/step - loss: 10342.8444 - regression_loss: 4945.7425 - val_loss: 1412.1984 - val_regression_loss: 683.7573\n","Epoch 12/50\n","537/537 [==============================] - 0s 54us/step - loss: 11277.6675 - regression_loss: 5502.5443 - val_loss: 1677.9003 - val_regression_loss: 821.9116\n","Epoch 13/50\n","537/537 [==============================] - 0s 58us/step - loss: 13259.5060 - regression_loss: 6531.5734 - val_loss: 1794.2162 - val_regression_loss: 881.4014\n","Epoch 14/50\n","537/537 [==============================] - 0s 57us/step - loss: 14376.8565 - regression_loss: 7099.3003 - val_loss: 1653.6406 - val_regression_loss: 810.1769\n","Epoch 15/50\n","537/537 [==============================] - 0s 54us/step - loss: 12560.2830 - regression_loss: 6179.7965 - val_loss: 1370.0098 - val_regression_loss: 666.2195\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 9789.9788 - regression_loss: 4774.8599 - val_loss: 1124.5931 - val_regression_loss: 540.8710\n","Epoch 17/50\n","537/537 [==============================] - 0s 49us/step - loss: 7556.4405 - regression_loss: 3636.2263 - val_loss: 1057.4315 - val_regression_loss: 504.7783\n","Epoch 18/50\n","537/537 [==============================] - 0s 52us/step - loss: 6620.0785 - regression_loss: 3145.7388 - val_loss: 1155.5652 - val_regression_loss: 552.1174\n","Epoch 19/50\n","537/537 [==============================] - 0s 57us/step - loss: 7151.3207 - regression_loss: 3396.0820 - val_loss: 1249.5013 - val_regression_loss: 598.5587\n","Epoch 20/50\n","537/537 [==============================] - 0s 52us/step - loss: 7785.4106 - regression_loss: 3706.9849 - val_loss: 1200.4171 - val_regression_loss: 574.6777\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 7432.3883 - regression_loss: 3537.6487 - val_loss: 1047.4880 - val_regression_loss: 499.6698\n","Epoch 22/50\n","537/537 [==============================] - 0s 57us/step - loss: 6311.4847 - regression_loss: 2988.3396 - val_loss: 915.0553 - val_regression_loss: 435.2036\n","Epoch 23/50\n","537/537 [==============================] - 0s 57us/step - loss: 5431.7041 - regression_loss: 2564.3526 - val_loss: 860.7557 - val_regression_loss: 409.6977\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 5217.8518 - regression_loss: 2468.7897 - val_loss: 853.0325 - val_regression_loss: 407.1244\n","Epoch 25/50\n","537/537 [==============================] - 0s 53us/step - loss: 5239.8165 - regression_loss: 2489.1132 - val_loss: 835.0550 - val_regression_loss: 398.9187\n","Epoch 26/50\n","537/537 [==============================] - 0s 54us/step - loss: 5279.8650 - regression_loss: 2515.0379 - val_loss: 787.1321 - val_regression_loss: 375.2736\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 5015.3027 - regression_loss: 2387.0962 - val_loss: 733.4938 - val_regression_loss: 348.3743\n","Epoch 28/50\n","537/537 [==============================] - 0s 52us/step - loss: 4411.1848 - regression_loss: 2082.1747 - val_loss: 704.6136 - val_regression_loss: 333.6038\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 4104.2335 - regression_loss: 1926.9878 - val_loss: 699.6342 - val_regression_loss: 330.6525\n","Epoch 30/50\n","537/537 [==============================] - 0s 58us/step - loss: 4042.5090 - regression_loss: 1893.4039 - val_loss: 693.7297 - val_regression_loss: 327.2416\n","Epoch 31/50\n","537/537 [==============================] - 0s 61us/step - loss: 4050.2822 - regression_loss: 1896.5523 - val_loss: 671.2712 - val_regression_loss: 315.6974\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 3794.7544 - regression_loss: 1760.0385 - val_loss: 643.0250 - val_regression_loss: 301.5287\n","Epoch 33/50\n","537/537 [==============================] - 0s 59us/step - loss: 3622.5305 - regression_loss: 1674.0484 - val_loss: 624.6375 - val_regression_loss: 292.5351\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 3388.2340 - regression_loss: 1556.1029 - val_loss: 613.4963 - val_regression_loss: 287.3080\n","Epoch 35/50\n","537/537 [==============================] - 0s 55us/step - loss: 3440.4761 - regression_loss: 1583.5790 - val_loss: 598.2549 - val_regression_loss: 280.0576\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 3304.1030 - regression_loss: 1521.8966 - val_loss: 578.7372 - val_regression_loss: 270.6289\n","Epoch 37/50\n","537/537 [==============================] - 0s 56us/step - loss: 3082.4658 - regression_loss: 1413.8455 - val_loss: 564.6237 - val_regression_loss: 263.8437\n","Epoch 38/50\n","537/537 [==============================] - 0s 56us/step - loss: 2966.9441 - regression_loss: 1359.2549 - val_loss: 556.0634 - val_regression_loss: 259.7953\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 2919.4357 - regression_loss: 1335.3114 - val_loss: 545.6779 - val_regression_loss: 254.7620\n","Epoch 40/50\n","537/537 [==============================] - 0s 58us/step - loss: 2801.2068 - regression_loss: 1277.6903 - val_loss: 531.6191 - val_regression_loss: 247.8217\n","Epoch 41/50\n","537/537 [==============================] - 0s 55us/step - loss: 2626.0806 - regression_loss: 1188.5428 - val_loss: 518.5000 - val_regression_loss: 241.3071\n","Epoch 42/50\n","537/537 [==============================] - 0s 48us/step - loss: 2641.0015 - regression_loss: 1195.2243 - val_loss: 510.8646 - val_regression_loss: 237.4461\n","Epoch 43/50\n","537/537 [==============================] - 0s 50us/step - loss: 2549.1722 - regression_loss: 1147.2535 - val_loss: 505.0623 - val_regression_loss: 234.4595\n","Epoch 44/50\n","537/537 [==============================] - 0s 52us/step - loss: 2445.7019 - regression_loss: 1095.4910 - val_loss: 499.1080 - val_regression_loss: 231.3700\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 2356.3219 - regression_loss: 1048.9634 - val_loss: 494.7196 - val_regression_loss: 229.1332\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 2294.1627 - regression_loss: 1016.4897 - val_loss: 490.9597 - val_regression_loss: 227.3416\n","Epoch 47/50\n","537/537 [==============================] - 0s 56us/step - loss: 2308.6790 - regression_loss: 1024.7745 - val_loss: 483.0068 - val_regression_loss: 223.5822\n","Epoch 48/50\n","537/537 [==============================] - 0s 56us/step - loss: 2227.5613 - regression_loss: 985.1777 - val_loss: 474.2457 - val_regression_loss: 219.4755\n","Epoch 49/50\n","537/537 [==============================] - 0s 50us/step - loss: 2193.9207 - regression_loss: 971.6523 - val_loss: 465.5399 - val_regression_loss: 215.3676\n","Epoch 50/50\n","537/537 [==============================] - 0s 56us/step - loss: 2171.7956 - regression_loss: 960.5361 - val_loss: 457.6580 - val_regression_loss: 211.5526\n","***************************** elapsed_time is:  3.497187376022339\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 858543.5528 - regression_loss: 429058.9657 - val_loss: 94858.2656 - val_regression_loss: 47402.2656\n","Epoch 2/50\n","537/537 [==============================] - 0s 61us/step - loss: 821651.9644 - regression_loss: 410626.4025 - val_loss: 87872.0469 - val_regression_loss: 43911.0547\n","Epoch 3/50\n","537/537 [==============================] - 0s 50us/step - loss: 742802.8390 - regression_loss: 371217.3254 - val_loss: 77999.4531 - val_regression_loss: 38976.8789\n","Epoch 4/50\n","537/537 [==============================] - 0s 56us/step - loss: 671231.4669 - regression_loss: 335448.6601 - val_loss: 65022.2148 - val_regression_loss: 32490.1484\n","Epoch 5/50\n","537/537 [==============================] - 0s 61us/step - loss: 554906.0872 - regression_loss: 277301.5152 - val_loss: 49784.8789 - val_regression_loss: 24872.6348\n","Epoch 6/50\n","537/537 [==============================] - 0s 47us/step - loss: 428107.6162 - regression_loss: 213910.0650 - val_loss: 34060.8008 - val_regression_loss: 17010.6855\n","Epoch 7/50\n","537/537 [==============================] - 0s 54us/step - loss: 309834.2651 - regression_loss: 154774.9200 - val_loss: 20605.8047 - val_regression_loss: 10282.3164\n","Epoch 8/50\n","537/537 [==============================] - 0s 53us/step - loss: 184665.1945 - regression_loss: 92182.6898 - val_loss: 13133.6377 - val_regression_loss: 6544.7891\n","Epoch 9/50\n","537/537 [==============================] - 0s 54us/step - loss: 134545.6713 - regression_loss: 67109.2918 - val_loss: 14885.6914 - val_regression_loss: 7419.7246\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 145770.7445 - regression_loss: 72715.2293 - val_loss: 18783.5137 - val_regression_loss: 9370.4854\n","Epoch 11/50\n","537/537 [==============================] - 0s 52us/step - loss: 177618.4040 - regression_loss: 88655.4729 - val_loss: 16627.2656 - val_regression_loss: 8296.6367\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 170777.1819 - regression_loss: 85274.5802 - val_loss: 11965.9209 - val_regression_loss: 5969.0723\n","Epoch 13/50\n","537/537 [==============================] - 0s 51us/step - loss: 140250.4600 - regression_loss: 70041.2152 - val_loss: 8994.6777 - val_regression_loss: 4484.1680\n","Epoch 14/50\n","537/537 [==============================] - 0s 64us/step - loss: 117182.1985 - regression_loss: 58514.2854 - val_loss: 8527.8555 - val_regression_loss: 4249.5571\n","Epoch 15/50\n","537/537 [==============================] - 0s 54us/step - loss: 119313.3455 - regression_loss: 59568.6387 - val_loss: 9095.3066 - val_regression_loss: 4530.6528\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 113292.9169 - regression_loss: 56531.7221 - val_loss: 9498.0654 - val_regression_loss: 4728.6060\n","Epoch 17/50\n","537/537 [==============================] - 0s 53us/step - loss: 123440.7719 - regression_loss: 61577.2226 - val_loss: 9325.1182 - val_regression_loss: 4638.8013\n","Epoch 18/50\n","537/537 [==============================] - 0s 55us/step - loss: 118701.5980 - regression_loss: 59176.3247 - val_loss: 8774.0225 - val_regression_loss: 4361.0234\n","Epoch 19/50\n","537/537 [==============================] - 0s 66us/step - loss: 111798.7697 - regression_loss: 55704.5695 - val_loss: 8170.8950 - val_regression_loss: 4058.7817\n","Epoch 20/50\n","537/537 [==============================] - 0s 66us/step - loss: 102882.2096 - regression_loss: 51245.6353 - val_loss: 7773.9453 - val_regression_loss: 3861.0371\n","Epoch 21/50\n","537/537 [==============================] - 0s 59us/step - loss: 94061.8498 - regression_loss: 46841.8696 - val_loss: 7649.3257 - val_regression_loss: 3800.3164\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 96512.5829 - regression_loss: 48090.4177 - val_loss: 7605.4570 - val_regression_loss: 3780.0947\n","Epoch 23/50\n","537/537 [==============================] - 0s 50us/step - loss: 95333.0112 - regression_loss: 47515.0364 - val_loss: 7297.4951 - val_regression_loss: 3627.5095\n","Epoch 24/50\n","537/537 [==============================] - 0s 59us/step - loss: 91428.3526 - regression_loss: 45570.9640 - val_loss: 6714.0586 - val_regression_loss: 3336.8420\n","Epoch 25/50\n","537/537 [==============================] - 0s 47us/step - loss: 82725.4091 - regression_loss: 41228.4309 - val_loss: 6148.6187 - val_regression_loss: 3054.9585\n","Epoch 26/50\n","537/537 [==============================] - 0s 56us/step - loss: 74337.9895 - regression_loss: 37033.5805 - val_loss: 5871.5459 - val_regression_loss: 2917.1370\n","Epoch 27/50\n","537/537 [==============================] - 0s 54us/step - loss: 75191.3450 - regression_loss: 37470.9399 - val_loss: 5798.9614 - val_regression_loss: 2881.3613\n","Epoch 28/50\n","537/537 [==============================] - 0s 56us/step - loss: 69839.5690 - regression_loss: 34792.9139 - val_loss: 5668.2373 - val_regression_loss: 2816.2708\n","Epoch 29/50\n","537/537 [==============================] - 0s 57us/step - loss: 66467.8698 - regression_loss: 33109.2876 - val_loss: 5389.1489 - val_regression_loss: 2676.7559\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 64765.8456 - regression_loss: 32258.3875 - val_loss: 5048.6670 - val_regression_loss: 2506.3291\n","Epoch 31/50\n","537/537 [==============================] - 0s 57us/step - loss: 58756.2829 - regression_loss: 29250.7827 - val_loss: 4791.9546 - val_regression_loss: 2377.7229\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 53940.2601 - regression_loss: 26839.0381 - val_loss: 4597.6831 - val_regression_loss: 2280.4146\n","Epoch 33/50\n","537/537 [==============================] - 0s 55us/step - loss: 49537.4678 - regression_loss: 24637.4799 - val_loss: 4324.8618 - val_regression_loss: 2143.9719\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 46660.3150 - regression_loss: 23200.9582 - val_loss: 3936.1995 - val_regression_loss: 1949.8347\n","Epoch 35/50\n","537/537 [==============================] - 0s 54us/step - loss: 37796.1934 - regression_loss: 18765.0696 - val_loss: 3535.1748 - val_regression_loss: 1749.6874\n","Epoch 36/50\n","537/537 [==============================] - 0s 52us/step - loss: 38886.9173 - regression_loss: 19314.3619 - val_loss: 3230.2617 - val_regression_loss: 1597.5428\n","Epoch 37/50\n","537/537 [==============================] - 0s 61us/step - loss: 36121.5232 - regression_loss: 17931.6336 - val_loss: 2989.8547 - val_regression_loss: 1477.4506\n","Epoch 38/50\n","537/537 [==============================] - 0s 49us/step - loss: 29428.9970 - regression_loss: 14585.2032 - val_loss: 2804.9893 - val_regression_loss: 1384.9395\n","Epoch 39/50\n","537/537 [==============================] - 0s 53us/step - loss: 30122.4426 - regression_loss: 14931.6292 - val_loss: 2700.1235 - val_regression_loss: 1332.2544\n","Epoch 40/50\n","537/537 [==============================] - 0s 51us/step - loss: 27677.8793 - regression_loss: 13705.6285 - val_loss: 2539.3188 - val_regression_loss: 1251.7512\n","Epoch 41/50\n","537/537 [==============================] - 0s 55us/step - loss: 25886.3791 - regression_loss: 12810.2454 - val_loss: 2276.2070 - val_regression_loss: 1120.2842\n","Epoch 42/50\n","537/537 [==============================] - 0s 57us/step - loss: 23996.7664 - regression_loss: 11867.1593 - val_loss: 2028.5334 - val_regression_loss: 996.5663\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 22357.5659 - regression_loss: 11047.7841 - val_loss: 1841.3566 - val_regression_loss: 902.9752\n","Epoch 44/50\n","537/537 [==============================] - 0s 53us/step - loss: 20783.8329 - regression_loss: 10261.8006 - val_loss: 1721.8867 - val_regression_loss: 843.0269\n","Epoch 45/50\n","537/537 [==============================] - 0s 56us/step - loss: 19036.5545 - regression_loss: 9385.3019 - val_loss: 1660.4137 - val_regression_loss: 811.9797\n","Epoch 46/50\n","537/537 [==============================] - 0s 49us/step - loss: 18402.8727 - regression_loss: 9069.0970 - val_loss: 1605.5098 - val_regression_loss: 784.3583\n","Epoch 47/50\n","537/537 [==============================] - 0s 56us/step - loss: 16734.0352 - regression_loss: 8233.1261 - val_loss: 1517.7378 - val_regression_loss: 740.5208\n","Epoch 48/50\n","537/537 [==============================] - 0s 57us/step - loss: 16099.6037 - regression_loss: 7918.8978 - val_loss: 1415.5636 - val_regression_loss: 689.6093\n","Epoch 49/50\n","537/537 [==============================] - 0s 54us/step - loss: 14991.2152 - regression_loss: 7365.4650 - val_loss: 1373.7828 - val_regression_loss: 668.7146\n","Epoch 50/50\n","537/537 [==============================] - 0s 51us/step - loss: 13089.3161 - regression_loss: 6417.0825 - val_loss: 1386.0375 - val_regression_loss: 674.6415\n","***************************** elapsed_time is:  3.3108022212982178\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 186091.5172 - regression_loss: 93171.3922 - val_loss: 23692.7520 - val_regression_loss: 11864.0137\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 163573.8597 - regression_loss: 81876.2420 - val_loss: 20768.1719 - val_regression_loss: 10396.0732\n","Epoch 3/50\n","537/537 [==============================] - 0s 49us/step - loss: 145389.5735 - regression_loss: 72753.4771 - val_loss: 16893.7148 - val_regression_loss: 8450.9336\n","Epoch 4/50\n","537/537 [==============================] - 0s 52us/step - loss: 116468.5164 - regression_loss: 58224.6505 - val_loss: 12273.0596 - val_regression_loss: 6129.5552\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 82935.4928 - regression_loss: 41375.7946 - val_loss: 7946.5923 - val_regression_loss: 3951.3057\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 55285.6790 - regression_loss: 27430.6799 - val_loss: 5826.6343 - val_regression_loss: 2872.3428\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 45441.8151 - regression_loss: 22362.4940 - val_loss: 5913.1533 - val_regression_loss: 2904.9463\n","Epoch 8/50\n","537/537 [==============================] - 0s 46us/step - loss: 52364.8051 - regression_loss: 25743.2539 - val_loss: 5141.5977 - val_regression_loss: 2527.4873\n","Epoch 9/50\n","537/537 [==============================] - 0s 44us/step - loss: 47245.9925 - regression_loss: 23256.8224 - val_loss: 3999.0730 - val_regression_loss: 1971.8335\n","Epoch 10/50\n","537/537 [==============================] - 0s 53us/step - loss: 35948.1242 - regression_loss: 17733.3587 - val_loss: 3638.9580 - val_regression_loss: 1805.4348\n","Epoch 11/50\n","537/537 [==============================] - 0s 54us/step - loss: 31696.1195 - regression_loss: 15722.4524 - val_loss: 4047.6860 - val_regression_loss: 2018.2222\n","Epoch 12/50\n","537/537 [==============================] - 0s 52us/step - loss: 32389.1071 - regression_loss: 16135.5816 - val_loss: 4436.2598 - val_regression_loss: 2215.6306\n","Epoch 13/50\n","537/537 [==============================] - 0s 53us/step - loss: 33054.0195 - regression_loss: 16486.3860 - val_loss: 4277.1963 - val_regression_loss: 2134.9868\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 30694.5577 - regression_loss: 15298.8701 - val_loss: 3660.6304 - val_regression_loss: 1822.5986\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 27169.9474 - regression_loss: 13501.7144 - val_loss: 3005.8916 - val_regression_loss: 1489.5651\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 23852.9710 - regression_loss: 11802.8885 - val_loss: 2663.5715 - val_regression_loss: 1312.6803\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 22491.8726 - regression_loss: 11073.6424 - val_loss: 2584.0393 - val_regression_loss: 1268.8916\n","Epoch 18/50\n","537/537 [==============================] - 0s 53us/step - loss: 23806.9288 - regression_loss: 11701.9567 - val_loss: 2450.4065 - val_regression_loss: 1200.9370\n","Epoch 19/50\n","537/537 [==============================] - 0s 50us/step - loss: 22865.6294 - regression_loss: 11223.2863 - val_loss: 2224.2563 - val_regression_loss: 1089.6583\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 20640.4830 - regression_loss: 10128.6652 - val_loss: 2109.2085 - val_regression_loss: 1035.6168\n","Epoch 21/50\n","537/537 [==============================] - 0s 51us/step - loss: 18201.8969 - regression_loss: 8937.7171 - val_loss: 2122.4521 - val_regression_loss: 1045.7799\n","Epoch 22/50\n","537/537 [==============================] - 0s 50us/step - loss: 17775.3506 - regression_loss: 8759.2873 - val_loss: 2055.4270 - val_regression_loss: 1014.2538\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 17590.5853 - regression_loss: 8685.1160 - val_loss: 1823.2173 - val_regression_loss: 898.2318\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 15638.5327 - regression_loss: 7711.9160 - val_loss: 1588.2675 - val_regression_loss: 779.7076\n","Epoch 25/50\n","537/537 [==============================] - 0s 56us/step - loss: 14407.1216 - regression_loss: 7090.6639 - val_loss: 1454.7640 - val_regression_loss: 711.7213\n","Epoch 26/50\n","537/537 [==============================] - 0s 52us/step - loss: 13413.0391 - regression_loss: 6588.0028 - val_loss: 1359.2272 - val_regression_loss: 663.2144\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 13250.8198 - regression_loss: 6506.9547 - val_loss: 1252.1423 - val_regression_loss: 609.5633\n","Epoch 28/50\n","537/537 [==============================] - 0s 52us/step - loss: 12249.1557 - regression_loss: 6004.5845 - val_loss: 1178.1566 - val_regression_loss: 572.7890\n","Epoch 29/50\n","537/537 [==============================] - 0s 45us/step - loss: 11051.8345 - regression_loss: 5402.7934 - val_loss: 1126.2031 - val_regression_loss: 546.6183\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 10848.9040 - regression_loss: 5308.2403 - val_loss: 1015.6870 - val_regression_loss: 489.8471\n","Epoch 31/50\n","537/537 [==============================] - 0s 55us/step - loss: 9961.3313 - regression_loss: 4849.3129 - val_loss: 902.1437 - val_regression_loss: 431.0752\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 9273.8134 - regression_loss: 4493.4312 - val_loss: 840.9172 - val_regression_loss: 399.1066\n","Epoch 33/50\n","537/537 [==============================] - 0s 66us/step - loss: 7904.9805 - regression_loss: 3804.0918 - val_loss: 772.3885 - val_regression_loss: 365.6277\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 8259.3295 - regression_loss: 3986.9458 - val_loss: 701.5959 - val_regression_loss: 331.9651\n","Epoch 35/50\n","537/537 [==============================] - 0s 55us/step - loss: 7688.1090 - regression_loss: 3719.5967 - val_loss: 653.2143 - val_regression_loss: 309.1888\n","Epoch 36/50\n","537/537 [==============================] - 0s 57us/step - loss: 6544.0889 - regression_loss: 3162.3744 - val_loss: 610.4861 - val_regression_loss: 287.9798\n","Epoch 37/50\n","537/537 [==============================] - 0s 55us/step - loss: 6794.9213 - regression_loss: 3287.9434 - val_loss: 578.2650 - val_regression_loss: 270.5282\n","Epoch 38/50\n","537/537 [==============================] - 0s 56us/step - loss: 6374.5477 - regression_loss: 3069.0067 - val_loss: 565.8192 - val_regression_loss: 263.1475\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 5925.7306 - regression_loss: 2838.5344 - val_loss: 532.2519 - val_regression_loss: 246.1549\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 5539.1712 - regression_loss: 2643.5847 - val_loss: 498.3258 - val_regression_loss: 229.4335\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 5222.5707 - regression_loss: 2488.7788 - val_loss: 481.6277 - val_regression_loss: 220.8734\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 4937.3776 - regression_loss: 2344.2266 - val_loss: 481.3564 - val_regression_loss: 219.9618\n","Epoch 43/50\n","537/537 [==============================] - 0s 63us/step - loss: 4659.3550 - regression_loss: 2198.7403 - val_loss: 484.5167 - val_regression_loss: 221.1838\n","Epoch 44/50\n","537/537 [==============================] - 0s 58us/step - loss: 4543.3951 - regression_loss: 2137.8950 - val_loss: 467.7477 - val_regression_loss: 213.1946\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 4326.6693 - regression_loss: 2033.9331 - val_loss: 445.0010 - val_regression_loss: 202.5111\n","Epoch 46/50\n","537/537 [==============================] - 0s 55us/step - loss: 4152.7030 - regression_loss: 1954.3470 - val_loss: 433.3236 - val_regression_loss: 196.9207\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 4030.1545 - regression_loss: 1896.0093 - val_loss: 434.5252 - val_regression_loss: 197.1826\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 3849.7617 - regression_loss: 1800.9557 - val_loss: 429.0700 - val_regression_loss: 194.3038\n","Epoch 49/50\n","537/537 [==============================] - 0s 68us/step - loss: 3811.3228 - regression_loss: 1782.7188 - val_loss: 411.1042 - val_regression_loss: 185.3940\n","Epoch 50/50\n","537/537 [==============================] - 0s 72us/step - loss: 3677.8069 - regression_loss: 1715.5817 - val_loss: 390.0244 - val_regression_loss: 175.0649\n","***************************** elapsed_time is:  3.2353312969207764\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 19719.3164 - regression_loss: 9790.7675 - val_loss: 1555.6582 - val_regression_loss: 759.0591\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 14436.0401 - regression_loss: 7111.9603 - val_loss: 1061.9015 - val_regression_loss: 504.8905\n","Epoch 3/50\n","537/537 [==============================] - 0s 49us/step - loss: 9929.5044 - regression_loss: 4804.2481 - val_loss: 696.7039 - val_regression_loss: 310.4323\n","Epoch 4/50\n","537/537 [==============================] - 0s 54us/step - loss: 6167.9719 - regression_loss: 2820.8393 - val_loss: 638.5852 - val_regression_loss: 270.6985\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 5071.7258 - regression_loss: 2179.3623 - val_loss: 440.0892 - val_regression_loss: 179.8433\n","Epoch 6/50\n","537/537 [==============================] - 0s 53us/step - loss: 3612.6987 - regression_loss: 1512.0730 - val_loss: 281.7555 - val_regression_loss: 117.9348\n","Epoch 7/50\n","537/537 [==============================] - 0s 48us/step - loss: 2586.6373 - regression_loss: 1135.0278 - val_loss: 376.8348 - val_regression_loss: 178.6739\n","Epoch 8/50\n","537/537 [==============================] - 0s 44us/step - loss: 3477.9379 - regression_loss: 1685.3689 - val_loss: 408.7310 - val_regression_loss: 196.7435\n","Epoch 9/50\n","537/537 [==============================] - 0s 49us/step - loss: 3632.4178 - regression_loss: 1775.9620 - val_loss: 305.3559 - val_regression_loss: 140.7375\n","Epoch 10/50\n","537/537 [==============================] - 0s 48us/step - loss: 2666.8440 - regression_loss: 1254.0183 - val_loss: 239.0694 - val_regression_loss: 101.3385\n","Epoch 11/50\n","537/537 [==============================] - 0s 47us/step - loss: 2118.8016 - regression_loss: 931.1296 - val_loss: 254.6299 - val_regression_loss: 104.0530\n","Epoch 12/50\n","537/537 [==============================] - 0s 50us/step - loss: 2216.9503 - regression_loss: 938.5140 - val_loss: 266.6694 - val_regression_loss: 107.9854\n","Epoch 13/50\n","537/537 [==============================] - 0s 55us/step - loss: 2312.3122 - regression_loss: 969.4939 - val_loss: 252.2445 - val_regression_loss: 101.3248\n","Epoch 14/50\n","537/537 [==============================] - 0s 49us/step - loss: 2190.2167 - regression_loss: 910.4763 - val_loss: 241.7943 - val_regression_loss: 97.9806\n","Epoch 15/50\n","537/537 [==============================] - 0s 52us/step - loss: 2100.2329 - regression_loss: 881.9816 - val_loss: 229.4313 - val_regression_loss: 94.0854\n","Epoch 16/50\n","537/537 [==============================] - 0s 51us/step - loss: 1973.1550 - regression_loss: 839.2171 - val_loss: 211.0146 - val_regression_loss: 87.2041\n","Epoch 17/50\n","537/537 [==============================] - 0s 46us/step - loss: 1805.6668 - regression_loss: 776.5032 - val_loss: 202.3403 - val_regression_loss: 84.8698\n","Epoch 18/50\n","537/537 [==============================] - 0s 48us/step - loss: 1736.9604 - regression_loss: 763.2514 - val_loss: 207.6721 - val_regression_loss: 88.8117\n","Epoch 19/50\n","537/537 [==============================] - 0s 53us/step - loss: 1697.7143 - regression_loss: 750.9758 - val_loss: 211.8092 - val_regression_loss: 91.1543\n","Epoch 20/50\n","537/537 [==============================] - 0s 45us/step - loss: 1721.6290 - regression_loss: 762.7448 - val_loss: 213.0808 - val_regression_loss: 91.2648\n","Epoch 21/50\n","537/537 [==============================] - 0s 56us/step - loss: 1632.7925 - regression_loss: 710.7385 - val_loss: 218.2531 - val_regression_loss: 92.8345\n","Epoch 22/50\n","537/537 [==============================] - 0s 44us/step - loss: 1659.6504 - regression_loss: 719.1771 - val_loss: 216.0012 - val_regression_loss: 90.4836\n","Epoch 23/50\n","537/537 [==============================] - 0s 54us/step - loss: 1592.9580 - regression_loss: 670.1924 - val_loss: 207.3776 - val_regression_loss: 85.1459\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 1479.3426 - regression_loss: 612.6355 - val_loss: 203.0280 - val_regression_loss: 82.4310\n","Epoch 25/50\n","537/537 [==============================] - 0s 49us/step - loss: 1506.5503 - regression_loss: 622.6521 - val_loss: 203.5425 - val_regression_loss: 82.7398\n","Epoch 26/50\n","537/537 [==============================] - 0s 52us/step - loss: 1566.8843 - regression_loss: 655.3565 - val_loss: 201.7007 - val_regression_loss: 82.3398\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 1521.0193 - regression_loss: 637.2266 - val_loss: 200.2635 - val_regression_loss: 82.3224\n","Epoch 28/50\n","537/537 [==============================] - 0s 53us/step - loss: 1456.1080 - regression_loss: 612.5890 - val_loss: 201.5137 - val_regression_loss: 83.4432\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 1479.8198 - regression_loss: 625.0615 - val_loss: 203.1159 - val_regression_loss: 84.3536\n","Epoch 30/50\n","537/537 [==============================] - 0s 54us/step - loss: 1453.6946 - regression_loss: 614.4018 - val_loss: 204.2964 - val_regression_loss: 84.8006\n","Epoch 31/50\n","537/537 [==============================] - 0s 53us/step - loss: 1411.9246 - regression_loss: 593.4506 - val_loss: 203.0216 - val_regression_loss: 84.0465\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 1425.3421 - regression_loss: 596.4284 - val_loss: 198.6936 - val_regression_loss: 81.9336\n","Epoch 33/50\n","537/537 [==============================] - 0s 52us/step - loss: 1364.2582 - regression_loss: 569.1577 - val_loss: 196.1672 - val_regression_loss: 80.5863\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 1376.0568 - regression_loss: 574.5976 - val_loss: 195.0709 - val_regression_loss: 79.6875\n","Epoch 35/50\n","537/537 [==============================] - 0s 53us/step - loss: 1345.5596 - regression_loss: 555.8274 - val_loss: 195.2359 - val_regression_loss: 79.3815\n","Epoch 36/50\n","537/537 [==============================] - 0s 57us/step - loss: 1341.6529 - regression_loss: 552.4247 - val_loss: 196.4420 - val_regression_loss: 79.6862\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 1330.8010 - regression_loss: 545.0977 - val_loss: 196.7673 - val_regression_loss: 80.0180\n","Epoch 38/50\n","537/537 [==============================] - 0s 55us/step - loss: 1313.1466 - regression_loss: 537.5318 - val_loss: 197.4120 - val_regression_loss: 80.6052\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 1298.7256 - regression_loss: 532.3479 - val_loss: 196.8901 - val_regression_loss: 80.4787\n","Epoch 40/50\n","537/537 [==============================] - 0s 61us/step - loss: 1307.7141 - regression_loss: 539.2915 - val_loss: 196.1096 - val_regression_loss: 80.1073\n","Epoch 41/50\n","537/537 [==============================] - 0s 54us/step - loss: 1283.8985 - regression_loss: 528.5236 - val_loss: 195.1800 - val_regression_loss: 79.6418\n","Epoch 42/50\n","537/537 [==============================] - 0s 63us/step - loss: 1288.9211 - regression_loss: 531.9117 - val_loss: 195.1239 - val_regression_loss: 79.4717\n","Epoch 43/50\n","537/537 [==============================] - 0s 55us/step - loss: 1254.6437 - regression_loss: 513.8156 - val_loss: 195.5404 - val_regression_loss: 79.4882\n","Epoch 44/50\n","537/537 [==============================] - 0s 50us/step - loss: 1276.1333 - regression_loss: 523.8582 - val_loss: 196.2441 - val_regression_loss: 79.6454\n","Epoch 45/50\n","537/537 [==============================] - 0s 52us/step - loss: 1255.6678 - regression_loss: 514.0869 - val_loss: 197.0109 - val_regression_loss: 79.9408\n","Epoch 46/50\n","537/537 [==============================] - 0s 54us/step - loss: 1229.7825 - regression_loss: 497.9931 - val_loss: 196.5308 - val_regression_loss: 79.8310\n","Epoch 47/50\n","537/537 [==============================] - 0s 52us/step - loss: 1246.3884 - regression_loss: 507.0020 - val_loss: 195.8739 - val_regression_loss: 79.7594\n","Epoch 48/50\n","537/537 [==============================] - 0s 55us/step - loss: 1231.2784 - regression_loss: 503.3925 - val_loss: 195.6120 - val_regression_loss: 79.6330\n","Epoch 49/50\n","537/537 [==============================] - 0s 55us/step - loss: 1235.3244 - regression_loss: 503.8808 - val_loss: 195.8778 - val_regression_loss: 79.6057\n","Epoch 50/50\n","537/537 [==============================] - 0s 52us/step - loss: 1236.9960 - regression_loss: 506.6826 - val_loss: 196.0763 - val_regression_loss: 79.5566\n","***************************** elapsed_time is:  3.4635841846466064\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 7867.1580 - regression_loss: 3761.7144 - val_loss: 752.7688 - val_regression_loss: 357.1129\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 5133.1506 - regression_loss: 2442.3268 - val_loss: 485.8483 - val_regression_loss: 229.6572\n","Epoch 3/50\n","537/537 [==============================] - 0s 53us/step - loss: 3459.2364 - regression_loss: 1649.8653 - val_loss: 309.1287 - val_regression_loss: 143.5727\n","Epoch 4/50\n","537/537 [==============================] - 0s 54us/step - loss: 2353.3582 - regression_loss: 1109.4319 - val_loss: 185.4233 - val_regression_loss: 77.2842\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 1542.1220 - regression_loss: 661.7882 - val_loss: 217.1973 - val_regression_loss: 85.2776\n","Epoch 6/50\n","537/537 [==============================] - 0s 49us/step - loss: 1792.1273 - regression_loss: 714.0460 - val_loss: 273.1438 - val_regression_loss: 110.5226\n","Epoch 7/50\n","537/537 [==============================] - 0s 45us/step - loss: 2108.6601 - regression_loss: 853.8105 - val_loss: 214.7453 - val_regression_loss: 84.3580\n","Epoch 8/50\n","537/537 [==============================] - 0s 47us/step - loss: 1631.7204 - regression_loss: 641.0464 - val_loss: 166.3315 - val_regression_loss: 64.8192\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 1268.1089 - regression_loss: 499.5028 - val_loss: 168.8583 - val_regression_loss: 69.8302\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 1318.1073 - regression_loss: 557.3207 - val_loss: 186.9209 - val_regression_loss: 80.7333\n","Epoch 11/50\n","537/537 [==============================] - 0s 54us/step - loss: 1439.9524 - regression_loss: 635.5375 - val_loss: 192.6326 - val_regression_loss: 83.7957\n","Epoch 12/50\n","537/537 [==============================] - 0s 49us/step - loss: 1424.5307 - regression_loss: 626.9105 - val_loss: 183.2355 - val_regression_loss: 78.3272\n","Epoch 13/50\n","537/537 [==============================] - 0s 53us/step - loss: 1349.6546 - regression_loss: 584.2330 - val_loss: 168.4640 - val_regression_loss: 69.4640\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 1222.2461 - regression_loss: 504.5870 - val_loss: 160.9702 - val_regression_loss: 63.7310\n","Epoch 15/50\n","537/537 [==============================] - 0s 49us/step - loss: 1220.0481 - regression_loss: 486.1303 - val_loss: 161.5808 - val_regression_loss: 62.2333\n","Epoch 16/50\n","537/537 [==============================] - 0s 52us/step - loss: 1265.3577 - regression_loss: 488.7328 - val_loss: 163.5105 - val_regression_loss: 62.5967\n","Epoch 17/50\n","537/537 [==============================] - 0s 53us/step - loss: 1259.6899 - regression_loss: 482.2502 - val_loss: 161.1854 - val_regression_loss: 62.1245\n","Epoch 18/50\n","537/537 [==============================] - 0s 53us/step - loss: 1212.2167 - regression_loss: 469.7194 - val_loss: 158.3980 - val_regression_loss: 61.9207\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 1181.5774 - regression_loss: 466.3991 - val_loss: 157.2890 - val_regression_loss: 62.4544\n","Epoch 20/50\n","537/537 [==============================] - 0s 59us/step - loss: 1177.1006 - regression_loss: 474.3566 - val_loss: 157.0891 - val_regression_loss: 63.1258\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 1170.6687 - regression_loss: 480.0448 - val_loss: 156.9631 - val_regression_loss: 63.3758\n","Epoch 22/50\n","537/537 [==============================] - 0s 56us/step - loss: 1173.9827 - regression_loss: 482.0120 - val_loss: 156.1967 - val_regression_loss: 62.8789\n","Epoch 23/50\n","537/537 [==============================] - 0s 56us/step - loss: 1154.0028 - regression_loss: 472.7661 - val_loss: 156.0110 - val_regression_loss: 62.2436\n","Epoch 24/50\n","537/537 [==============================] - 0s 56us/step - loss: 1158.4416 - regression_loss: 468.7043 - val_loss: 157.7350 - val_regression_loss: 62.4180\n","Epoch 25/50\n","537/537 [==============================] - 0s 56us/step - loss: 1159.6777 - regression_loss: 465.2787 - val_loss: 157.9550 - val_regression_loss: 61.9746\n","Epoch 26/50\n","537/537 [==============================] - 0s 55us/step - loss: 1149.5074 - regression_loss: 452.3253 - val_loss: 156.2407 - val_regression_loss: 60.9115\n","Epoch 27/50\n","537/537 [==============================] - 0s 54us/step - loss: 1169.2063 - regression_loss: 462.5102 - val_loss: 153.2235 - val_regression_loss: 59.5658\n","Epoch 28/50\n","537/537 [==============================] - 0s 62us/step - loss: 1146.9387 - regression_loss: 453.6048 - val_loss: 152.4510 - val_regression_loss: 59.5536\n","Epoch 29/50\n","537/537 [==============================] - 0s 66us/step - loss: 1134.0174 - regression_loss: 452.0546 - val_loss: 154.3193 - val_regression_loss: 60.8743\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 1134.4069 - regression_loss: 455.1172 - val_loss: 157.3199 - val_regression_loss: 62.6685\n","Epoch 31/50\n","537/537 [==============================] - 0s 53us/step - loss: 1142.4365 - regression_loss: 460.8608 - val_loss: 156.7743 - val_regression_loss: 62.6282\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 1147.2571 - regression_loss: 468.5103 - val_loss: 155.5933 - val_regression_loss: 62.0269\n","Epoch 33/50\n","537/537 [==============================] - 0s 58us/step - loss: 1128.2046 - regression_loss: 455.9919 - val_loss: 154.6239 - val_regression_loss: 61.3266\n","Epoch 34/50\n","537/537 [==============================] - 0s 58us/step - loss: 1137.1256 - regression_loss: 457.2750 - val_loss: 155.0910 - val_regression_loss: 61.2179\n","Epoch 35/50\n","537/537 [==============================] - 0s 50us/step - loss: 1126.6125 - regression_loss: 451.5036 - val_loss: 156.4828 - val_regression_loss: 61.6771\n","Epoch 36/50\n","537/537 [==============================] - 0s 56us/step - loss: 1139.3917 - regression_loss: 456.0103 - val_loss: 155.6611 - val_regression_loss: 61.2302\n","Epoch 37/50\n","537/537 [==============================] - 0s 62us/step - loss: 1134.9200 - regression_loss: 454.3923 - val_loss: 153.6368 - val_regression_loss: 60.3091\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 1128.1749 - regression_loss: 453.9346 - val_loss: 152.9942 - val_regression_loss: 60.1396\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 1134.8047 - regression_loss: 455.5091 - val_loss: 155.3680 - val_regression_loss: 61.4351\n","Epoch 40/50\n","537/537 [==============================] - 0s 58us/step - loss: 1118.2943 - regression_loss: 449.0484 - val_loss: 157.0307 - val_regression_loss: 62.2841\n","Epoch 41/50\n","537/537 [==============================] - 0s 54us/step - loss: 1116.9585 - regression_loss: 448.8990 - val_loss: 155.4994 - val_regression_loss: 61.4989\n","Epoch 42/50\n","537/537 [==============================] - 0s 54us/step - loss: 1134.9128 - regression_loss: 458.6918 - val_loss: 153.4208 - val_regression_loss: 60.3976\n","Epoch 43/50\n","537/537 [==============================] - 0s 60us/step - loss: 1108.8630 - regression_loss: 446.1479 - val_loss: 153.7993 - val_regression_loss: 60.4576\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 1119.1787 - regression_loss: 447.7633 - val_loss: 155.0805 - val_regression_loss: 61.0021\n","Epoch 45/50\n","537/537 [==============================] - 0s 55us/step - loss: 1099.7498 - regression_loss: 441.6342 - val_loss: 156.2024 - val_regression_loss: 61.5763\n","Epoch 46/50\n","537/537 [==============================] - 0s 57us/step - loss: 1116.4773 - regression_loss: 449.8526 - val_loss: 155.3050 - val_regression_loss: 61.2387\n","Epoch 47/50\n","537/537 [==============================] - 0s 62us/step - loss: 1124.2285 - regression_loss: 449.8122 - val_loss: 154.5895 - val_regression_loss: 60.9884\n","Epoch 48/50\n","537/537 [==============================] - 0s 56us/step - loss: 1093.4199 - regression_loss: 437.6106 - val_loss: 155.5582 - val_regression_loss: 61.4773\n","Epoch 49/50\n","537/537 [==============================] - 0s 55us/step - loss: 1103.4737 - regression_loss: 442.8129 - val_loss: 158.6377 - val_regression_loss: 62.9702\n","Epoch 50/50\n","537/537 [==============================] - 0s 54us/step - loss: 1088.1808 - regression_loss: 438.2352 - val_loss: 154.9384 - val_regression_loss: 61.1489\n","***************************** elapsed_time is:  3.304755210876465\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 19821.0779 - regression_loss: 9735.3912 - val_loss: 2005.6969 - val_regression_loss: 979.5840\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 15093.7328 - regression_loss: 7372.1144 - val_loss: 1398.1934 - val_regression_loss: 675.6747\n","Epoch 3/50\n","537/537 [==============================] - 0s 52us/step - loss: 10386.7558 - regression_loss: 5017.2521 - val_loss: 849.7031 - val_regression_loss: 401.0664\n","Epoch 4/50\n","537/537 [==============================] - 0s 52us/step - loss: 6289.6443 - regression_loss: 2965.7520 - val_loss: 617.0137 - val_regression_loss: 284.7677\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 4889.4783 - regression_loss: 2266.2923 - val_loss: 414.6397 - val_regression_loss: 184.9966\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 3137.2134 - regression_loss: 1402.0513 - val_loss: 272.2729 - val_regression_loss: 115.5581\n","Epoch 7/50\n","537/537 [==============================] - 0s 51us/step - loss: 1872.2856 - regression_loss: 783.2259 - val_loss: 387.7370 - val_regression_loss: 174.6923\n","Epoch 8/50\n","537/537 [==============================] - 0s 46us/step - loss: 2552.7072 - regression_loss: 1134.1036 - val_loss: 479.6918 - val_regression_loss: 221.5875\n","Epoch 9/50\n","537/537 [==============================] - 0s 49us/step - loss: 3164.3756 - regression_loss: 1446.9563 - val_loss: 392.1716 - val_regression_loss: 178.3951\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 2520.0173 - regression_loss: 1128.2275 - val_loss: 279.0806 - val_regression_loss: 122.1407\n","Epoch 11/50\n","537/537 [==============================] - 0s 52us/step - loss: 1754.9815 - regression_loss: 749.7219 - val_loss: 238.3159 - val_regression_loss: 101.8428\n","Epoch 12/50\n","537/537 [==============================] - 0s 52us/step - loss: 1571.9928 - regression_loss: 658.8863 - val_loss: 248.0016 - val_regression_loss: 106.6510\n","Epoch 13/50\n","537/537 [==============================] - 0s 54us/step - loss: 1680.4100 - regression_loss: 710.2234 - val_loss: 262.9390 - val_regression_loss: 114.0947\n","Epoch 14/50\n","537/537 [==============================] - 0s 47us/step - loss: 1758.5036 - regression_loss: 750.5062 - val_loss: 274.8186 - val_regression_loss: 120.0956\n","Epoch 15/50\n","537/537 [==============================] - 0s 46us/step - loss: 1805.8240 - regression_loss: 774.5692 - val_loss: 278.6516 - val_regression_loss: 122.1476\n","Epoch 16/50\n","537/537 [==============================] - 0s 53us/step - loss: 1698.1169 - regression_loss: 720.8416 - val_loss: 265.4027 - val_regression_loss: 115.6894\n","Epoch 17/50\n","537/537 [==============================] - 0s 49us/step - loss: 1585.6868 - regression_loss: 665.7525 - val_loss: 242.4300 - val_regression_loss: 104.3617\n","\n","Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 1434.5499 - regression_loss: 594.4646 - val_loss: 233.2364 - val_regression_loss: 99.8291\n","Epoch 19/50\n","537/537 [==============================] - 0s 67us/step - loss: 1394.7787 - regression_loss: 573.7512 - val_loss: 227.2388 - val_regression_loss: 96.8806\n","Epoch 20/50\n","537/537 [==============================] - 0s 53us/step - loss: 1408.1171 - regression_loss: 578.8422 - val_loss: 224.0505 - val_regression_loss: 95.3229\n","Epoch 21/50\n","537/537 [==============================] - 0s 70us/step - loss: 1392.0074 - regression_loss: 574.6594 - val_loss: 221.9896 - val_regression_loss: 94.3144\n","Epoch 22/50\n","537/537 [==============================] - 0s 48us/step - loss: 1349.9886 - regression_loss: 551.2581 - val_loss: 220.5699 - val_regression_loss: 93.6156\n","Epoch 23/50\n","537/537 [==============================] - 0s 48us/step - loss: 1405.5874 - regression_loss: 580.8863 - val_loss: 219.7226 - val_regression_loss: 93.1921\n","Epoch 24/50\n","537/537 [==============================] - 0s 59us/step - loss: 1372.4220 - regression_loss: 562.8002 - val_loss: 219.2662 - val_regression_loss: 92.9557\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 1371.6766 - regression_loss: 565.4123 - val_loss: 218.2825 - val_regression_loss: 92.4484\n","Epoch 26/50\n","537/537 [==============================] - 0s 53us/step - loss: 1346.9680 - regression_loss: 552.0342 - val_loss: 216.3810 - val_regression_loss: 91.4784\n","Epoch 27/50\n","537/537 [==============================] - 0s 56us/step - loss: 1319.6987 - regression_loss: 538.9710 - val_loss: 213.3057 - val_regression_loss: 89.9202\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 1321.2055 - regression_loss: 539.8363 - val_loss: 209.8580 - val_regression_loss: 88.1768\n","Epoch 29/50\n","537/537 [==============================] - 0s 54us/step - loss: 1314.3389 - regression_loss: 535.1514 - val_loss: 206.7211 - val_regression_loss: 86.5933\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1306.3476 - regression_loss: 530.8059 - val_loss: 204.7730 - val_regression_loss: 85.6120\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 1304.4215 - regression_loss: 532.5239 - val_loss: 203.7598 - val_regression_loss: 85.1058\n","Epoch 32/50\n","537/537 [==============================] - 0s 48us/step - loss: 1282.2699 - regression_loss: 520.9941 - val_loss: 203.7026 - val_regression_loss: 85.0842\n","Epoch 33/50\n","537/537 [==============================] - 0s 56us/step - loss: 1296.2688 - regression_loss: 526.8777 - val_loss: 204.6155 - val_regression_loss: 85.5519\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 1277.1655 - regression_loss: 518.4081 - val_loss: 204.7521 - val_regression_loss: 85.6305\n","Epoch 35/50\n","537/537 [==============================] - 0s 47us/step - loss: 1276.3017 - regression_loss: 517.0196 - val_loss: 203.8086 - val_regression_loss: 85.1690\n","Epoch 36/50\n","537/537 [==============================] - 0s 65us/step - loss: 1279.6335 - regression_loss: 519.3772 - val_loss: 201.6686 - val_regression_loss: 84.1058\n","Epoch 37/50\n","537/537 [==============================] - 0s 55us/step - loss: 1265.0481 - regression_loss: 515.1755 - val_loss: 199.6674 - val_regression_loss: 83.1106\n","Epoch 38/50\n","537/537 [==============================] - 0s 53us/step - loss: 1253.8208 - regression_loss: 507.1000 - val_loss: 197.9551 - val_regression_loss: 82.2555\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 1274.3217 - regression_loss: 518.8783 - val_loss: 196.9828 - val_regression_loss: 81.7701\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 1252.3549 - regression_loss: 506.0285 - val_loss: 196.7524 - val_regression_loss: 81.6551\n","Epoch 41/50\n","537/537 [==============================] - 0s 54us/step - loss: 1258.9756 - regression_loss: 511.5981 - val_loss: 197.2224 - val_regression_loss: 81.8900\n","Epoch 42/50\n","537/537 [==============================] - 0s 59us/step - loss: 1256.4861 - regression_loss: 509.1345 - val_loss: 197.4382 - val_regression_loss: 81.9966\n","Epoch 43/50\n","537/537 [==============================] - 0s 56us/step - loss: 1249.1729 - regression_loss: 507.3634 - val_loss: 197.4376 - val_regression_loss: 81.9950\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 1249.4666 - regression_loss: 506.1296 - val_loss: 196.9512 - val_regression_loss: 81.7502\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 1236.1841 - regression_loss: 500.4497 - val_loss: 196.1781 - val_regression_loss: 81.3643\n","Epoch 46/50\n","537/537 [==============================] - 0s 54us/step - loss: 1240.5201 - regression_loss: 502.3817 - val_loss: 195.3675 - val_regression_loss: 80.9607\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 1240.6752 - regression_loss: 501.5776 - val_loss: 194.7721 - val_regression_loss: 80.6659\n","Epoch 48/50\n","537/537 [==============================] - 0s 50us/step - loss: 1231.4421 - regression_loss: 497.8928 - val_loss: 194.6050 - val_regression_loss: 80.5873\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 1231.2938 - regression_loss: 499.7450 - val_loss: 194.9562 - val_regression_loss: 80.7697\n","Epoch 50/50\n","537/537 [==============================] - 0s 52us/step - loss: 1218.1104 - regression_loss: 490.7611 - val_loss: 195.0260 - val_regression_loss: 80.8111\n","***************************** elapsed_time is:  3.3317325115203857\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 204097.8834 - regression_loss: 101870.7189 - val_loss: 23188.7422 - val_regression_loss: 11571.0410\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 183346.5272 - regression_loss: 91500.0481 - val_loss: 20144.3945 - val_regression_loss: 10049.3867\n","Epoch 3/50\n","537/537 [==============================] - 0s 46us/step - loss: 159319.1621 - regression_loss: 79490.5564 - val_loss: 16053.2920 - val_regression_loss: 8003.8677\n","Epoch 4/50\n","537/537 [==============================] - 0s 49us/step - loss: 126049.1853 - regression_loss: 62855.7271 - val_loss: 11008.7588 - val_regression_loss: 5480.8271\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 86244.0963 - regression_loss: 42946.7408 - val_loss: 5868.0464 - val_regression_loss: 2908.4844\n","Epoch 6/50\n","537/537 [==============================] - 0s 50us/step - loss: 44944.3857 - regression_loss: 22280.9469 - val_loss: 2492.6975 - val_regression_loss: 1217.3925\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 18545.8219 - regression_loss: 9053.4597 - val_loss: 2806.1968 - val_regression_loss: 1370.6885\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 22062.5393 - regression_loss: 10784.4519 - val_loss: 3918.3262 - val_regression_loss: 1927.4523\n","Epoch 9/50\n","537/537 [==============================] - 0s 52us/step - loss: 31750.5879 - regression_loss: 15634.0908 - val_loss: 3293.2932 - val_regression_loss: 1618.7274\n","Epoch 10/50\n","537/537 [==============================] - 0s 59us/step - loss: 25645.8830 - regression_loss: 12613.9976 - val_loss: 2031.0988 - val_regression_loss: 991.6327\n","Epoch 11/50\n","537/537 [==============================] - 0s 47us/step - loss: 15455.1153 - regression_loss: 7550.5184 - val_loss: 1339.8636 - val_regression_loss: 649.0542\n","Epoch 12/50\n","537/537 [==============================] - 0s 43us/step - loss: 9130.5357 - regression_loss: 4411.7576 - val_loss: 1400.5627 - val_regression_loss: 681.3669\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 9197.1866 - regression_loss: 4459.9778 - val_loss: 1734.9194 - val_regression_loss: 849.6686\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 12151.9995 - regression_loss: 5947.9869 - val_loss: 1904.4661 - val_regression_loss: 934.9861\n","Epoch 15/50\n","537/537 [==============================] - 0s 51us/step - loss: 13745.5716 - regression_loss: 6748.3195 - val_loss: 1784.8490 - val_regression_loss: 875.3407\n","Epoch 16/50\n","537/537 [==============================] - 0s 53us/step - loss: 12496.9319 - regression_loss: 6125.1997 - val_loss: 1474.4081 - val_regression_loss: 720.0410\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 10400.9527 - regression_loss: 5074.2247 - val_loss: 1152.8665 - val_regression_loss: 559.0504\n","\n","Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 18/50\n","537/537 [==============================] - 0s 57us/step - loss: 7676.6157 - regression_loss: 3710.3691 - val_loss: 1041.4454 - val_regression_loss: 503.2146\n","Epoch 19/50\n","537/537 [==============================] - 0s 57us/step - loss: 6964.7459 - regression_loss: 3354.7999 - val_loss: 978.7294 - val_regression_loss: 471.7495\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 6423.0962 - regression_loss: 3081.8140 - val_loss: 964.7904 - val_regression_loss: 464.7049\n","Epoch 21/50\n","537/537 [==============================] - 0s 63us/step - loss: 6327.3387 - regression_loss: 3034.0807 - val_loss: 982.2935 - val_regression_loss: 473.4195\n","Epoch 22/50\n","537/537 [==============================] - 0s 48us/step - loss: 6380.7628 - regression_loss: 3061.0287 - val_loss: 1006.7411 - val_regression_loss: 485.6393\n","Epoch 23/50\n","537/537 [==============================] - 0s 58us/step - loss: 6522.4582 - regression_loss: 3130.7147 - val_loss: 1018.0596 - val_regression_loss: 491.3197\n","Epoch 24/50\n","537/537 [==============================] - 0s 54us/step - loss: 6637.0582 - regression_loss: 3189.9017 - val_loss: 1008.1536 - val_regression_loss: 486.4003\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 6404.3845 - regression_loss: 3075.4267 - val_loss: 981.4760 - val_regression_loss: 473.1059\n","Epoch 26/50\n","537/537 [==============================] - 0s 54us/step - loss: 6129.7014 - regression_loss: 2936.9903 - val_loss: 949.1740 - val_regression_loss: 456.9984\n","Epoch 27/50\n","537/537 [==============================] - 0s 66us/step - loss: 5973.0308 - regression_loss: 2857.4498 - val_loss: 922.0605 - val_regression_loss: 443.4819\n","Epoch 28/50\n","537/537 [==============================] - 0s 61us/step - loss: 5838.8169 - regression_loss: 2789.1350 - val_loss: 904.8740 - val_regression_loss: 434.9251\n","Epoch 29/50\n","537/537 [==============================] - 0s 55us/step - loss: 5451.8629 - regression_loss: 2597.5134 - val_loss: 896.8552 - val_regression_loss: 430.9468\n","Epoch 30/50\n","537/537 [==============================] - 0s 56us/step - loss: 5246.3660 - regression_loss: 2494.8079 - val_loss: 893.3585 - val_regression_loss: 429.2237\n","Epoch 31/50\n","537/537 [==============================] - 0s 57us/step - loss: 5486.4685 - regression_loss: 2614.1736 - val_loss: 889.5967 - val_regression_loss: 427.3609\n","Epoch 32/50\n","537/537 [==============================] - 0s 52us/step - loss: 5399.9753 - regression_loss: 2571.9682 - val_loss: 882.9731 - val_regression_loss: 424.0595\n","Epoch 33/50\n","537/537 [==============================] - 0s 56us/step - loss: 5325.4314 - regression_loss: 2535.8381 - val_loss: 873.6678 - val_regression_loss: 419.4102\n","Epoch 34/50\n","537/537 [==============================] - 0s 50us/step - loss: 5343.8854 - regression_loss: 2542.9623 - val_loss: 863.0516 - val_regression_loss: 414.0969\n","Epoch 35/50\n","537/537 [==============================] - 0s 49us/step - loss: 5341.0984 - regression_loss: 2543.2714 - val_loss: 852.9119 - val_regression_loss: 409.0131\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 5099.8337 - regression_loss: 2423.2477 - val_loss: 848.5402 - val_regression_loss: 406.8174\n","Epoch 37/50\n","537/537 [==============================] - 0s 57us/step - loss: 5104.6770 - regression_loss: 2425.3572 - val_loss: 844.3251 - val_regression_loss: 404.6971\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 5013.9820 - regression_loss: 2380.9377 - val_loss: 840.2906 - val_regression_loss: 402.6661\n","Epoch 39/50\n","537/537 [==============================] - 0s 50us/step - loss: 4920.3314 - regression_loss: 2331.3408 - val_loss: 836.2747 - val_regression_loss: 400.6436\n","Epoch 40/50\n","537/537 [==============================] - 0s 53us/step - loss: 4978.8624 - regression_loss: 2360.8972 - val_loss: 832.0796 - val_regression_loss: 398.5290\n","Epoch 41/50\n","537/537 [==============================] - 0s 51us/step - loss: 4959.9602 - regression_loss: 2353.0998 - val_loss: 827.6857 - val_regression_loss: 396.3157\n","Epoch 42/50\n","537/537 [==============================] - 0s 47us/step - loss: 4943.2817 - regression_loss: 2342.8792 - val_loss: 822.9114 - val_regression_loss: 393.9137\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 4730.4907 - regression_loss: 2237.2572 - val_loss: 817.7331 - val_regression_loss: 391.3109\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 4823.4964 - regression_loss: 2287.8297 - val_loss: 812.4206 - val_regression_loss: 388.6436\n","Epoch 45/50\n","537/537 [==============================] - 0s 54us/step - loss: 4799.6095 - regression_loss: 2272.2022 - val_loss: 807.0087 - val_regression_loss: 385.9286\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 4727.9172 - regression_loss: 2237.2639 - val_loss: 801.6741 - val_regression_loss: 383.2529\n","Epoch 47/50\n","537/537 [==============================] - 0s 53us/step - loss: 4720.5326 - regression_loss: 2232.0679 - val_loss: 796.3562 - val_regression_loss: 380.5900\n","Epoch 48/50\n","537/537 [==============================] - 0s 58us/step - loss: 4704.1239 - regression_loss: 2226.3486 - val_loss: 791.3239 - val_regression_loss: 378.0721\n","Epoch 49/50\n","537/537 [==============================] - 0s 58us/step - loss: 4595.8763 - regression_loss: 2168.5610 - val_loss: 786.2332 - val_regression_loss: 375.5273\n","Epoch 50/50\n","537/537 [==============================] - 0s 60us/step - loss: 4408.7481 - regression_loss: 2073.2237 - val_loss: 781.2343 - val_regression_loss: 373.0306\n","***************************** elapsed_time is:  3.270911693572998\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 23913.4018 - regression_loss: 11765.8011 - val_loss: 2025.0366 - val_regression_loss: 989.0450\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 18015.3691 - regression_loss: 8837.2522 - val_loss: 1355.5073 - val_regression_loss: 656.6766\n","Epoch 3/50\n","537/537 [==============================] - 0s 51us/step - loss: 12443.7536 - regression_loss: 6069.6919 - val_loss: 777.0482 - val_regression_loss: 369.9584\n","Epoch 4/50\n","537/537 [==============================] - 0s 50us/step - loss: 7311.3923 - regression_loss: 3523.4060 - val_loss: 614.0580 - val_regression_loss: 290.8664\n","Epoch 5/50\n","537/537 [==============================] - 0s 51us/step - loss: 5566.6955 - regression_loss: 2669.8382 - val_loss: 523.6425 - val_regression_loss: 245.8570\n","Epoch 6/50\n","537/537 [==============================] - 0s 52us/step - loss: 4344.9643 - regression_loss: 2060.4682 - val_loss: 357.7665 - val_regression_loss: 161.1974\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 2645.9658 - regression_loss: 1194.7370 - val_loss: 377.2098 - val_regression_loss: 168.7660\n","Epoch 8/50\n","537/537 [==============================] - 0s 54us/step - loss: 2776.1997 - regression_loss: 1241.5532 - val_loss: 472.9435 - val_regression_loss: 215.6413\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 3560.3234 - regression_loss: 1624.8728 - val_loss: 432.1581 - val_regression_loss: 195.6493\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 3357.7803 - regression_loss: 1529.7231 - val_loss: 308.2940 - val_regression_loss: 134.7842\n","Epoch 11/50\n","537/537 [==============================] - 0s 58us/step - loss: 2484.4260 - regression_loss: 1103.2240 - val_loss: 226.2051 - val_regression_loss: 94.9115\n","Epoch 12/50\n","537/537 [==============================] - 0s 58us/step - loss: 1856.0551 - regression_loss: 800.6802 - val_loss: 225.6075 - val_regression_loss: 95.5779\n","Epoch 13/50\n","537/537 [==============================] - 0s 68us/step - loss: 1906.9713 - regression_loss: 834.5184 - val_loss: 250.4082 - val_regression_loss: 108.5658\n","Epoch 14/50\n","537/537 [==============================] - 0s 57us/step - loss: 2182.7518 - regression_loss: 975.2018 - val_loss: 242.3281 - val_regression_loss: 104.7008\n","Epoch 15/50\n","537/537 [==============================] - 0s 62us/step - loss: 2154.8988 - regression_loss: 964.2018 - val_loss: 216.4299 - val_regression_loss: 91.5838\n","Epoch 16/50\n","537/537 [==============================] - 0s 54us/step - loss: 1960.8751 - regression_loss: 865.6249 - val_loss: 202.5013 - val_regression_loss: 84.2432\n","Epoch 17/50\n","537/537 [==============================] - 0s 52us/step - loss: 1808.3393 - regression_loss: 783.0600 - val_loss: 201.3941 - val_regression_loss: 83.2327\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 1750.5231 - regression_loss: 752.4427 - val_loss: 206.8913 - val_regression_loss: 85.5025\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 1663.5590 - regression_loss: 707.2902 - val_loss: 221.5408 - val_regression_loss: 92.3746\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 1590.5398 - regression_loss: 662.6108 - val_loss: 237.3143 - val_regression_loss: 99.9929\n","Epoch 21/50\n","537/537 [==============================] - 0s 58us/step - loss: 1633.1662 - regression_loss: 683.7118 - val_loss: 239.6203 - val_regression_loss: 101.1797\n","Epoch 22/50\n","537/537 [==============================] - 0s 57us/step - loss: 1652.5424 - regression_loss: 694.6931 - val_loss: 226.8671 - val_regression_loss: 95.0854\n","Epoch 23/50\n","537/537 [==============================] - 0s 57us/step - loss: 1546.2059 - regression_loss: 641.8487 - val_loss: 214.7630 - val_regression_loss: 89.3677\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 1545.6150 - regression_loss: 644.9868 - val_loss: 210.9179 - val_regression_loss: 87.7080\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 1496.1422 - regression_loss: 623.7951 - val_loss: 211.9526 - val_regression_loss: 88.4185\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 1491.0244 - regression_loss: 626.1375 - val_loss: 217.0655 - val_regression_loss: 91.1029\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 1508.2734 - regression_loss: 631.2435 - val_loss: 223.7421 - val_regression_loss: 94.4730\n","Epoch 28/50\n","537/537 [==============================] - 0s 55us/step - loss: 1460.5714 - regression_loss: 609.9225 - val_loss: 224.8472 - val_regression_loss: 94.9351\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 1453.1695 - regression_loss: 603.0647 - val_loss: 223.5536 - val_regression_loss: 94.1345\n","Epoch 30/50\n","537/537 [==============================] - 0s 56us/step - loss: 1407.2836 - regression_loss: 579.1851 - val_loss: 224.6002 - val_regression_loss: 94.5108\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 1407.8772 - regression_loss: 579.4335 - val_loss: 227.9956 - val_regression_loss: 96.1224\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 1430.4979 - regression_loss: 590.0742 - val_loss: 232.6058 - val_regression_loss: 98.4346\n","Epoch 33/50\n","537/537 [==============================] - 0s 53us/step - loss: 1434.7293 - regression_loss: 592.5760 - val_loss: 236.3317 - val_regression_loss: 100.3720\n","Epoch 34/50\n","537/537 [==============================] - 0s 53us/step - loss: 1418.5973 - regression_loss: 584.0399 - val_loss: 236.5362 - val_regression_loss: 100.5842\n","Epoch 35/50\n","537/537 [==============================] - 0s 56us/step - loss: 1399.0762 - regression_loss: 577.3364 - val_loss: 234.6711 - val_regression_loss: 99.7644\n","Epoch 36/50\n","537/537 [==============================] - 0s 55us/step - loss: 1382.9009 - regression_loss: 568.4530 - val_loss: 233.2764 - val_regression_loss: 99.1635\n","Epoch 37/50\n","537/537 [==============================] - 0s 58us/step - loss: 1367.3708 - regression_loss: 561.8297 - val_loss: 233.7458 - val_regression_loss: 99.4716\n","Epoch 38/50\n","537/537 [==============================] - 0s 55us/step - loss: 1365.7636 - regression_loss: 561.6235 - val_loss: 234.6301 - val_regression_loss: 99.9435\n","Epoch 39/50\n","537/537 [==============================] - 0s 61us/step - loss: 1377.0918 - regression_loss: 568.9991 - val_loss: 234.6796 - val_regression_loss: 99.9515\n","Epoch 40/50\n","537/537 [==============================] - 0s 55us/step - loss: 1348.2032 - regression_loss: 554.9458 - val_loss: 234.6373 - val_regression_loss: 99.8899\n","Epoch 41/50\n","537/537 [==============================] - 0s 51us/step - loss: 1348.7505 - regression_loss: 556.1156 - val_loss: 234.4904 - val_regression_loss: 99.7640\n","Epoch 42/50\n","537/537 [==============================] - 0s 55us/step - loss: 1339.3527 - regression_loss: 551.3943 - val_loss: 234.9319 - val_regression_loss: 99.9737\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 1330.7108 - regression_loss: 545.7511 - val_loss: 236.1458 - val_regression_loss: 100.6161\n","Epoch 44/50\n","537/537 [==============================] - 0s 53us/step - loss: 1324.5351 - regression_loss: 542.6108 - val_loss: 235.3243 - val_regression_loss: 100.2429\n","Epoch 45/50\n","537/537 [==============================] - 0s 55us/step - loss: 1324.7722 - regression_loss: 542.2649 - val_loss: 235.6189 - val_regression_loss: 100.4514\n","Epoch 46/50\n","537/537 [==============================] - 0s 55us/step - loss: 1299.4415 - regression_loss: 532.1169 - val_loss: 235.0316 - val_regression_loss: 100.1927\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 1312.3691 - regression_loss: 536.4240 - val_loss: 235.0644 - val_regression_loss: 100.2359\n","Epoch 48/50\n","537/537 [==============================] - 0s 58us/step - loss: 1297.0200 - regression_loss: 531.0247 - val_loss: 234.5552 - val_regression_loss: 99.9707\n","Epoch 49/50\n","537/537 [==============================] - 0s 51us/step - loss: 1296.3185 - regression_loss: 530.3336 - val_loss: 235.1969 - val_regression_loss: 100.2833\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 1312.3800 - regression_loss: 539.0877 - val_loss: 234.2440 - val_regression_loss: 99.7816\n","***************************** elapsed_time is:  3.587289810180664\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 778952.0327 - regression_loss: 388860.7805 - val_loss: 98943.7188 - val_regression_loss: 49395.4531\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 730951.6699 - regression_loss: 364908.2126 - val_loss: 91829.1406 - val_regression_loss: 45843.8750\n","Epoch 3/50\n","537/537 [==============================] - 0s 50us/step - loss: 674894.9754 - regression_loss: 336930.0470 - val_loss: 81842.8984 - val_regression_loss: 40858.0977\n","Epoch 4/50\n","537/537 [==============================] - 0s 52us/step - loss: 605941.1846 - regression_loss: 302499.6521 - val_loss: 68367.2734 - val_regression_loss: 34129.7891\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 500000.9174 - regression_loss: 249606.7856 - val_loss: 51875.6406 - val_regression_loss: 25895.5586\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 371056.5585 - regression_loss: 185222.8752 - val_loss: 34288.4531 - val_regression_loss: 17114.7344\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 241327.5584 - regression_loss: 120459.5549 - val_loss: 19525.9043 - val_regression_loss: 9745.7949\n","Epoch 8/50\n","537/537 [==============================] - 0s 48us/step - loss: 136339.3312 - regression_loss: 68064.0121 - val_loss: 13410.7119 - val_regression_loss: 6697.9800\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 105324.1079 - regression_loss: 52632.5282 - val_loss: 16361.2012 - val_regression_loss: 8176.5405\n","Epoch 10/50\n","537/537 [==============================] - 0s 52us/step - loss: 147521.5090 - regression_loss: 73756.9726 - val_loss: 15843.5625 - val_regression_loss: 7913.2944\n","Epoch 11/50\n","537/537 [==============================] - 0s 46us/step - loss: 146688.5889 - regression_loss: 73301.5514 - val_loss: 11747.6465 - val_regression_loss: 5858.2974\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 103993.8252 - regression_loss: 51894.5092 - val_loss: 9211.2891 - val_regression_loss: 4584.5166\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 73684.9061 - regression_loss: 36696.9015 - val_loss: 9698.3486 - val_regression_loss: 4824.7002\n","Epoch 14/50\n","537/537 [==============================] - 0s 46us/step - loss: 66606.8921 - regression_loss: 33135.7629 - val_loss: 11678.7783 - val_regression_loss: 5813.0303\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 73231.1057 - regression_loss: 36437.8740 - val_loss: 13331.3291 - val_regression_loss: 6638.4502\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 80944.8922 - regression_loss: 40287.7626 - val_loss: 13693.2090 - val_regression_loss: 6819.5186\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 87610.3360 - regression_loss: 43622.1065 - val_loss: 12752.2021 - val_regression_loss: 6350.0566\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 81173.1731 - regression_loss: 40411.5212 - val_loss: 11080.9131 - val_regression_loss: 5516.1157\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 69455.5248 - regression_loss: 34566.6246 - val_loss: 9368.5371 - val_regression_loss: 4661.9468\n","\n","Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 58454.2771 - regression_loss: 29084.1048 - val_loss: 8693.9922 - val_regression_loss: 4325.6851\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 55674.9476 - regression_loss: 27701.0800 - val_loss: 8191.6660 - val_regression_loss: 4075.4724\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 55569.7621 - regression_loss: 27653.0596 - val_loss: 7851.2178 - val_regression_loss: 3906.0808\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 55943.9036 - regression_loss: 27846.2351 - val_loss: 7633.5952 - val_regression_loss: 3797.9302\n","Epoch 24/50\n","537/537 [==============================] - 0s 52us/step - loss: 53824.2594 - regression_loss: 26795.2135 - val_loss: 7489.8813 - val_regression_loss: 3726.5156\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 56235.4407 - regression_loss: 28001.4665 - val_loss: 7372.5449 - val_regression_loss: 3668.0879\n","Epoch 26/50\n","537/537 [==============================] - 0s 48us/step - loss: 52518.8315 - regression_loss: 26145.2447 - val_loss: 7277.4541 - val_regression_loss: 3620.5725\n","Epoch 27/50\n","537/537 [==============================] - 0s 53us/step - loss: 51494.3047 - regression_loss: 25635.3464 - val_loss: 7212.4683 - val_regression_loss: 3587.9275\n","Epoch 28/50\n","537/537 [==============================] - 0s 52us/step - loss: 50074.2569 - regression_loss: 24922.9005 - val_loss: 7183.0068 - val_regression_loss: 3572.9287\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 51146.0716 - regression_loss: 25454.6110 - val_loss: 7177.9038 - val_regression_loss: 3570.0520\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 49736.1218 - regression_loss: 24750.7129 - val_loss: 7182.1763 - val_regression_loss: 3571.8442\n","Epoch 31/50\n","537/537 [==============================] - 0s 52us/step - loss: 48879.1594 - regression_loss: 24316.6343 - val_loss: 7168.0181 - val_regression_loss: 3564.4575\n","Epoch 32/50\n","537/537 [==============================] - 0s 45us/step - loss: 47057.4278 - regression_loss: 23404.7038 - val_loss: 7108.7183 - val_regression_loss: 3534.5635\n","Epoch 33/50\n","537/537 [==============================] - 0s 54us/step - loss: 47215.2239 - regression_loss: 23481.9805 - val_loss: 6997.7188 - val_regression_loss: 3478.8997\n","Epoch 34/50\n","537/537 [==============================] - 0s 55us/step - loss: 46485.0640 - regression_loss: 23114.8111 - val_loss: 6839.9731 - val_regression_loss: 3399.9424\n","Epoch 35/50\n","537/537 [==============================] - 0s 58us/step - loss: 46163.7629 - regression_loss: 22953.4246 - val_loss: 6652.1606 - val_regression_loss: 3306.0176\n","Epoch 36/50\n","537/537 [==============================] - 0s 55us/step - loss: 43972.1720 - regression_loss: 21858.0813 - val_loss: 6452.3672 - val_regression_loss: 3206.1580\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 43797.4753 - regression_loss: 21770.6297 - val_loss: 6259.7607 - val_regression_loss: 3109.9143\n","Epoch 38/50\n","537/537 [==============================] - 0s 50us/step - loss: 42159.9703 - regression_loss: 20953.6625 - val_loss: 6077.7407 - val_regression_loss: 3018.9893\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 42358.9980 - regression_loss: 21049.5375 - val_loss: 5911.7539 - val_regression_loss: 2936.0854\n","Epoch 40/50\n","537/537 [==============================] - 0s 52us/step - loss: 39680.4647 - regression_loss: 19712.1396 - val_loss: 5770.6309 - val_regression_loss: 2865.6040\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 39471.5120 - regression_loss: 19609.6665 - val_loss: 5648.7046 - val_regression_loss: 2804.6956\n","Epoch 42/50\n","537/537 [==============================] - 0s 62us/step - loss: 38171.2544 - regression_loss: 18958.6899 - val_loss: 5541.6055 - val_regression_loss: 2751.1726\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 35458.3378 - regression_loss: 17600.5921 - val_loss: 5442.2256 - val_regression_loss: 2701.5012\n","Epoch 44/50\n","537/537 [==============================] - 0s 54us/step - loss: 36962.4854 - regression_loss: 18353.7531 - val_loss: 5337.4810 - val_regression_loss: 2649.1470\n","Epoch 45/50\n","537/537 [==============================] - 0s 59us/step - loss: 35988.4329 - regression_loss: 17864.6905 - val_loss: 5231.4478 - val_regression_loss: 2596.1489\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 33617.3907 - regression_loss: 16679.8620 - val_loss: 5124.0786 - val_regression_loss: 2542.4814\n","Epoch 47/50\n","537/537 [==============================] - 0s 53us/step - loss: 31154.5766 - regression_loss: 15450.3602 - val_loss: 5007.2485 - val_regression_loss: 2484.0942\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 32253.5679 - regression_loss: 15997.8907 - val_loss: 4869.7900 - val_regression_loss: 2415.4197\n","Epoch 49/50\n","537/537 [==============================] - 0s 52us/step - loss: 31342.7250 - regression_loss: 15541.1659 - val_loss: 4723.7446 - val_regression_loss: 2342.4614\n","Epoch 50/50\n","537/537 [==============================] - 0s 54us/step - loss: 28979.1667 - regression_loss: 14364.6070 - val_loss: 4584.6470 - val_regression_loss: 2272.9463\n","***************************** elapsed_time is:  3.265089988708496\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 20597.7653 - regression_loss: 10105.4596 - val_loss: 1911.7971 - val_regression_loss: 932.0416\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 15567.2864 - regression_loss: 7605.3639 - val_loss: 1359.0615 - val_regression_loss: 657.8882\n","Epoch 3/50\n","537/537 [==============================] - 0s 52us/step - loss: 10957.5750 - regression_loss: 5318.8322 - val_loss: 827.3832 - val_regression_loss: 394.4182\n","Epoch 4/50\n","537/537 [==============================] - 0s 54us/step - loss: 6443.3341 - regression_loss: 3080.1646 - val_loss: 587.5903 - val_regression_loss: 276.9038\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 4447.0078 - regression_loss: 2099.3813 - val_loss: 533.7844 - val_regression_loss: 250.6672\n","Epoch 6/50\n","537/537 [==============================] - 0s 53us/step - loss: 4109.7227 - regression_loss: 1934.6751 - val_loss: 392.3660 - val_regression_loss: 178.8001\n","Epoch 7/50\n","537/537 [==============================] - 0s 56us/step - loss: 3272.7868 - regression_loss: 1503.1492 - val_loss: 308.6189 - val_regression_loss: 135.5514\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 2759.5325 - regression_loss: 1234.2058 - val_loss: 333.4235 - val_regression_loss: 147.3387\n","Epoch 9/50\n","537/537 [==============================] - 0s 53us/step - loss: 2783.1917 - regression_loss: 1241.8112 - val_loss: 373.3571 - val_regression_loss: 167.6092\n","Epoch 10/50\n","537/537 [==============================] - 0s 57us/step - loss: 2917.3452 - regression_loss: 1312.1813 - val_loss: 355.1745 - val_regression_loss: 159.3750\n","Epoch 11/50\n","537/537 [==============================] - 0s 54us/step - loss: 2628.9628 - regression_loss: 1178.2320 - val_loss: 297.0413 - val_regression_loss: 131.3216\n","Epoch 12/50\n","537/537 [==============================] - 0s 56us/step - loss: 2171.1734 - regression_loss: 956.8113 - val_loss: 248.3163 - val_regression_loss: 107.8577\n","Epoch 13/50\n","537/537 [==============================] - 0s 54us/step - loss: 1798.1019 - regression_loss: 778.2744 - val_loss: 243.9462 - val_regression_loss: 106.2839\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 1834.7141 - regression_loss: 799.2870 - val_loss: 264.5959 - val_regression_loss: 116.8513\n","Epoch 15/50\n","537/537 [==============================] - 0s 51us/step - loss: 2008.7715 - regression_loss: 887.7426 - val_loss: 263.3075 - val_regression_loss: 116.1387\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 1912.1413 - regression_loss: 837.2262 - val_loss: 242.3433 - val_regression_loss: 105.4253\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 1743.2860 - regression_loss: 752.0209 - val_loss: 234.2357 - val_regression_loss: 101.0766\n","Epoch 18/50\n","537/537 [==============================] - 0s 56us/step - loss: 1576.8748 - regression_loss: 665.3938 - val_loss: 246.8821 - val_regression_loss: 107.1257\n","Epoch 19/50\n","537/537 [==============================] - 0s 55us/step - loss: 1638.4688 - regression_loss: 692.3167 - val_loss: 259.1177 - val_regression_loss: 113.0423\n","Epoch 20/50\n","537/537 [==============================] - 0s 51us/step - loss: 1635.4483 - regression_loss: 689.5261 - val_loss: 257.2603 - val_regression_loss: 112.0218\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 1645.2401 - regression_loss: 695.2393 - val_loss: 245.5678 - val_regression_loss: 106.2052\n","Epoch 22/50\n","537/537 [==============================] - 0s 59us/step - loss: 1570.6855 - regression_loss: 659.0822 - val_loss: 236.8435 - val_regression_loss: 101.9894\n","Epoch 23/50\n","537/537 [==============================] - 0s 69us/step - loss: 1462.1592 - regression_loss: 606.0728 - val_loss: 236.1904 - val_regression_loss: 101.8926\n","Epoch 24/50\n","537/537 [==============================] - 0s 59us/step - loss: 1486.9234 - regression_loss: 620.8241 - val_loss: 239.9422 - val_regression_loss: 104.0187\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 1429.7378 - regression_loss: 591.9318 - val_loss: 245.4413 - val_regression_loss: 106.9416\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 1431.4132 - regression_loss: 595.2288 - val_loss: 252.7338 - val_regression_loss: 110.6418\n","Epoch 27/50\n","537/537 [==============================] - 0s 53us/step - loss: 1419.1292 - regression_loss: 590.0690 - val_loss: 258.6433 - val_regression_loss: 113.5476\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 1417.0868 - regression_loss: 590.2038 - val_loss: 259.1042 - val_regression_loss: 113.6870\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 1384.0225 - regression_loss: 574.4125 - val_loss: 255.1302 - val_regression_loss: 111.6065\n","Epoch 30/50\n","537/537 [==============================] - 0s 56us/step - loss: 1339.5305 - regression_loss: 551.3683 - val_loss: 251.0052 - val_regression_loss: 109.4685\n","Epoch 31/50\n","537/537 [==============================] - 0s 52us/step - loss: 1340.6503 - regression_loss: 547.6918 - val_loss: 248.3337 - val_regression_loss: 108.1120\n","Epoch 32/50\n","537/537 [==============================] - 0s 53us/step - loss: 1349.1937 - regression_loss: 550.9337 - val_loss: 245.4334 - val_regression_loss: 106.6912\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 1327.4672 - regression_loss: 543.2218 - val_loss: 242.8409 - val_regression_loss: 105.4694\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 1299.6320 - regression_loss: 530.2111 - val_loss: 240.5928 - val_regression_loss: 104.4330\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 1297.1141 - regression_loss: 529.3960 - val_loss: 237.7989 - val_regression_loss: 103.1151\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 1292.0414 - regression_loss: 529.3130 - val_loss: 233.3930 - val_regression_loss: 100.9533\n","Epoch 37/50\n","537/537 [==============================] - 0s 49us/step - loss: 1301.0703 - regression_loss: 533.1927 - val_loss: 227.9993 - val_regression_loss: 98.2645\n","Epoch 38/50\n","537/537 [==============================] - 0s 57us/step - loss: 1290.5378 - regression_loss: 529.2150 - val_loss: 221.4599 - val_regression_loss: 94.9661\n","Epoch 39/50\n","537/537 [==============================] - 0s 47us/step - loss: 1292.5850 - regression_loss: 529.9972 - val_loss: 215.2118 - val_regression_loss: 91.7742\n","Epoch 40/50\n","537/537 [==============================] - 0s 51us/step - loss: 1267.3593 - regression_loss: 516.7255 - val_loss: 210.5419 - val_regression_loss: 89.3895\n","Epoch 41/50\n","537/537 [==============================] - 0s 52us/step - loss: 1256.9638 - regression_loss: 510.2586 - val_loss: 205.9738 - val_regression_loss: 87.0928\n","Epoch 42/50\n","537/537 [==============================] - 0s 48us/step - loss: 1239.3222 - regression_loss: 502.4140 - val_loss: 201.8419 - val_regression_loss: 85.0399\n","Epoch 43/50\n","537/537 [==============================] - 0s 58us/step - loss: 1257.7995 - regression_loss: 513.2074 - val_loss: 198.2742 - val_regression_loss: 83.2706\n","Epoch 44/50\n","537/537 [==============================] - 0s 53us/step - loss: 1250.9035 - regression_loss: 509.6735 - val_loss: 194.8452 - val_regression_loss: 81.5756\n","Epoch 45/50\n","537/537 [==============================] - 0s 46us/step - loss: 1246.7375 - regression_loss: 508.4893 - val_loss: 191.3527 - val_regression_loss: 79.8115\n","Epoch 46/50\n","537/537 [==============================] - 0s 49us/step - loss: 1235.8487 - regression_loss: 506.6133 - val_loss: 187.6868 - val_regression_loss: 77.9299\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 1212.5473 - regression_loss: 493.4106 - val_loss: 184.5667 - val_regression_loss: 76.3128\n","Epoch 48/50\n","537/537 [==============================] - 0s 50us/step - loss: 1213.0400 - regression_loss: 491.3615 - val_loss: 181.7535 - val_regression_loss: 74.8492\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 1197.8531 - regression_loss: 485.8823 - val_loss: 179.5092 - val_regression_loss: 73.6920\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 1236.9771 - regression_loss: 504.2553 - val_loss: 177.9254 - val_regression_loss: 72.8787\n","***************************** elapsed_time is:  3.2288925647735596\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 31146.1521 - regression_loss: 15476.1310 - val_loss: 2709.9548 - val_regression_loss: 1337.6263\n","Epoch 2/50\n","537/537 [==============================] - 0s 47us/step - loss: 24152.2733 - regression_loss: 11961.2939 - val_loss: 1822.2676 - val_regression_loss: 889.2955\n","Epoch 3/50\n","537/537 [==============================] - 0s 50us/step - loss: 15961.3592 - regression_loss: 7828.0118 - val_loss: 1078.5105 - val_regression_loss: 510.4970\n","Epoch 4/50\n","537/537 [==============================] - 0s 47us/step - loss: 9422.1441 - regression_loss: 4504.4725 - val_loss: 932.9518 - val_regression_loss: 429.5367\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 7542.2791 - regression_loss: 3498.8933 - val_loss: 867.2915 - val_regression_loss: 396.8112\n","Epoch 6/50\n","537/537 [==============================] - 0s 46us/step - loss: 7079.9279 - regression_loss: 3266.4827 - val_loss: 544.5705 - val_regression_loss: 242.0062\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 4597.5165 - regression_loss: 2077.1263 - val_loss: 403.6007 - val_regression_loss: 179.1794\n","Epoch 8/50\n","537/537 [==============================] - 0s 52us/step - loss: 3891.3805 - regression_loss: 1786.2387 - val_loss: 486.3310 - val_regression_loss: 225.8516\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 4650.3837 - regression_loss: 2205.6825 - val_loss: 525.2696 - val_regression_loss: 247.1684\n","Epoch 10/50\n","537/537 [==============================] - 0s 53us/step - loss: 5009.2860 - regression_loss: 2399.1037 - val_loss: 432.3015 - val_regression_loss: 200.0663\n","Epoch 11/50\n","537/537 [==============================] - 0s 49us/step - loss: 4252.7332 - regression_loss: 2017.4113 - val_loss: 324.2919 - val_regression_loss: 144.3145\n","Epoch 12/50\n","537/537 [==============================] - 0s 49us/step - loss: 3236.5378 - regression_loss: 1495.7976 - val_loss: 306.9892 - val_regression_loss: 133.7075\n","Epoch 13/50\n","537/537 [==============================] - 0s 46us/step - loss: 2885.3627 - regression_loss: 1305.5344 - val_loss: 359.8728 - val_regression_loss: 158.5935\n","Epoch 14/50\n","537/537 [==============================] - 0s 44us/step - loss: 3174.5668 - regression_loss: 1438.6837 - val_loss: 381.9402 - val_regression_loss: 168.8908\n","Epoch 15/50\n","537/537 [==============================] - 0s 52us/step - loss: 3244.6006 - regression_loss: 1470.5770 - val_loss: 346.7559 - val_regression_loss: 151.4710\n","Epoch 16/50\n","537/537 [==============================] - 0s 51us/step - loss: 2972.4354 - regression_loss: 1333.6394 - val_loss: 307.3033 - val_regression_loss: 132.6341\n","Epoch 17/50\n","537/537 [==============================] - 0s 58us/step - loss: 2630.5311 - regression_loss: 1170.7232 - val_loss: 288.2730 - val_regression_loss: 124.2831\n","Epoch 18/50\n","537/537 [==============================] - 0s 53us/step - loss: 2522.9870 - regression_loss: 1128.1488 - val_loss: 274.6099 - val_regression_loss: 118.5848\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 2512.2484 - regression_loss: 1130.8842 - val_loss: 257.6999 - val_regression_loss: 111.0470\n","Epoch 20/50\n","537/537 [==============================] - 0s 43us/step - loss: 2273.1621 - regression_loss: 1018.9629 - val_loss: 249.0316 - val_regression_loss: 107.2977\n","Epoch 21/50\n","537/537 [==============================] - 0s 43us/step - loss: 2263.2958 - regression_loss: 1022.2195 - val_loss: 254.0320 - val_regression_loss: 109.9175\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 2220.6265 - regression_loss: 1000.5747 - val_loss: 257.6248 - val_regression_loss: 111.3375\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 2240.1683 - regression_loss: 1010.1851 - val_loss: 250.8931 - val_regression_loss: 107.2817\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 2122.0301 - regression_loss: 941.9106 - val_loss: 245.8828 - val_regression_loss: 104.0976\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 2027.8474 - regression_loss: 888.9189 - val_loss: 248.5116 - val_regression_loss: 104.9232\n","Epoch 26/50\n","537/537 [==============================] - 0s 52us/step - loss: 1996.7842 - regression_loss: 869.1153 - val_loss: 251.2618 - val_regression_loss: 105.9700\n","Epoch 27/50\n","537/537 [==============================] - 0s 55us/step - loss: 1961.7030 - regression_loss: 850.3226 - val_loss: 251.4771 - val_regression_loss: 105.9470\n","Epoch 28/50\n","537/537 [==============================] - 0s 57us/step - loss: 1893.6043 - regression_loss: 815.7057 - val_loss: 250.9919 - val_regression_loss: 105.8139\n","Epoch 29/50\n","537/537 [==============================] - 0s 56us/step - loss: 1811.1877 - regression_loss: 778.3579 - val_loss: 246.6987 - val_regression_loss: 104.0176\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1813.7241 - regression_loss: 779.0687 - val_loss: 236.4717 - val_regression_loss: 99.4189\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 1767.8667 - regression_loss: 758.9185 - val_loss: 227.1975 - val_regression_loss: 95.2694\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 1722.4715 - regression_loss: 741.3206 - val_loss: 223.0553 - val_regression_loss: 93.4805\n","Epoch 33/50\n","537/537 [==============================] - 0s 53us/step - loss: 1686.4208 - regression_loss: 724.5702 - val_loss: 221.4404 - val_regression_loss: 92.7351\n","Epoch 34/50\n","537/537 [==============================] - 0s 51us/step - loss: 1672.8666 - regression_loss: 718.5507 - val_loss: 223.6757 - val_regression_loss: 93.6532\n","Epoch 35/50\n","537/537 [==============================] - 0s 51us/step - loss: 1602.0654 - regression_loss: 679.4750 - val_loss: 227.0177 - val_regression_loss: 95.1283\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 1598.6012 - regression_loss: 677.3067 - val_loss: 227.3198 - val_regression_loss: 95.1439\n","Epoch 37/50\n","537/537 [==============================] - 0s 53us/step - loss: 1591.1050 - regression_loss: 669.9489 - val_loss: 223.6908 - val_regression_loss: 93.3419\n","Epoch 38/50\n","537/537 [==============================] - 0s 57us/step - loss: 1500.7355 - regression_loss: 625.0963 - val_loss: 219.9935 - val_regression_loss: 91.5623\n","Epoch 39/50\n","537/537 [==============================] - 0s 51us/step - loss: 1521.8710 - regression_loss: 637.9860 - val_loss: 217.3097 - val_regression_loss: 90.2985\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 1484.2909 - regression_loss: 620.3892 - val_loss: 216.7742 - val_regression_loss: 90.1042\n","Epoch 41/50\n","537/537 [==============================] - 0s 53us/step - loss: 1468.1481 - regression_loss: 615.7050 - val_loss: 216.4061 - val_regression_loss: 90.0411\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 1472.5371 - regression_loss: 612.0980 - val_loss: 215.3701 - val_regression_loss: 89.6928\n","Epoch 43/50\n","537/537 [==============================] - 0s 47us/step - loss: 1428.4746 - regression_loss: 592.5381 - val_loss: 214.8883 - val_regression_loss: 89.5423\n","Epoch 44/50\n","537/537 [==============================] - 0s 49us/step - loss: 1417.4113 - regression_loss: 589.9691 - val_loss: 215.6024 - val_regression_loss: 89.8480\n","Epoch 45/50\n","537/537 [==============================] - 0s 50us/step - loss: 1408.2082 - regression_loss: 584.0290 - val_loss: 216.8548 - val_regression_loss: 90.3565\n","Epoch 46/50\n","537/537 [==============================] - 0s 45us/step - loss: 1396.4701 - regression_loss: 578.0792 - val_loss: 218.0993 - val_regression_loss: 90.8989\n","Epoch 47/50\n","537/537 [==============================] - 0s 43us/step - loss: 1374.2249 - regression_loss: 565.5551 - val_loss: 217.0605 - val_regression_loss: 90.4106\n","Epoch 48/50\n","537/537 [==============================] - 0s 52us/step - loss: 1387.1821 - regression_loss: 571.6010 - val_loss: 215.9667 - val_regression_loss: 89.9431\n","Epoch 49/50\n","537/537 [==============================] - 0s 48us/step - loss: 1352.0758 - regression_loss: 555.8899 - val_loss: 215.6358 - val_regression_loss: 89.7982\n","Epoch 50/50\n","537/537 [==============================] - 0s 54us/step - loss: 1371.8722 - regression_loss: 566.1082 - val_loss: 215.6803 - val_regression_loss: 89.8434\n","***************************** elapsed_time is:  3.364626407623291\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 50232.6535 - regression_loss: 24973.9129 - val_loss: 5204.5903 - val_regression_loss: 2583.3643\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 42099.9588 - regression_loss: 20904.5181 - val_loss: 3982.9587 - val_regression_loss: 1972.1597\n","Epoch 3/50\n","537/537 [==============================] - 0s 46us/step - loss: 31704.4060 - regression_loss: 15705.7399 - val_loss: 2578.6101 - val_regression_loss: 1269.1669\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 21239.3458 - regression_loss: 10463.8091 - val_loss: 1347.1379 - val_regression_loss: 652.3796\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 11752.1352 - regression_loss: 5712.5098 - val_loss: 998.0754 - val_regression_loss: 477.4283\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 10254.1692 - regression_loss: 4960.2429 - val_loss: 1072.5153 - val_regression_loss: 516.6086\n","Epoch 7/50\n","537/537 [==============================] - 0s 50us/step - loss: 11189.0586 - regression_loss: 5443.2259 - val_loss: 862.1404 - val_regression_loss: 414.2328\n","Epoch 8/50\n","537/537 [==============================] - 0s 50us/step - loss: 9307.0735 - regression_loss: 4525.7214 - val_loss: 682.4286 - val_regression_loss: 326.2332\n","Epoch 9/50\n","537/537 [==============================] - 0s 54us/step - loss: 7296.3521 - regression_loss: 3538.0580 - val_loss: 782.8901 - val_regression_loss: 377.2548\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 7161.6062 - regression_loss: 3473.7343 - val_loss: 921.7264 - val_regression_loss: 446.5507\n","Epoch 11/50\n","537/537 [==============================] - 0s 54us/step - loss: 7463.3537 - regression_loss: 3621.7163 - val_loss: 868.9006 - val_regression_loss: 419.4070\n","Epoch 12/50\n","537/537 [==============================] - 0s 46us/step - loss: 7198.0699 - regression_loss: 3482.3543 - val_loss: 674.2730 - val_regression_loss: 320.9948\n","Epoch 13/50\n","537/537 [==============================] - 0s 47us/step - loss: 6046.2446 - regression_loss: 2897.1027 - val_loss: 508.3610 - val_regression_loss: 236.8132\n","Epoch 14/50\n","537/537 [==============================] - 0s 44us/step - loss: 5134.4907 - regression_loss: 2431.8493 - val_loss: 465.1520 - val_regression_loss: 214.1617\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 4776.9543 - regression_loss: 2240.6792 - val_loss: 483.2966 - val_regression_loss: 222.7197\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 5477.6503 - regression_loss: 2590.1029 - val_loss: 464.9891 - val_regression_loss: 213.7149\n","Epoch 17/50\n","537/537 [==============================] - 0s 53us/step - loss: 5181.9626 - regression_loss: 2440.2490 - val_loss: 421.5595 - val_regression_loss: 192.6669\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 4526.1640 - regression_loss: 2117.6416 - val_loss: 421.2547 - val_regression_loss: 193.3582\n","Epoch 19/50\n","537/537 [==============================] - 0s 47us/step - loss: 4278.1346 - regression_loss: 2002.6968 - val_loss: 455.8189 - val_regression_loss: 211.3514\n","Epoch 20/50\n","537/537 [==============================] - 0s 49us/step - loss: 4304.8309 - regression_loss: 2022.8332 - val_loss: 460.7245 - val_regression_loss: 214.2249\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 4203.2462 - regression_loss: 1978.3298 - val_loss: 419.5826 - val_regression_loss: 193.7669\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 3923.7761 - regression_loss: 1839.3584 - val_loss: 377.2917 - val_regression_loss: 172.4887\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 3697.9778 - regression_loss: 1724.8286 - val_loss: 360.2119 - val_regression_loss: 163.6868\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 3586.0691 - regression_loss: 1668.9701 - val_loss: 346.9368 - val_regression_loss: 156.8171\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 3478.4254 - regression_loss: 1611.2121 - val_loss: 330.9331 - val_regression_loss: 148.7078\n","Epoch 26/50\n","537/537 [==============================] - 0s 55us/step - loss: 3154.0558 - regression_loss: 1448.3956 - val_loss: 329.2310 - val_regression_loss: 147.8580\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 2949.7638 - regression_loss: 1345.9098 - val_loss: 331.8404 - val_regression_loss: 149.1470\n","Epoch 28/50\n","537/537 [==============================] - 0s 52us/step - loss: 3016.6802 - regression_loss: 1382.1161 - val_loss: 316.9325 - val_regression_loss: 141.5498\n","Epoch 29/50\n","537/537 [==============================] - 0s 52us/step - loss: 2814.3055 - regression_loss: 1280.9162 - val_loss: 301.7629 - val_regression_loss: 133.7628\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 2646.7461 - regression_loss: 1191.5115 - val_loss: 300.0078 - val_regression_loss: 132.8294\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 2586.1974 - regression_loss: 1163.0783 - val_loss: 295.0013 - val_regression_loss: 130.4904\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 2430.7626 - regression_loss: 1087.5727 - val_loss: 288.0432 - val_regression_loss: 127.2969\n","Epoch 33/50\n","537/537 [==============================] - 0s 49us/step - loss: 2329.1159 - regression_loss: 1041.3575 - val_loss: 284.2524 - val_regression_loss: 125.5805\n","Epoch 34/50\n","537/537 [==============================] - 0s 56us/step - loss: 2288.2248 - regression_loss: 1022.8744 - val_loss: 281.0334 - val_regression_loss: 123.9073\n","Epoch 35/50\n","537/537 [==============================] - 0s 56us/step - loss: 2177.8694 - regression_loss: 968.0561 - val_loss: 280.5740 - val_regression_loss: 123.4774\n","Epoch 36/50\n","537/537 [==============================] - 0s 49us/step - loss: 2128.5789 - regression_loss: 942.8314 - val_loss: 280.8245 - val_regression_loss: 123.4099\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 2075.6999 - regression_loss: 917.0605 - val_loss: 269.9034 - val_regression_loss: 117.9029\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 2004.8721 - regression_loss: 879.5928 - val_loss: 261.6992 - val_regression_loss: 113.7771\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 1865.5758 - regression_loss: 809.8252 - val_loss: 257.6012 - val_regression_loss: 111.6168\n","Epoch 40/50\n","537/537 [==============================] - 0s 55us/step - loss: 1804.5284 - regression_loss: 780.3908 - val_loss: 256.8179 - val_regression_loss: 111.0786\n","Epoch 41/50\n","537/537 [==============================] - 0s 58us/step - loss: 1807.0030 - regression_loss: 783.1981 - val_loss: 259.7196 - val_regression_loss: 112.4024\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 1751.0828 - regression_loss: 754.6153 - val_loss: 253.7297 - val_regression_loss: 109.4359\n","Epoch 43/50\n","537/537 [==============================] - 0s 52us/step - loss: 1699.0256 - regression_loss: 728.8295 - val_loss: 244.7213 - val_regression_loss: 104.9906\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 1655.5875 - regression_loss: 708.4834 - val_loss: 241.3074 - val_regression_loss: 103.2394\n","Epoch 45/50\n","537/537 [==============================] - 0s 58us/step - loss: 1630.8827 - regression_loss: 696.8031 - val_loss: 240.5672 - val_regression_loss: 102.7748\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 1608.5711 - regression_loss: 687.0975 - val_loss: 242.2555 - val_regression_loss: 103.5027\n","Epoch 47/50\n","537/537 [==============================] - 0s 53us/step - loss: 1579.3304 - regression_loss: 670.8207 - val_loss: 239.1035 - val_regression_loss: 101.8727\n","Epoch 48/50\n","537/537 [==============================] - 0s 52us/step - loss: 1570.3416 - regression_loss: 665.5555 - val_loss: 229.3408 - val_regression_loss: 97.0462\n","Epoch 49/50\n","537/537 [==============================] - 0s 50us/step - loss: 1521.1170 - regression_loss: 642.2479 - val_loss: 228.5137 - val_regression_loss: 96.5834\n","Epoch 50/50\n","537/537 [==============================] - 0s 55us/step - loss: 1501.9506 - regression_loss: 632.1446 - val_loss: 232.2810 - val_regression_loss: 98.3744\n","***************************** elapsed_time is:  3.1657378673553467\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 59112.1513 - regression_loss: 29574.5865 - val_loss: 6253.6240 - val_regression_loss: 3120.8169\n","Epoch 2/50\n","537/537 [==============================] - 0s 49us/step - loss: 48629.2169 - regression_loss: 24298.9567 - val_loss: 4898.4673 - val_regression_loss: 2437.6294\n","Epoch 3/50\n","537/537 [==============================] - 0s 49us/step - loss: 36729.9357 - regression_loss: 18306.2036 - val_loss: 3410.5632 - val_regression_loss: 1684.5081\n","Epoch 4/50\n","537/537 [==============================] - 0s 47us/step - loss: 22616.2575 - regression_loss: 11171.0640 - val_loss: 2388.2014 - val_regression_loss: 1158.0260\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 13771.6854 - regression_loss: 6636.0438 - val_loss: 2664.7185 - val_regression_loss: 1278.5413\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 14761.1420 - regression_loss: 6990.1390 - val_loss: 2438.8521 - val_regression_loss: 1166.1119\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 13207.9607 - regression_loss: 6215.3646 - val_loss: 1625.4941 - val_regression_loss: 771.6257\n","Epoch 8/50\n","537/537 [==============================] - 0s 49us/step - loss: 7348.2133 - regression_loss: 3383.7666 - val_loss: 1214.2825 - val_regression_loss: 579.8159\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 5399.7623 - regression_loss: 2521.1381 - val_loss: 1281.2356 - val_regression_loss: 623.1539\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 6691.5900 - regression_loss: 3241.6024 - val_loss: 1387.2703 - val_regression_loss: 680.4340\n","Epoch 11/50\n","537/537 [==============================] - 0s 48us/step - loss: 7715.3450 - regression_loss: 3783.2483 - val_loss: 1298.8044 - val_regression_loss: 636.0517\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 6805.7015 - regression_loss: 3324.0426 - val_loss: 1110.6061 - val_regression_loss: 539.4272\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 4876.4292 - regression_loss: 2334.2757 - val_loss: 1001.6091 - val_regression_loss: 481.7327\n","Epoch 14/50\n","537/537 [==============================] - 0s 48us/step - loss: 3541.8218 - regression_loss: 1639.2810 - val_loss: 1016.0187 - val_regression_loss: 486.2577\n","Epoch 15/50\n","537/537 [==============================] - 0s 64us/step - loss: 3898.1923 - regression_loss: 1794.0213 - val_loss: 1031.0422 - val_regression_loss: 492.3893\n","Epoch 16/50\n","537/537 [==============================] - 0s 62us/step - loss: 4224.5891 - regression_loss: 1942.8760 - val_loss: 965.8729 - val_regression_loss: 460.0170\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 3876.7216 - regression_loss: 1771.8197 - val_loss: 910.1262 - val_regression_loss: 433.5649\n","Epoch 18/50\n","537/537 [==============================] - 0s 56us/step - loss: 3444.0657 - regression_loss: 1566.9641 - val_loss: 910.7689 - val_regression_loss: 435.7979\n","Epoch 19/50\n","537/537 [==============================] - 0s 53us/step - loss: 3492.0883 - regression_loss: 1604.8837 - val_loss: 903.5289 - val_regression_loss: 433.8129\n","Epoch 20/50\n","537/537 [==============================] - 0s 52us/step - loss: 3307.3283 - regression_loss: 1526.0987 - val_loss: 866.2466 - val_regression_loss: 416.3750\n","Epoch 21/50\n","537/537 [==============================] - 0s 51us/step - loss: 2924.5720 - regression_loss: 1343.4368 - val_loss: 845.1000 - val_regression_loss: 406.8311\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 2637.1409 - regression_loss: 1208.0330 - val_loss: 846.7269 - val_regression_loss: 408.5128\n","Epoch 23/50\n","537/537 [==============================] - 0s 54us/step - loss: 2710.0829 - regression_loss: 1250.5070 - val_loss: 829.5801 - val_regression_loss: 400.3891\n","Epoch 24/50\n","537/537 [==============================] - 0s 54us/step - loss: 2582.2038 - regression_loss: 1190.1891 - val_loss: 783.1923 - val_regression_loss: 377.0837\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 2369.4037 - regression_loss: 1084.6671 - val_loss: 736.9840 - val_regression_loss: 353.3812\n","Epoch 26/50\n","537/537 [==============================] - 0s 56us/step - loss: 2250.1205 - regression_loss: 1023.7843 - val_loss: 704.3532 - val_regression_loss: 336.1330\n","Epoch 27/50\n","537/537 [==============================] - 0s 77us/step - loss: 2191.2109 - regression_loss: 986.2727 - val_loss: 677.5054 - val_regression_loss: 321.5630\n","Epoch 28/50\n","537/537 [==============================] - 0s 55us/step - loss: 2099.6582 - regression_loss: 935.0812 - val_loss: 658.0499 - val_regression_loss: 310.7940\n","Epoch 29/50\n","537/537 [==============================] - 0s 48us/step - loss: 1936.3858 - regression_loss: 844.5040 - val_loss: 649.3608 - val_regression_loss: 305.7937\n","Epoch 30/50\n","537/537 [==============================] - 0s 47us/step - loss: 2036.3065 - regression_loss: 892.2430 - val_loss: 638.2649 - val_regression_loss: 300.3393\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 1951.2554 - regression_loss: 849.5161 - val_loss: 623.9076 - val_regression_loss: 293.8379\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 1878.3082 - regression_loss: 819.7381 - val_loss: 614.9790 - val_regression_loss: 290.2057\n","Epoch 33/50\n","537/537 [==============================] - 0s 56us/step - loss: 1876.4902 - regression_loss: 827.2194 - val_loss: 611.5286 - val_regression_loss: 288.9138\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 1848.1519 - regression_loss: 818.7961 - val_loss: 607.3431 - val_regression_loss: 286.6236\n","Epoch 35/50\n","537/537 [==============================] - 0s 47us/step - loss: 1805.7829 - regression_loss: 797.5525 - val_loss: 602.3846 - val_regression_loss: 283.5469\n","Epoch 36/50\n","537/537 [==============================] - 0s 52us/step - loss: 1678.0280 - regression_loss: 727.6601 - val_loss: 596.9338 - val_regression_loss: 280.1804\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 1757.9517 - regression_loss: 764.5862 - val_loss: 585.1402 - val_regression_loss: 273.8147\n","Epoch 38/50\n","537/537 [==============================] - 0s 61us/step - loss: 1723.3498 - regression_loss: 745.6831 - val_loss: 570.9381 - val_regression_loss: 266.5520\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 1656.0870 - regression_loss: 713.1086 - val_loss: 558.0253 - val_regression_loss: 260.0409\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 1669.5307 - regression_loss: 717.4883 - val_loss: 545.9442 - val_regression_loss: 253.9850\n","Epoch 41/50\n","537/537 [==============================] - 0s 50us/step - loss: 1613.3424 - regression_loss: 687.9510 - val_loss: 536.4990 - val_regression_loss: 249.3383\n","Epoch 42/50\n","537/537 [==============================] - 0s 48us/step - loss: 1545.8128 - regression_loss: 653.3230 - val_loss: 529.6307 - val_regression_loss: 246.2035\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1603.5202 - regression_loss: 687.6965 - val_loss: 522.2657 - val_regression_loss: 242.8692\n","Epoch 44/50\n","537/537 [==============================] - 0s 52us/step - loss: 1598.6533 - regression_loss: 688.4997 - val_loss: 516.1885 - val_regression_loss: 240.0153\n","Epoch 45/50\n","537/537 [==============================] - 0s 55us/step - loss: 1572.7486 - regression_loss: 676.2149 - val_loss: 510.9252 - val_regression_loss: 237.3679\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 1547.3945 - regression_loss: 661.0058 - val_loss: 505.2905 - val_regression_loss: 234.4487\n","Epoch 47/50\n","537/537 [==============================] - 0s 50us/step - loss: 1502.7275 - regression_loss: 639.7836 - val_loss: 499.9413 - val_regression_loss: 231.6536\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 1516.6629 - regression_loss: 645.3493 - val_loss: 494.7114 - val_regression_loss: 228.9361\n","Epoch 49/50\n","537/537 [==============================] - 0s 66us/step - loss: 1506.7585 - regression_loss: 637.8477 - val_loss: 488.3207 - val_regression_loss: 225.7980\n","Epoch 50/50\n","537/537 [==============================] - 0s 51us/step - loss: 1475.1250 - regression_loss: 620.5932 - val_loss: 481.7671 - val_regression_loss: 222.6019\n","***************************** elapsed_time is:  3.1773204803466797\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 99628.9458 - regression_loss: 49390.1458 - val_loss: 10609.8320 - val_regression_loss: 5254.1895\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 90379.7781 - regression_loss: 44800.3250 - val_loss: 8779.1533 - val_regression_loss: 4344.4180\n","Epoch 3/50\n","537/537 [==============================] - 0s 52us/step - loss: 73956.5730 - regression_loss: 36635.0942 - val_loss: 6444.7319 - val_regression_loss: 3185.4163\n","Epoch 4/50\n","537/537 [==============================] - 0s 44us/step - loss: 54641.0771 - regression_loss: 27038.7543 - val_loss: 4017.5901 - val_regression_loss: 1983.4696\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 33759.2443 - regression_loss: 16693.6387 - val_loss: 2642.0486 - val_regression_loss: 1310.0808\n","Epoch 6/50\n","537/537 [==============================] - 0s 47us/step - loss: 22711.3254 - regression_loss: 11279.2814 - val_loss: 2975.1174 - val_regression_loss: 1486.5762\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 26548.1627 - regression_loss: 13272.7576 - val_loss: 2526.7815 - val_regression_loss: 1259.0001\n","Epoch 8/50\n","537/537 [==============================] - 0s 46us/step - loss: 23808.2569 - regression_loss: 11870.5258 - val_loss: 1629.5439 - val_regression_loss: 801.4362\n","Epoch 9/50\n","537/537 [==============================] - 0s 49us/step - loss: 17944.4351 - regression_loss: 8866.7213 - val_loss: 1287.9452 - val_regression_loss: 622.0917\n","Epoch 10/50\n","537/537 [==============================] - 0s 52us/step - loss: 15277.6939 - regression_loss: 7460.4312 - val_loss: 1502.1073 - val_regression_loss: 723.8319\n","Epoch 11/50\n","537/537 [==============================] - 0s 57us/step - loss: 17204.6691 - regression_loss: 8382.2392 - val_loss: 1724.5182 - val_regression_loss: 833.5664\n","Epoch 12/50\n","537/537 [==============================] - 0s 54us/step - loss: 18393.9565 - regression_loss: 8968.7162 - val_loss: 1642.8481 - val_regression_loss: 794.4083\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 17036.3979 - regression_loss: 8307.4343 - val_loss: 1328.9618 - val_regression_loss: 641.2030\n","Epoch 14/50\n","537/537 [==============================] - 0s 52us/step - loss: 13245.0271 - regression_loss: 6449.8929 - val_loss: 1032.3768 - val_regression_loss: 497.4529\n","Epoch 15/50\n","537/537 [==============================] - 0s 45us/step - loss: 12006.7757 - regression_loss: 5866.6345 - val_loss: 917.2021 - val_regression_loss: 444.1026\n","Epoch 16/50\n","537/537 [==============================] - 0s 52us/step - loss: 11448.3099 - regression_loss: 5621.1161 - val_loss: 933.3007 - val_regression_loss: 454.8416\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 11117.5588 - regression_loss: 5480.3955 - val_loss: 889.1699 - val_regression_loss: 433.1468\n","Epoch 18/50\n","537/537 [==============================] - 0s 48us/step - loss: 11277.7166 - regression_loss: 5567.2538 - val_loss: 775.0744 - val_regression_loss: 374.4461\n","Epoch 19/50\n","537/537 [==============================] - 0s 46us/step - loss: 9976.6131 - regression_loss: 4902.0107 - val_loss: 719.6199 - val_regression_loss: 343.9887\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 8959.0787 - regression_loss: 4371.5280 - val_loss: 747.5336 - val_regression_loss: 355.3019\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 8436.5753 - regression_loss: 4088.9053 - val_loss: 757.6645 - val_regression_loss: 358.7177\n","Epoch 22/50\n","537/537 [==============================] - 0s 50us/step - loss: 8244.9198 - regression_loss: 3978.9466 - val_loss: 679.1611 - val_regression_loss: 319.2534\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 7444.6549 - regression_loss: 3572.5887 - val_loss: 580.0940 - val_regression_loss: 270.5735\n","Epoch 24/50\n","537/537 [==============================] - 0s 54us/step - loss: 6618.4287 - regression_loss: 3163.5426 - val_loss: 536.8095 - val_regression_loss: 250.1495\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 6207.7889 - regression_loss: 2968.6238 - val_loss: 517.3662 - val_regression_loss: 241.2305\n","Epoch 26/50\n","537/537 [==============================] - 0s 55us/step - loss: 5948.1392 - regression_loss: 2845.3237 - val_loss: 471.0682 - val_regression_loss: 218.0571\n","Epoch 27/50\n","537/537 [==============================] - 0s 56us/step - loss: 5363.3325 - regression_loss: 2555.2078 - val_loss: 436.5086 - val_regression_loss: 200.2405\n","Epoch 28/50\n","537/537 [==============================] - 0s 54us/step - loss: 4830.8953 - regression_loss: 2283.1804 - val_loss: 427.8860 - val_regression_loss: 195.6320\n","Epoch 29/50\n","537/537 [==============================] - 0s 52us/step - loss: 4722.1045 - regression_loss: 2227.5312 - val_loss: 404.7269 - val_regression_loss: 184.5515\n","Epoch 30/50\n","537/537 [==============================] - 0s 55us/step - loss: 4448.3433 - regression_loss: 2095.5701 - val_loss: 374.4863 - val_regression_loss: 170.5361\n","Epoch 31/50\n","537/537 [==============================] - 0s 57us/step - loss: 4086.1384 - regression_loss: 1922.3764 - val_loss: 368.8648 - val_regression_loss: 168.7349\n","Epoch 32/50\n","537/537 [==============================] - 0s 58us/step - loss: 3904.1370 - regression_loss: 1837.2064 - val_loss: 363.3727 - val_regression_loss: 166.2071\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 3741.7976 - regression_loss: 1755.0962 - val_loss: 337.9157 - val_regression_loss: 152.9211\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 3309.1235 - regression_loss: 1532.4674 - val_loss: 318.4001 - val_regression_loss: 142.5099\n","Epoch 35/50\n","537/537 [==============================] - 0s 53us/step - loss: 3366.5184 - regression_loss: 1552.8246 - val_loss: 308.3929 - val_regression_loss: 137.5640\n","Epoch 36/50\n","537/537 [==============================] - 0s 56us/step - loss: 3070.8441 - regression_loss: 1403.7659 - val_loss: 303.6471 - val_regression_loss: 135.8557\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 3004.4013 - regression_loss: 1374.7706 - val_loss: 315.0089 - val_regression_loss: 142.4546\n","Epoch 38/50\n","537/537 [==============================] - 0s 52us/step - loss: 2886.4748 - regression_loss: 1318.1334 - val_loss: 319.8516 - val_regression_loss: 145.4318\n","Epoch 39/50\n","537/537 [==============================] - 0s 55us/step - loss: 2696.0107 - regression_loss: 1225.3463 - val_loss: 307.6767 - val_regression_loss: 139.4267\n","Epoch 40/50\n","537/537 [==============================] - 0s 47us/step - loss: 2710.0704 - regression_loss: 1233.8915 - val_loss: 300.1384 - val_regression_loss: 135.5930\n","Epoch 41/50\n","537/537 [==============================] - 0s 48us/step - loss: 2656.1271 - regression_loss: 1206.6375 - val_loss: 301.1192 - val_regression_loss: 136.2102\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 2521.8442 - regression_loss: 1140.0132 - val_loss: 307.3988 - val_regression_loss: 139.6343\n","Epoch 43/50\n","537/537 [==============================] - 0s 44us/step - loss: 2547.8393 - regression_loss: 1155.8242 - val_loss: 317.1067 - val_regression_loss: 144.6891\n","Epoch 44/50\n","537/537 [==============================] - 0s 47us/step - loss: 2432.6219 - regression_loss: 1097.9904 - val_loss: 312.9884 - val_regression_loss: 142.4380\n","Epoch 45/50\n","537/537 [==============================] - 0s 51us/step - loss: 2440.5381 - regression_loss: 1101.6644 - val_loss: 301.7803 - val_regression_loss: 136.4582\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 2343.8537 - regression_loss: 1047.8332 - val_loss: 294.9303 - val_regression_loss: 132.7936\n","Epoch 47/50\n","537/537 [==============================] - 0s 49us/step - loss: 2312.3799 - regression_loss: 1031.7468 - val_loss: 293.8717 - val_regression_loss: 132.3076\n","Epoch 48/50\n","537/537 [==============================] - 0s 50us/step - loss: 2256.0880 - regression_loss: 1006.5515 - val_loss: 296.7232 - val_regression_loss: 133.8795\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 2257.3964 - regression_loss: 1007.9986 - val_loss: 294.1700 - val_regression_loss: 132.5896\n","Epoch 50/50\n","537/537 [==============================] - 0s 69us/step - loss: 2166.9277 - regression_loss: 962.2358 - val_loss: 288.4657 - val_regression_loss: 129.5821\n","***************************** elapsed_time is:  3.399287223815918\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 29326.2875 - regression_loss: 14392.0608 - val_loss: 3033.8052 - val_regression_loss: 1485.5190\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 22968.9547 - regression_loss: 11272.8621 - val_loss: 2226.5769 - val_regression_loss: 1089.7917\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 16383.2322 - regression_loss: 8040.6659 - val_loss: 1372.2183 - val_regression_loss: 670.8951\n","Epoch 4/50\n","537/537 [==============================] - 0s 48us/step - loss: 9513.7454 - regression_loss: 4670.2479 - val_loss: 818.1655 - val_regression_loss: 400.8766\n","Epoch 5/50\n","537/537 [==============================] - 0s 50us/step - loss: 5805.6549 - regression_loss: 2871.1485 - val_loss: 775.9210 - val_regression_loss: 378.6168\n","Epoch 6/50\n","537/537 [==============================] - 0s 52us/step - loss: 5972.1498 - regression_loss: 2940.1796 - val_loss: 710.8458 - val_regression_loss: 336.0688\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 5275.5933 - regression_loss: 2501.1460 - val_loss: 649.1924 - val_regression_loss: 297.2160\n","Epoch 8/50\n","537/537 [==============================] - 0s 53us/step - loss: 4071.7389 - regression_loss: 1844.3030 - val_loss: 710.5487 - val_regression_loss: 325.3834\n","Epoch 9/50\n","537/537 [==============================] - 0s 44us/step - loss: 4114.4597 - regression_loss: 1843.6119 - val_loss: 751.1132 - val_regression_loss: 347.1205\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 4224.4202 - regression_loss: 1912.6541 - val_loss: 669.5536 - val_regression_loss: 309.4622\n","Epoch 11/50\n","537/537 [==============================] - 0s 55us/step - loss: 3842.2378 - regression_loss: 1751.1443 - val_loss: 532.0009 - val_regression_loss: 244.3220\n","Epoch 12/50\n","537/537 [==============================] - 0s 50us/step - loss: 3086.9270 - regression_loss: 1403.4760 - val_loss: 434.6779 - val_regression_loss: 199.1221\n","Epoch 13/50\n","537/537 [==============================] - 0s 50us/step - loss: 2639.2417 - regression_loss: 1205.4022 - val_loss: 418.4789 - val_regression_loss: 193.6469\n","Epoch 14/50\n","537/537 [==============================] - 0s 52us/step - loss: 2705.0546 - regression_loss: 1257.8529 - val_loss: 438.4128 - val_regression_loss: 204.7631\n","Epoch 15/50\n","537/537 [==============================] - 0s 46us/step - loss: 2928.3640 - regression_loss: 1379.2766 - val_loss: 430.2079 - val_regression_loss: 200.1528\n","Epoch 16/50\n","537/537 [==============================] - 0s 47us/step - loss: 2834.0457 - regression_loss: 1328.3218 - val_loss: 405.0827 - val_regression_loss: 185.9347\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 2495.4609 - regression_loss: 1145.0630 - val_loss: 399.4120 - val_regression_loss: 181.0175\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 2266.5726 - regression_loss: 1011.1117 - val_loss: 414.6422 - val_regression_loss: 186.8184\n","Epoch 19/50\n","537/537 [==============================] - 0s 44us/step - loss: 2350.6838 - regression_loss: 1041.1076 - val_loss: 422.4467 - val_regression_loss: 189.5666\n","Epoch 20/50\n","537/537 [==============================] - 0s 57us/step - loss: 2374.7626 - regression_loss: 1045.2076 - val_loss: 407.5736 - val_regression_loss: 181.7954\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 2286.8470 - regression_loss: 998.9475 - val_loss: 377.8406 - val_regression_loss: 167.3427\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 2160.5578 - regression_loss: 938.5883 - val_loss: 348.4810 - val_regression_loss: 153.6322\n","Epoch 23/50\n","537/537 [==============================] - 0s 55us/step - loss: 2089.7894 - regression_loss: 911.8330 - val_loss: 326.3825 - val_regression_loss: 143.7252\n","Epoch 24/50\n","537/537 [==============================] - 0s 48us/step - loss: 2047.9475 - regression_loss: 899.1904 - val_loss: 310.3087 - val_regression_loss: 136.6224\n","Epoch 25/50\n","537/537 [==============================] - 0s 46us/step - loss: 2030.1971 - regression_loss: 897.2749 - val_loss: 303.4628 - val_regression_loss: 133.6444\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1931.0455 - regression_loss: 850.9943 - val_loss: 306.7480 - val_regression_loss: 135.3150\n","Epoch 27/50\n","537/537 [==============================] - 0s 51us/step - loss: 1909.6995 - regression_loss: 841.4740 - val_loss: 312.1652 - val_regression_loss: 137.8193\n","Epoch 28/50\n","537/537 [==============================] - 0s 52us/step - loss: 1903.6894 - regression_loss: 835.2953 - val_loss: 309.7919 - val_regression_loss: 136.4408\n","Epoch 29/50\n","537/537 [==============================] - 0s 51us/step - loss: 1849.3023 - regression_loss: 805.4099 - val_loss: 300.6326 - val_regression_loss: 131.6902\n","Epoch 30/50\n","537/537 [==============================] - 0s 50us/step - loss: 1780.6714 - regression_loss: 768.4747 - val_loss: 290.9624 - val_regression_loss: 126.6931\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 1748.0082 - regression_loss: 754.9423 - val_loss: 283.9356 - val_regression_loss: 122.9999\n","Epoch 32/50\n","537/537 [==============================] - 0s 48us/step - loss: 1733.8737 - regression_loss: 741.4866 - val_loss: 277.7687 - val_regression_loss: 119.8542\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 1660.7176 - regression_loss: 705.3529 - val_loss: 275.8882 - val_regression_loss: 118.9607\n","Epoch 34/50\n","537/537 [==============================] - 0s 50us/step - loss: 1592.0076 - regression_loss: 671.4232 - val_loss: 273.4261 - val_regression_loss: 117.9869\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 1639.6006 - regression_loss: 697.3057 - val_loss: 264.0443 - val_regression_loss: 113.7643\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 1582.5043 - regression_loss: 673.3940 - val_loss: 255.7357 - val_regression_loss: 110.0209\n","Epoch 37/50\n","537/537 [==============================] - 0s 51us/step - loss: 1629.2162 - regression_loss: 698.7212 - val_loss: 249.1693 - val_regression_loss: 106.9201\n","Epoch 38/50\n","537/537 [==============================] - 0s 47us/step - loss: 1605.2906 - regression_loss: 689.7439 - val_loss: 246.2290 - val_regression_loss: 105.3072\n","Epoch 39/50\n","537/537 [==============================] - 0s 47us/step - loss: 1548.5385 - regression_loss: 655.9555 - val_loss: 246.2296 - val_regression_loss: 105.0192\n","Epoch 40/50\n","537/537 [==============================] - 0s 61us/step - loss: 1506.9342 - regression_loss: 633.4355 - val_loss: 245.4530 - val_regression_loss: 104.3729\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 1519.5840 - regression_loss: 637.6815 - val_loss: 238.7017 - val_regression_loss: 100.9882\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 1538.6423 - regression_loss: 649.0380 - val_loss: 232.8340 - val_regression_loss: 98.1520\n","Epoch 43/50\n","537/537 [==============================] - 0s 49us/step - loss: 1491.8424 - regression_loss: 630.1182 - val_loss: 229.0027 - val_regression_loss: 96.3365\n","Epoch 44/50\n","537/537 [==============================] - 0s 49us/step - loss: 1497.5050 - regression_loss: 628.6063 - val_loss: 228.2520 - val_regression_loss: 96.0451\n","Epoch 45/50\n","537/537 [==============================] - 0s 65us/step - loss: 1464.5526 - regression_loss: 616.1693 - val_loss: 227.1568 - val_regression_loss: 95.5545\n","Epoch 46/50\n","537/537 [==============================] - 0s 55us/step - loss: 1472.8922 - regression_loss: 618.9756 - val_loss: 221.5694 - val_regression_loss: 92.9239\n","Epoch 47/50\n","537/537 [==============================] - 0s 52us/step - loss: 1454.6716 - regression_loss: 611.8013 - val_loss: 216.5315 - val_regression_loss: 90.4384\n","Epoch 48/50\n","537/537 [==============================] - 0s 51us/step - loss: 1469.4095 - regression_loss: 620.0053 - val_loss: 212.8123 - val_regression_loss: 88.5332\n","Epoch 49/50\n","537/537 [==============================] - 0s 54us/step - loss: 1381.2350 - regression_loss: 574.8419 - val_loss: 211.6774 - val_regression_loss: 87.8497\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 1414.3191 - regression_loss: 590.8868 - val_loss: 209.8349 - val_regression_loss: 86.8674\n","***************************** elapsed_time is:  3.112541437149048\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 19385.3679 - regression_loss: 9510.3775 - val_loss: 1734.4502 - val_regression_loss: 844.7471\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 14961.9635 - regression_loss: 7313.5647 - val_loss: 1220.6177 - val_regression_loss: 590.1132\n","Epoch 3/50\n","537/537 [==============================] - 0s 48us/step - loss: 10251.8362 - regression_loss: 4976.6332 - val_loss: 792.4144 - val_regression_loss: 378.6263\n","Epoch 4/50\n","537/537 [==============================] - 0s 51us/step - loss: 6396.2512 - regression_loss: 3069.7689 - val_loss: 702.3095 - val_regression_loss: 335.9193\n","Epoch 5/50\n","537/537 [==============================] - 0s 46us/step - loss: 5520.2367 - regression_loss: 2649.1060 - val_loss: 513.3291 - val_regression_loss: 241.6509\n","Epoch 6/50\n","537/537 [==============================] - 0s 47us/step - loss: 3920.2031 - regression_loss: 1849.8305 - val_loss: 271.5966 - val_regression_loss: 119.4124\n","Epoch 7/50\n","537/537 [==============================] - 0s 50us/step - loss: 2265.8655 - regression_loss: 1008.3528 - val_loss: 248.2375 - val_regression_loss: 105.9102\n","Epoch 8/50\n","537/537 [==============================] - 0s 48us/step - loss: 2328.6822 - regression_loss: 1026.5930 - val_loss: 335.8878 - val_regression_loss: 148.8618\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 3167.2783 - regression_loss: 1436.1760 - val_loss: 320.2556 - val_regression_loss: 141.3350\n","Epoch 10/50\n","537/537 [==============================] - 0s 50us/step - loss: 3020.7924 - regression_loss: 1367.1045 - val_loss: 244.6932 - val_regression_loss: 104.3060\n","Epoch 11/50\n","537/537 [==============================] - 0s 46us/step - loss: 2290.1256 - regression_loss: 1011.8257 - val_loss: 195.0792 - val_regression_loss: 80.2986\n","Epoch 12/50\n","537/537 [==============================] - 0s 50us/step - loss: 1728.7726 - regression_loss: 735.9409 - val_loss: 197.9416 - val_regression_loss: 82.4055\n","Epoch 13/50\n","537/537 [==============================] - 0s 48us/step - loss: 1713.3680 - regression_loss: 736.8327 - val_loss: 227.4698 - val_regression_loss: 97.6261\n","Epoch 14/50\n","537/537 [==============================] - 0s 43us/step - loss: 1871.6246 - regression_loss: 820.2597 - val_loss: 238.5861 - val_regression_loss: 103.3682\n","Epoch 15/50\n","537/537 [==============================] - 0s 53us/step - loss: 1906.8728 - regression_loss: 839.2875 - val_loss: 221.7211 - val_regression_loss: 94.8813\n","Epoch 16/50\n","537/537 [==============================] - 0s 50us/step - loss: 1821.4404 - regression_loss: 794.6467 - val_loss: 199.2873 - val_regression_loss: 83.4454\n","Epoch 17/50\n","537/537 [==============================] - 0s 53us/step - loss: 1598.7717 - regression_loss: 682.2708 - val_loss: 185.1798 - val_regression_loss: 76.1164\n","Epoch 18/50\n","537/537 [==============================] - 0s 54us/step - loss: 1561.7138 - regression_loss: 660.4139 - val_loss: 175.7696 - val_regression_loss: 71.1336\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 1528.5041 - regression_loss: 640.3916 - val_loss: 170.2400 - val_regression_loss: 68.1285\n","Epoch 20/50\n","537/537 [==============================] - 0s 45us/step - loss: 1525.6657 - regression_loss: 637.4421 - val_loss: 169.1034 - val_regression_loss: 67.3958\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 1544.1964 - regression_loss: 643.6057 - val_loss: 169.1552 - val_regression_loss: 67.3823\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 1498.4249 - regression_loss: 622.2397 - val_loss: 168.4649 - val_regression_loss: 67.1169\n","Epoch 23/50\n","537/537 [==============================] - 0s 50us/step - loss: 1490.9992 - regression_loss: 617.8226 - val_loss: 168.8036 - val_regression_loss: 67.4349\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 1422.5747 - regression_loss: 588.6274 - val_loss: 171.4569 - val_regression_loss: 68.9191\n","Epoch 25/50\n","537/537 [==============================] - 0s 56us/step - loss: 1406.3083 - regression_loss: 581.2270 - val_loss: 174.7814 - val_regression_loss: 70.7168\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1382.8922 - regression_loss: 571.8578 - val_loss: 177.0738 - val_regression_loss: 71.9603\n","Epoch 27/50\n","537/537 [==============================] - 0s 51us/step - loss: 1394.7370 - regression_loss: 579.3852 - val_loss: 177.2578 - val_regression_loss: 72.0921\n","Epoch 28/50\n","537/537 [==============================] - 0s 48us/step - loss: 1397.3883 - regression_loss: 579.4863 - val_loss: 175.3002 - val_regression_loss: 71.0725\n","Epoch 29/50\n","537/537 [==============================] - 0s 59us/step - loss: 1375.8027 - regression_loss: 567.9756 - val_loss: 172.0855 - val_regression_loss: 69.3609\n","Epoch 30/50\n","537/537 [==============================] - 0s 57us/step - loss: 1336.5023 - regression_loss: 546.7928 - val_loss: 169.6378 - val_regression_loss: 68.0068\n","Epoch 31/50\n","537/537 [==============================] - 0s 50us/step - loss: 1363.4374 - regression_loss: 561.6691 - val_loss: 168.3229 - val_regression_loss: 67.2666\n","Epoch 32/50\n","537/537 [==============================] - 0s 52us/step - loss: 1350.5859 - regression_loss: 555.2719 - val_loss: 167.0411 - val_regression_loss: 66.6107\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 1308.9291 - regression_loss: 535.0246 - val_loss: 165.6758 - val_regression_loss: 65.9670\n","Epoch 34/50\n","537/537 [==============================] - 0s 57us/step - loss: 1309.7965 - regression_loss: 532.5431 - val_loss: 165.0997 - val_regression_loss: 65.7333\n","Epoch 35/50\n","537/537 [==============================] - 0s 55us/step - loss: 1313.6726 - regression_loss: 536.9574 - val_loss: 165.0725 - val_regression_loss: 65.7736\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 1329.9494 - regression_loss: 543.3946 - val_loss: 165.5975 - val_regression_loss: 66.0551\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 1305.8405 - regression_loss: 532.0423 - val_loss: 166.3647 - val_regression_loss: 66.4294\n","Epoch 38/50\n","537/537 [==============================] - 0s 50us/step - loss: 1274.1167 - regression_loss: 519.9900 - val_loss: 166.9199 - val_regression_loss: 66.6946\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 1261.9602 - regression_loss: 515.0552 - val_loss: 167.2248 - val_regression_loss: 66.8421\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 1255.0503 - regression_loss: 510.8768 - val_loss: 166.5546 - val_regression_loss: 66.4746\n","Epoch 41/50\n","537/537 [==============================] - 0s 52us/step - loss: 1287.6932 - regression_loss: 524.7344 - val_loss: 165.0142 - val_regression_loss: 65.6900\n","Epoch 42/50\n","537/537 [==============================] - 0s 53us/step - loss: 1255.1188 - regression_loss: 509.2487 - val_loss: 163.7441 - val_regression_loss: 65.0472\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 1259.3070 - regression_loss: 512.2660 - val_loss: 162.8818 - val_regression_loss: 64.6187\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 1237.7878 - regression_loss: 503.0036 - val_loss: 162.7833 - val_regression_loss: 64.5448\n","Epoch 45/50\n","537/537 [==============================] - 0s 51us/step - loss: 1201.5637 - regression_loss: 484.1690 - val_loss: 162.9786 - val_regression_loss: 64.6146\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 1252.9691 - regression_loss: 509.2320 - val_loss: 163.0336 - val_regression_loss: 64.6364\n","Epoch 47/50\n","537/537 [==============================] - 0s 51us/step - loss: 1226.6615 - regression_loss: 498.8793 - val_loss: 161.9111 - val_regression_loss: 64.0716\n","Epoch 48/50\n","537/537 [==============================] - 0s 48us/step - loss: 1217.3572 - regression_loss: 494.6629 - val_loss: 160.8802 - val_regression_loss: 63.5488\n","Epoch 49/50\n","537/537 [==============================] - 0s 50us/step - loss: 1171.6419 - regression_loss: 470.6749 - val_loss: 160.3371 - val_regression_loss: 63.2544\n","Epoch 50/50\n","537/537 [==============================] - 0s 46us/step - loss: 1212.5091 - regression_loss: 491.3397 - val_loss: 159.9442 - val_regression_loss: 63.0279\n","***************************** elapsed_time is:  3.135211944580078\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 3153437.3349 - regression_loss: 1578685.3936 - val_loss: 353652.7500 - val_regression_loss: 177090.8438\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 3131750.0293 - regression_loss: 1568036.4061 - val_loss: 342254.4688 - val_regression_loss: 171417.7188\n","Epoch 3/50\n","537/537 [==============================] - 0s 48us/step - loss: 3009232.5655 - regression_loss: 1506983.2819 - val_loss: 325749.4062 - val_regression_loss: 163206.0312\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 2911127.1490 - regression_loss: 1458308.7445 - val_loss: 302886.5000 - val_regression_loss: 151842.4688\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 2688312.5864 - regression_loss: 1347417.7072 - val_loss: 273238.4375 - val_regression_loss: 137142.7500\n","Epoch 6/50\n","537/537 [==============================] - 0s 46us/step - loss: 2444909.2074 - regression_loss: 1226776.8291 - val_loss: 237252.0312 - val_regression_loss: 119396.3984\n","Epoch 7/50\n","537/537 [==============================] - 0s 56us/step - loss: 2095458.0245 - regression_loss: 1054013.9525 - val_loss: 195661.4531 - val_regression_loss: 99147.4297\n","Epoch 8/50\n","537/537 [==============================] - 0s 60us/step - loss: 1810519.0557 - regression_loss: 916304.7956 - val_loss: 148978.7500 - val_regression_loss: 77092.2500\n","Epoch 9/50\n","537/537 [==============================] - 0s 47us/step - loss: 1386160.4550 - regression_loss: 714753.8764 - val_loss: 98694.9844 - val_regression_loss: 54245.8125\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 929501.0473 - regression_loss: 504876.6465 - val_loss: 54669.1289 - val_regression_loss: 32455.6738\n","Epoch 11/50\n","537/537 [==============================] - 0s 49us/step - loss: 627632.1554 - regression_loss: 358125.6064 - val_loss: 29067.9688 - val_regression_loss: 16040.7383\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 402445.9860 - regression_loss: 215956.4644 - val_loss: 30818.7637 - val_regression_loss: 11959.5703\n","Epoch 13/50\n","537/537 [==============================] - 0s 44us/step - loss: 382413.9061 - regression_loss: 166036.9862 - val_loss: 53885.5781 - val_regression_loss: 19555.6250\n","Epoch 14/50\n","537/537 [==============================] - 0s 45us/step - loss: 570896.3437 - regression_loss: 229782.1954 - val_loss: 59579.4258 - val_regression_loss: 21654.0625\n","Epoch 15/50\n","537/537 [==============================] - 0s 56us/step - loss: 599991.2088 - regression_loss: 239330.7790 - val_loss: 45077.5742 - val_regression_loss: 16039.4863\n","Epoch 16/50\n","537/537 [==============================] - 0s 57us/step - loss: 483297.3400 - regression_loss: 193358.5984 - val_loss: 29883.4414 - val_regression_loss: 10961.5508\n","Epoch 17/50\n","537/537 [==============================] - 0s 48us/step - loss: 373286.7523 - regression_loss: 158804.5948 - val_loss: 23315.0996 - val_regression_loss: 10054.2910\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 328784.1771 - regression_loss: 154813.3073 - val_loss: 23980.6328 - val_regression_loss: 12166.0566\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 317444.2680 - regression_loss: 163541.1935 - val_loss: 26679.3008 - val_regression_loss: 14563.6660\n","Epoch 20/50\n","537/537 [==============================] - 0s 57us/step - loss: 353681.7443 - regression_loss: 190436.5234 - val_loss: 27825.7949 - val_regression_loss: 15521.8730\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 365628.8235 - regression_loss: 200278.0460 - val_loss: 26695.7207 - val_regression_loss: 14801.6289\n","Epoch 22/50\n","537/537 [==============================] - 0s 54us/step - loss: 344842.8716 - regression_loss: 187734.2389 - val_loss: 24104.0820 - val_regression_loss: 12937.9102\n","Epoch 23/50\n","537/537 [==============================] - 0s 51us/step - loss: 324590.6176 - regression_loss: 173608.5836 - val_loss: 21416.5742 - val_regression_loss: 10709.9375\n","Epoch 24/50\n","537/537 [==============================] - 0s 52us/step - loss: 296512.2184 - regression_loss: 153275.3766 - val_loss: 20095.0449 - val_regression_loss: 8992.8164\n","Epoch 25/50\n","537/537 [==============================] - 0s 49us/step - loss: 268730.1747 - regression_loss: 130760.7064 - val_loss: 20828.1777 - val_regression_loss: 8342.0459\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 253876.5826 - regression_loss: 115338.3301 - val_loss: 22858.4531 - val_regression_loss: 8597.9307\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 262904.7845 - regression_loss: 114897.8734 - val_loss: 24551.3184 - val_regression_loss: 9096.4326\n","Epoch 28/50\n","537/537 [==============================] - 0s 47us/step - loss: 258179.7291 - regression_loss: 110419.1883 - val_loss: 24764.1934 - val_regression_loss: 9304.4697\n","Epoch 29/50\n","537/537 [==============================] - 0s 48us/step - loss: 244465.7122 - regression_loss: 105313.7474 - val_loss: 23770.3848 - val_regression_loss: 9230.0820\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 225582.9034 - regression_loss: 98999.3388 - val_loss: 22274.8281 - val_regression_loss: 9032.2637\n","Epoch 31/50\n","537/537 [==============================] - 0s 53us/step - loss: 221069.2181 - regression_loss: 101879.8637 - val_loss: 20970.2988 - val_regression_loss: 8850.3535\n","Epoch 32/50\n","537/537 [==============================] - 0s 65us/step - loss: 213910.7407 - regression_loss: 102714.2732 - val_loss: 19916.2344 - val_regression_loss: 8607.5596\n","Epoch 33/50\n","537/537 [==============================] - 0s 53us/step - loss: 201791.5018 - regression_loss: 97908.9861 - val_loss: 19020.2207 - val_regression_loss: 8247.0684\n","Epoch 34/50\n","537/537 [==============================] - 0s 50us/step - loss: 208663.7457 - regression_loss: 103014.4583 - val_loss: 18396.7324 - val_regression_loss: 7810.9355\n","Epoch 35/50\n","537/537 [==============================] - 0s 54us/step - loss: 199662.4715 - regression_loss: 97986.0506 - val_loss: 18053.0195 - val_regression_loss: 7392.1313\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 194447.5509 - regression_loss: 93445.2795 - val_loss: 18082.4160 - val_regression_loss: 7147.3535\n","Epoch 37/50\n","537/537 [==============================] - 0s 55us/step - loss: 183812.9155 - regression_loss: 86167.9169 - val_loss: 18194.8730 - val_regression_loss: 7034.9902\n","Epoch 38/50\n","537/537 [==============================] - 0s 56us/step - loss: 175285.6545 - regression_loss: 80976.6642 - val_loss: 18072.3633 - val_regression_loss: 6957.4980\n","Epoch 39/50\n","537/537 [==============================] - 0s 51us/step - loss: 154914.1938 - regression_loss: 70419.2095 - val_loss: 17441.0586 - val_regression_loss: 6807.6445\n","Epoch 40/50\n","537/537 [==============================] - 0s 51us/step - loss: 152934.4713 - regression_loss: 72380.9401 - val_loss: 16626.1523 - val_regression_loss: 6642.6548\n","Epoch 41/50\n","537/537 [==============================] - 0s 51us/step - loss: 153198.8742 - regression_loss: 74546.4203 - val_loss: 15575.3223 - val_regression_loss: 6412.5669\n","Epoch 42/50\n","537/537 [==============================] - 0s 55us/step - loss: 143841.6716 - regression_loss: 71792.1763 - val_loss: 14482.7295 - val_regression_loss: 6161.0327\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 136266.6654 - regression_loss: 69305.6070 - val_loss: 13439.2168 - val_regression_loss: 5898.8750\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 123794.4254 - regression_loss: 63519.6313 - val_loss: 12666.6035 - val_regression_loss: 5717.4438\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 119834.2648 - regression_loss: 62328.5598 - val_loss: 12486.5322 - val_regression_loss: 5744.3701\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 113319.0561 - regression_loss: 58324.9131 - val_loss: 12769.1553 - val_regression_loss: 5960.5732\n","Epoch 47/50\n","537/537 [==============================] - 0s 47us/step - loss: 107487.4044 - regression_loss: 54541.7341 - val_loss: 13142.1631 - val_regression_loss: 6206.3066\n","Epoch 48/50\n","537/537 [==============================] - 0s 51us/step - loss: 100736.4088 - regression_loss: 50558.6767 - val_loss: 13083.9463 - val_regression_loss: 6231.0830\n","Epoch 49/50\n","537/537 [==============================] - 0s 47us/step - loss: 92155.0265 - regression_loss: 45999.7736 - val_loss: 12571.7002 - val_regression_loss: 6019.6455\n","Epoch 50/50\n","537/537 [==============================] - 0s 59us/step - loss: 93334.2506 - regression_loss: 46806.7013 - val_loss: 11981.8828 - val_regression_loss: 5748.4531\n","***************************** elapsed_time is:  3.0784475803375244\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 24918.9880 - regression_loss: 12235.5109 - val_loss: 1957.7483 - val_regression_loss: 951.6102\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 19152.9151 - regression_loss: 9365.2884 - val_loss: 1336.6864 - val_regression_loss: 643.5594\n","Epoch 3/50\n","537/537 [==============================] - 0s 52us/step - loss: 13343.2211 - regression_loss: 6481.0001 - val_loss: 731.8269 - val_regression_loss: 344.1940\n","Epoch 4/50\n","537/537 [==============================] - 0s 53us/step - loss: 7422.4509 - regression_loss: 3545.3226 - val_loss: 500.4180 - val_regression_loss: 232.1045\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 4367.3434 - regression_loss: 2047.7835 - val_loss: 684.9883 - val_regression_loss: 325.9270\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 5080.0994 - regression_loss: 2416.1531 - val_loss: 568.7322 - val_regression_loss: 266.0764\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 4389.8095 - regression_loss: 2057.5723 - val_loss: 332.2182 - val_regression_loss: 145.6270\n","Epoch 8/50\n","537/537 [==============================] - 0s 46us/step - loss: 2992.3813 - regression_loss: 1341.3209 - val_loss: 276.0916 - val_regression_loss: 116.4627\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 3009.2489 - regression_loss: 1341.5925 - val_loss: 284.0378 - val_regression_loss: 120.8426\n","Epoch 10/50\n","537/537 [==============================] - 0s 63us/step - loss: 3279.8406 - regression_loss: 1482.0418 - val_loss: 228.6582 - val_regression_loss: 94.5250\n","Epoch 11/50\n","537/537 [==============================] - 0s 48us/step - loss: 2755.3452 - regression_loss: 1233.3396 - val_loss: 168.1559 - val_regression_loss: 65.9912\n","Epoch 12/50\n","537/537 [==============================] - 0s 45us/step - loss: 1920.8304 - regression_loss: 831.4206 - val_loss: 190.9059 - val_regression_loss: 78.8853\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 1867.6514 - regression_loss: 818.4879 - val_loss: 266.4348 - val_regression_loss: 117.4883\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 2238.4370 - regression_loss: 1010.5758 - val_loss: 276.9260 - val_regression_loss: 122.6937\n","Epoch 15/50\n","537/537 [==============================] - 0s 47us/step - loss: 2229.9911 - regression_loss: 1007.9016 - val_loss: 222.7079 - val_regression_loss: 94.9316\n","Epoch 16/50\n","537/537 [==============================] - 0s 48us/step - loss: 1856.8742 - regression_loss: 818.4301 - val_loss: 182.8544 - val_regression_loss: 74.1205\n","Epoch 17/50\n","537/537 [==============================] - 0s 49us/step - loss: 1589.0305 - regression_loss: 674.2045 - val_loss: 180.5040 - val_regression_loss: 72.1575\n","Epoch 18/50\n","537/537 [==============================] - 0s 54us/step - loss: 1701.8735 - regression_loss: 725.9167 - val_loss: 186.8030 - val_regression_loss: 74.7684\n","Epoch 19/50\n","537/537 [==============================] - 0s 48us/step - loss: 1698.4404 - regression_loss: 720.4424 - val_loss: 188.1313 - val_regression_loss: 75.1894\n","Epoch 20/50\n","537/537 [==============================] - 0s 45us/step - loss: 1702.1889 - regression_loss: 719.2182 - val_loss: 191.4050 - val_regression_loss: 76.9098\n","Epoch 21/50\n","537/537 [==============================] - 0s 51us/step - loss: 1551.4298 - regression_loss: 648.2888 - val_loss: 199.2719 - val_regression_loss: 81.2313\n","Epoch 22/50\n","537/537 [==============================] - 0s 46us/step - loss: 1553.5067 - regression_loss: 652.0853 - val_loss: 201.6069 - val_regression_loss: 82.9899\n","Epoch 23/50\n","537/537 [==============================] - 0s 48us/step - loss: 1505.2208 - regression_loss: 632.5945 - val_loss: 193.3615 - val_regression_loss: 79.4112\n","Epoch 24/50\n","537/537 [==============================] - 0s 48us/step - loss: 1498.6773 - regression_loss: 631.3184 - val_loss: 183.6136 - val_regression_loss: 74.8463\n","Epoch 25/50\n","537/537 [==============================] - 0s 47us/step - loss: 1471.6345 - regression_loss: 621.3972 - val_loss: 177.8636 - val_regression_loss: 71.9913\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1436.5061 - regression_loss: 604.7416 - val_loss: 176.7765 - val_regression_loss: 71.2789\n","Epoch 27/50\n","537/537 [==============================] - 0s 54us/step - loss: 1418.7532 - regression_loss: 594.2076 - val_loss: 178.5557 - val_regression_loss: 71.8845\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 1388.0254 - regression_loss: 576.3865 - val_loss: 185.6461 - val_regression_loss: 75.1041\n","Epoch 29/50\n","537/537 [==============================] - 0s 56us/step - loss: 1369.6975 - regression_loss: 562.9644 - val_loss: 195.4498 - val_regression_loss: 79.7664\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 1367.7908 - regression_loss: 562.6267 - val_loss: 198.0144 - val_regression_loss: 80.9746\n","Epoch 31/50\n","537/537 [==============================] - 0s 56us/step - loss: 1350.3526 - regression_loss: 552.5842 - val_loss: 192.1462 - val_regression_loss: 78.1323\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 1319.1163 - regression_loss: 537.1010 - val_loss: 184.8918 - val_regression_loss: 74.6667\n","Epoch 33/50\n","537/537 [==============================] - 0s 48us/step - loss: 1313.2771 - regression_loss: 537.0316 - val_loss: 180.7713 - val_regression_loss: 72.7612\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 1297.2689 - regression_loss: 530.0916 - val_loss: 180.8624 - val_regression_loss: 72.9259\n","Epoch 35/50\n","537/537 [==============================] - 0s 53us/step - loss: 1297.2836 - regression_loss: 531.4950 - val_loss: 183.3995 - val_regression_loss: 74.2206\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 1284.9267 - regression_loss: 523.9035 - val_loss: 185.8844 - val_regression_loss: 75.3940\n","Epoch 37/50\n","537/537 [==============================] - 0s 46us/step - loss: 1267.4281 - regression_loss: 515.2500 - val_loss: 187.1421 - val_regression_loss: 75.9040\n","Epoch 38/50\n","537/537 [==============================] - 0s 48us/step - loss: 1271.3553 - regression_loss: 519.1936 - val_loss: 185.7159 - val_regression_loss: 75.0541\n","Epoch 39/50\n","537/537 [==============================] - 0s 53us/step - loss: 1251.5445 - regression_loss: 509.0048 - val_loss: 184.6178 - val_regression_loss: 74.4073\n","Epoch 40/50\n","537/537 [==============================] - 0s 48us/step - loss: 1261.5815 - regression_loss: 509.7371 - val_loss: 183.8322 - val_regression_loss: 73.9866\n","Epoch 41/50\n","537/537 [==============================] - 0s 50us/step - loss: 1235.4045 - regression_loss: 499.0917 - val_loss: 183.9429 - val_regression_loss: 74.0626\n","Epoch 42/50\n","537/537 [==============================] - 0s 59us/step - loss: 1230.3672 - regression_loss: 497.3658 - val_loss: 183.4086 - val_regression_loss: 73.8481\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 1205.3670 - regression_loss: 485.1903 - val_loss: 182.8231 - val_regression_loss: 73.6070\n","Epoch 44/50\n","537/537 [==============================] - 0s 49us/step - loss: 1224.4159 - regression_loss: 494.3759 - val_loss: 181.4998 - val_regression_loss: 72.9602\n","Epoch 45/50\n","537/537 [==============================] - 0s 46us/step - loss: 1214.7402 - regression_loss: 488.5858 - val_loss: 180.1403 - val_regression_loss: 72.2467\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 1185.2905 - regression_loss: 474.7046 - val_loss: 179.9730 - val_regression_loss: 72.1112\n","Epoch 47/50\n","537/537 [==============================] - 0s 47us/step - loss: 1194.4594 - regression_loss: 478.7968 - val_loss: 180.1395 - val_regression_loss: 72.1283\n","Epoch 48/50\n","537/537 [==============================] - 0s 52us/step - loss: 1171.0079 - regression_loss: 466.5882 - val_loss: 180.9080 - val_regression_loss: 72.4665\n","Epoch 49/50\n","537/537 [==============================] - 0s 48us/step - loss: 1172.0367 - regression_loss: 467.9803 - val_loss: 181.0796 - val_regression_loss: 72.5704\n","Epoch 50/50\n","537/537 [==============================] - 0s 57us/step - loss: 1177.6397 - regression_loss: 470.9396 - val_loss: 179.1472 - val_regression_loss: 71.6120\n","***************************** elapsed_time is:  3.3785483837127686\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 9328.2363 - regression_loss: 4486.2833 - val_loss: 774.8954 - val_regression_loss: 364.6653\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 6600.5412 - regression_loss: 3131.8465 - val_loss: 485.2810 - val_regression_loss: 220.9026\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 4003.0093 - regression_loss: 1841.5285 - val_loss: 312.6989 - val_regression_loss: 135.7534\n","Epoch 4/50\n","537/537 [==============================] - 0s 49us/step - loss: 2405.9720 - regression_loss: 1052.1530 - val_loss: 261.4659 - val_regression_loss: 111.1062\n","Epoch 5/50\n","537/537 [==============================] - 0s 45us/step - loss: 1914.2160 - regression_loss: 814.0424 - val_loss: 260.8134 - val_regression_loss: 111.2167\n","Epoch 6/50\n","537/537 [==============================] - 0s 50us/step - loss: 1852.4858 - regression_loss: 785.9550 - val_loss: 287.4155 - val_regression_loss: 124.7121\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 2119.6035 - regression_loss: 920.7723 - val_loss: 264.5246 - val_regression_loss: 113.7865\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 1944.2626 - regression_loss: 836.3134 - val_loss: 202.5427 - val_regression_loss: 83.4952\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 1472.1361 - regression_loss: 607.1291 - val_loss: 171.3811 - val_regression_loss: 68.5347\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 1317.0662 - regression_loss: 534.0055 - val_loss: 176.5040 - val_regression_loss: 71.5014\n","Epoch 11/50\n","537/537 [==============================] - 0s 43us/step - loss: 1397.5612 - regression_loss: 578.2802 - val_loss: 186.8392 - val_regression_loss: 76.8470\n","Epoch 12/50\n","537/537 [==============================] - 0s 44us/step - loss: 1508.2105 - regression_loss: 634.9413 - val_loss: 179.0585 - val_regression_loss: 72.9521\n","Epoch 13/50\n","537/537 [==============================] - 0s 47us/step - loss: 1438.9943 - regression_loss: 603.3009 - val_loss: 164.2382 - val_regression_loss: 65.4120\n","Epoch 14/50\n","537/537 [==============================] - 0s 46us/step - loss: 1285.4625 - regression_loss: 522.4542 - val_loss: 159.4946 - val_regression_loss: 62.8303\n","Epoch 15/50\n","537/537 [==============================] - 0s 51us/step - loss: 1230.0326 - regression_loss: 495.8390 - val_loss: 166.3156 - val_regression_loss: 66.0054\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 1234.7200 - regression_loss: 495.1766 - val_loss: 174.7094 - val_regression_loss: 70.0235\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 1299.3795 - regression_loss: 523.6698 - val_loss: 176.0097 - val_regression_loss: 70.6186\n","Epoch 18/50\n","537/537 [==============================] - 0s 54us/step - loss: 1277.6135 - regression_loss: 516.7201 - val_loss: 170.0747 - val_regression_loss: 67.7218\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 1226.2993 - regression_loss: 490.1520 - val_loss: 162.9819 - val_regression_loss: 64.3046\n","Epoch 20/50\n","537/537 [==============================] - 0s 46us/step - loss: 1223.7506 - regression_loss: 491.9494 - val_loss: 158.3385 - val_regression_loss: 62.0997\n","Epoch 21/50\n","537/537 [==============================] - 0s 48us/step - loss: 1196.1362 - regression_loss: 480.5023 - val_loss: 156.2018 - val_regression_loss: 61.1002\n","Epoch 22/50\n","537/537 [==============================] - 0s 49us/step - loss: 1211.0257 - regression_loss: 488.4907 - val_loss: 155.0123 - val_regression_loss: 60.5138\n","Epoch 23/50\n","537/537 [==============================] - 0s 54us/step - loss: 1213.5877 - regression_loss: 492.2155 - val_loss: 154.3043 - val_regression_loss: 60.1219\n","Epoch 24/50\n","537/537 [==============================] - 0s 51us/step - loss: 1207.6528 - regression_loss: 486.3755 - val_loss: 155.0115 - val_regression_loss: 60.4085\n","Epoch 25/50\n","537/537 [==============================] - 0s 51us/step - loss: 1183.9714 - regression_loss: 474.4837 - val_loss: 157.1182 - val_regression_loss: 61.3786\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1192.2394 - regression_loss: 478.1159 - val_loss: 159.7290 - val_regression_loss: 62.6075\n","Epoch 27/50\n","537/537 [==============================] - 0s 48us/step - loss: 1166.5582 - regression_loss: 464.2354 - val_loss: 160.2233 - val_regression_loss: 62.8090\n","Epoch 28/50\n","537/537 [==============================] - 0s 48us/step - loss: 1189.3382 - regression_loss: 476.9601 - val_loss: 159.3720 - val_regression_loss: 62.3754\n","Epoch 29/50\n","537/537 [==============================] - 0s 52us/step - loss: 1188.9050 - regression_loss: 475.7697 - val_loss: 157.4066 - val_regression_loss: 61.4129\n","Epoch 30/50\n","537/537 [==============================] - 0s 48us/step - loss: 1169.0267 - regression_loss: 467.6299 - val_loss: 156.0199 - val_regression_loss: 60.7527\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 1161.8358 - regression_loss: 466.4783 - val_loss: 155.5153 - val_regression_loss: 60.5303\n","Epoch 32/50\n","537/537 [==============================] - 0s 50us/step - loss: 1189.1916 - regression_loss: 479.0959 - val_loss: 156.1152 - val_regression_loss: 60.8488\n","Epoch 33/50\n","537/537 [==============================] - 0s 55us/step - loss: 1169.1494 - regression_loss: 468.8165 - val_loss: 156.4262 - val_regression_loss: 61.0024\n","Epoch 34/50\n","537/537 [==============================] - 0s 47us/step - loss: 1170.8245 - regression_loss: 472.1278 - val_loss: 156.4574 - val_regression_loss: 61.0070\n","Epoch 35/50\n","537/537 [==============================] - 0s 45us/step - loss: 1165.8741 - regression_loss: 467.0534 - val_loss: 156.1560 - val_regression_loss: 60.8414\n","Epoch 36/50\n","537/537 [==============================] - 0s 46us/step - loss: 1145.5991 - regression_loss: 459.1992 - val_loss: 156.5504 - val_regression_loss: 61.0275\n","Epoch 37/50\n","537/537 [==============================] - 0s 53us/step - loss: 1156.4492 - regression_loss: 462.8307 - val_loss: 157.5447 - val_regression_loss: 61.5195\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 1156.2790 - regression_loss: 464.5443 - val_loss: 157.1692 - val_regression_loss: 61.3383\n","Epoch 39/50\n","537/537 [==============================] - 0s 57us/step - loss: 1142.2732 - regression_loss: 457.2006 - val_loss: 156.2837 - val_regression_loss: 60.9054\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 1143.4389 - regression_loss: 458.1132 - val_loss: 155.5989 - val_regression_loss: 60.5832\n","Epoch 41/50\n","537/537 [==============================] - 0s 51us/step - loss: 1131.5452 - regression_loss: 456.5313 - val_loss: 154.9795 - val_regression_loss: 60.2798\n","Epoch 42/50\n","537/537 [==============================] - 0s 47us/step - loss: 1129.9429 - regression_loss: 455.7639 - val_loss: 154.5238 - val_regression_loss: 60.0596\n","Epoch 43/50\n","537/537 [==============================] - 0s 50us/step - loss: 1111.3802 - regression_loss: 444.6095 - val_loss: 154.7592 - val_regression_loss: 60.1797\n","Epoch 44/50\n","537/537 [==============================] - 0s 49us/step - loss: 1143.2520 - regression_loss: 460.6721 - val_loss: 155.9202 - val_regression_loss: 60.7600\n","Epoch 45/50\n","537/537 [==============================] - 0s 48us/step - loss: 1134.8081 - regression_loss: 456.3145 - val_loss: 155.6158 - val_regression_loss: 60.6048\n","Epoch 46/50\n","537/537 [==============================] - 0s 46us/step - loss: 1138.9157 - regression_loss: 459.5423 - val_loss: 155.0845 - val_regression_loss: 60.3352\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 1115.9156 - regression_loss: 447.6341 - val_loss: 154.4434 - val_regression_loss: 60.0129\n","Epoch 48/50\n","537/537 [==============================] - 0s 49us/step - loss: 1136.3311 - regression_loss: 459.3973 - val_loss: 153.6710 - val_regression_loss: 59.6280\n","\n","Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 49/50\n","537/537 [==============================] - 0s 54us/step - loss: 1142.3412 - regression_loss: 461.5041 - val_loss: 153.4635 - val_regression_loss: 59.5279\n","Epoch 50/50\n","537/537 [==============================] - 0s 50us/step - loss: 1118.4654 - regression_loss: 450.2198 - val_loss: 153.6421 - val_regression_loss: 59.6199\n","***************************** elapsed_time is:  3.1465935707092285\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 273703.5631 - regression_loss: 136647.0080 - val_loss: 30944.5293 - val_regression_loss: 15446.1758\n","Epoch 2/50\n","537/537 [==============================] - 0s 50us/step - loss: 249009.9154 - regression_loss: 124315.8058 - val_loss: 27730.5918 - val_regression_loss: 13841.1660\n","Epoch 3/50\n","537/537 [==============================] - 0s 46us/step - loss: 224733.0515 - regression_loss: 112191.4297 - val_loss: 23326.0000 - val_regression_loss: 11640.8613\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 190673.4867 - regression_loss: 95176.6175 - val_loss: 17727.7344 - val_regression_loss: 8843.5312\n","Epoch 5/50\n","537/537 [==============================] - 0s 51us/step - loss: 144870.7281 - regression_loss: 72287.3169 - val_loss: 11705.6035 - val_regression_loss: 5833.8374\n","Epoch 6/50\n","537/537 [==============================] - 0s 43us/step - loss: 100236.9896 - regression_loss: 49979.3582 - val_loss: 6958.1709 - val_regression_loss: 3460.8691\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 64383.6352 - regression_loss: 32057.4243 - val_loss: 5711.6328 - val_regression_loss: 2837.7139\n","Epoch 8/50\n","537/537 [==============================] - 0s 46us/step - loss: 56195.9786 - regression_loss: 27962.0450 - val_loss: 6612.0498 - val_regression_loss: 3287.9976\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 62372.7398 - regression_loss: 31046.9330 - val_loss: 5720.3169 - val_regression_loss: 2842.5723\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 54696.6936 - regression_loss: 27214.2637 - val_loss: 4197.6460 - val_regression_loss: 2081.5747\n","Epoch 11/50\n","537/537 [==============================] - 0s 51us/step - loss: 43812.2147 - regression_loss: 21774.4514 - val_loss: 3472.5410 - val_regression_loss: 1719.1533\n","Epoch 12/50\n","537/537 [==============================] - 0s 46us/step - loss: 38171.8514 - regression_loss: 18954.8856 - val_loss: 3608.3601 - val_regression_loss: 1787.1677\n","Epoch 13/50\n","537/537 [==============================] - 0s 47us/step - loss: 37706.0424 - regression_loss: 18720.4247 - val_loss: 3914.0054 - val_regression_loss: 1940.1039\n","Epoch 14/50\n","537/537 [==============================] - 0s 46us/step - loss: 41152.3008 - regression_loss: 20447.1603 - val_loss: 3857.7979 - val_regression_loss: 1912.0626\n","Epoch 15/50\n","537/537 [==============================] - 0s 50us/step - loss: 40596.1916 - regression_loss: 20173.1966 - val_loss: 3421.2131 - val_regression_loss: 1693.7156\n","Epoch 16/50\n","537/537 [==============================] - 0s 59us/step - loss: 32719.1142 - regression_loss: 16232.4028 - val_loss: 2897.4653 - val_regression_loss: 1431.6072\n","Epoch 17/50\n","537/537 [==============================] - 0s 58us/step - loss: 28737.9265 - regression_loss: 14240.5813 - val_loss: 2582.7185 - val_regression_loss: 1273.8027\n","Epoch 18/50\n","537/537 [==============================] - 0s 45us/step - loss: 27210.2927 - regression_loss: 13475.2373 - val_loss: 2605.1458 - val_regression_loss: 1284.4635\n","Epoch 19/50\n","537/537 [==============================] - 0s 55us/step - loss: 27244.6613 - regression_loss: 13490.5039 - val_loss: 2782.0916 - val_regression_loss: 1372.4694\n","Epoch 20/50\n","537/537 [==============================] - 0s 49us/step - loss: 27865.0699 - regression_loss: 13797.4878 - val_loss: 2775.5474 - val_regression_loss: 1369.0277\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 26767.1535 - regression_loss: 13247.1740 - val_loss: 2536.1934 - val_regression_loss: 1249.4819\n","Epoch 22/50\n","537/537 [==============================] - 0s 48us/step - loss: 24285.9402 - regression_loss: 12008.3270 - val_loss: 2258.7812 - val_regression_loss: 1111.0709\n","Epoch 23/50\n","537/537 [==============================] - 0s 48us/step - loss: 22498.2922 - regression_loss: 11117.4167 - val_loss: 2090.0710 - val_regression_loss: 1027.0118\n","Epoch 24/50\n","537/537 [==============================] - 0s 56us/step - loss: 21394.1195 - regression_loss: 10566.1181 - val_loss: 1989.0817 - val_regression_loss: 976.7139\n","Epoch 25/50\n","537/537 [==============================] - 0s 47us/step - loss: 20767.9471 - regression_loss: 10254.7576 - val_loss: 1869.0477 - val_regression_loss: 916.7760\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 18731.3160 - regression_loss: 9237.9254 - val_loss: 1752.0808 - val_regression_loss: 858.2725\n","Epoch 27/50\n","537/537 [==============================] - 0s 51us/step - loss: 17747.8704 - regression_loss: 8747.4733 - val_loss: 1709.4478 - val_regression_loss: 836.8571\n","Epoch 28/50\n","537/537 [==============================] - 0s 48us/step - loss: 14452.9759 - regression_loss: 7100.1517 - val_loss: 1691.0001 - val_regression_loss: 827.5076\n","Epoch 29/50\n","537/537 [==============================] - 0s 48us/step - loss: 15183.2839 - regression_loss: 7464.8011 - val_loss: 1576.4198 - val_regression_loss: 770.1071\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 12556.4101 - regression_loss: 6149.5357 - val_loss: 1350.5197 - val_regression_loss: 657.0867\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 12827.0465 - regression_loss: 6285.8537 - val_loss: 1172.0094 - val_regression_loss: 567.7507\n","Epoch 32/50\n","537/537 [==============================] - 0s 48us/step - loss: 11807.7238 - regression_loss: 5775.2970 - val_loss: 1057.6361 - val_regression_loss: 510.4581\n","Epoch 33/50\n","537/537 [==============================] - 0s 49us/step - loss: 11012.5958 - regression_loss: 5374.8108 - val_loss: 983.6024 - val_regression_loss: 473.2963\n","Epoch 34/50\n","537/537 [==============================] - 0s 55us/step - loss: 10298.0727 - regression_loss: 5021.0185 - val_loss: 959.6174 - val_regression_loss: 461.1502\n","Epoch 35/50\n","537/537 [==============================] - 0s 45us/step - loss: 9499.1612 - regression_loss: 4619.6697 - val_loss: 952.1044 - val_regression_loss: 457.3202\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 8979.1776 - regression_loss: 4360.6493 - val_loss: 887.6315 - val_regression_loss: 425.1404\n","Epoch 37/50\n","537/537 [==============================] - 0s 48us/step - loss: 8344.6330 - regression_loss: 4046.4027 - val_loss: 776.5828 - val_regression_loss: 369.7866\n","Epoch 38/50\n","537/537 [==============================] - 0s 44us/step - loss: 7753.9364 - regression_loss: 3749.1403 - val_loss: 701.1090 - val_regression_loss: 332.2283\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 7045.0538 - regression_loss: 3396.9330 - val_loss: 667.8863 - val_regression_loss: 315.7293\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 6938.2860 - regression_loss: 3344.8606 - val_loss: 679.5590 - val_regression_loss: 321.5885\n","Epoch 41/50\n","537/537 [==============================] - 0s 51us/step - loss: 6434.4053 - regression_loss: 3091.8862 - val_loss: 691.6177 - val_regression_loss: 327.6575\n","Epoch 42/50\n","537/537 [==============================] - 0s 55us/step - loss: 6052.1507 - regression_loss: 2903.4812 - val_loss: 643.3588 - val_regression_loss: 303.6481\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 5647.4189 - regression_loss: 2698.2981 - val_loss: 585.1655 - val_regression_loss: 274.6961\n","Epoch 44/50\n","537/537 [==============================] - 0s 56us/step - loss: 4510.0505 - regression_loss: 2132.7630 - val_loss: 556.4293 - val_regression_loss: 260.4178\n","Epoch 45/50\n","537/537 [==============================] - 0s 54us/step - loss: 5030.2629 - regression_loss: 2392.8201 - val_loss: 567.2162 - val_regression_loss: 265.8095\n","Epoch 46/50\n","537/537 [==============================] - 0s 55us/step - loss: 4815.3365 - regression_loss: 2283.7595 - val_loss: 590.7762 - val_regression_loss: 277.5853\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 4428.3486 - regression_loss: 2092.6504 - val_loss: 562.5437 - val_regression_loss: 263.5642\n","Epoch 48/50\n","537/537 [==============================] - 0s 54us/step - loss: 4345.4995 - regression_loss: 2050.1734 - val_loss: 510.6758 - val_regression_loss: 237.7840\n","Epoch 49/50\n","537/537 [==============================] - 0s 58us/step - loss: 4203.9025 - regression_loss: 1980.5380 - val_loss: 502.1937 - val_regression_loss: 233.6080\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 3903.5433 - regression_loss: 1830.7099 - val_loss: 524.2131 - val_regression_loss: 244.6151\n","***************************** elapsed_time is:  3.093600034713745\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 7330.7596 - regression_loss: 3495.3978 - val_loss: 717.6703 - val_regression_loss: 335.7740\n","Epoch 2/50\n","537/537 [==============================] - 0s 58us/step - loss: 4985.7600 - regression_loss: 2320.9207 - val_loss: 527.3605 - val_regression_loss: 240.7082\n","Epoch 3/50\n","537/537 [==============================] - 0s 44us/step - loss: 3393.3234 - regression_loss: 1526.5678 - val_loss: 358.6222 - val_regression_loss: 157.6212\n","Epoch 4/50\n","537/537 [==============================] - 0s 45us/step - loss: 2109.5080 - regression_loss: 896.5056 - val_loss: 211.4556 - val_regression_loss: 86.3622\n","Epoch 5/50\n","537/537 [==============================] - 0s 50us/step - loss: 1354.2738 - regression_loss: 537.7867 - val_loss: 216.5499 - val_regression_loss: 91.2650\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 1897.8225 - regression_loss: 827.0941 - val_loss: 208.0315 - val_regression_loss: 87.7388\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 1946.4865 - regression_loss: 857.5246 - val_loss: 174.5088 - val_regression_loss: 70.6290\n","Epoch 8/50\n","537/537 [==============================] - 0s 47us/step - loss: 1501.4214 - regression_loss: 632.3106 - val_loss: 170.9975 - val_regression_loss: 68.2028\n","Epoch 9/50\n","537/537 [==============================] - 0s 52us/step - loss: 1276.8829 - regression_loss: 515.3462 - val_loss: 191.2217 - val_regression_loss: 77.8036\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 1311.3879 - regression_loss: 526.8192 - val_loss: 214.8595 - val_regression_loss: 89.3697\n","Epoch 11/50\n","537/537 [==============================] - 0s 50us/step - loss: 1483.6973 - regression_loss: 613.5696 - val_loss: 211.9073 - val_regression_loss: 87.8401\n","Epoch 12/50\n","537/537 [==============================] - 0s 46us/step - loss: 1439.7275 - regression_loss: 590.7272 - val_loss: 191.1913 - val_regression_loss: 77.6116\n","Epoch 13/50\n","537/537 [==============================] - 0s 48us/step - loss: 1324.8285 - regression_loss: 537.1460 - val_loss: 171.2558 - val_regression_loss: 67.9611\n","Epoch 14/50\n","537/537 [==============================] - 0s 54us/step - loss: 1248.8326 - regression_loss: 499.0483 - val_loss: 158.1833 - val_regression_loss: 61.8691\n","Epoch 15/50\n","537/537 [==============================] - 0s 53us/step - loss: 1226.2953 - regression_loss: 493.7588 - val_loss: 154.7008 - val_regression_loss: 60.5020\n","Epoch 16/50\n","537/537 [==============================] - 0s 48us/step - loss: 1257.4978 - regression_loss: 509.9172 - val_loss: 154.5532 - val_regression_loss: 60.5596\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 1260.8940 - regression_loss: 514.5182 - val_loss: 153.1249 - val_regression_loss: 59.7289\n","Epoch 18/50\n","537/537 [==============================] - 0s 52us/step - loss: 1220.5411 - regression_loss: 497.2563 - val_loss: 154.7491 - val_regression_loss: 60.3058\n","Epoch 19/50\n","537/537 [==============================] - 0s 50us/step - loss: 1232.1675 - regression_loss: 498.9029 - val_loss: 160.0609 - val_regression_loss: 62.6957\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 1209.0378 - regression_loss: 488.7944 - val_loss: 165.3169 - val_regression_loss: 65.1373\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 1217.6991 - regression_loss: 489.3114 - val_loss: 167.1062 - val_regression_loss: 65.9476\n","Epoch 22/50\n","537/537 [==============================] - 0s 46us/step - loss: 1223.7496 - regression_loss: 494.0065 - val_loss: 164.4325 - val_regression_loss: 64.6370\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 1209.8245 - regression_loss: 490.4808 - val_loss: 159.7783 - val_regression_loss: 62.4134\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 1189.7789 - regression_loss: 478.2792 - val_loss: 155.5917 - val_regression_loss: 60.4644\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 1191.4487 - regression_loss: 480.4168 - val_loss: 152.8346 - val_regression_loss: 59.2175\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 1190.5800 - regression_loss: 482.4851 - val_loss: 151.9058 - val_regression_loss: 58.8180\n","Epoch 27/50\n","537/537 [==============================] - 0s 48us/step - loss: 1177.0108 - regression_loss: 477.1021 - val_loss: 151.9742 - val_regression_loss: 58.8156\n","Epoch 28/50\n","537/537 [==============================] - 0s 56us/step - loss: 1167.9345 - regression_loss: 473.1046 - val_loss: 153.0685 - val_regression_loss: 59.2359\n","Epoch 29/50\n","537/537 [==============================] - 0s 47us/step - loss: 1165.4106 - regression_loss: 469.9562 - val_loss: 155.0546 - val_regression_loss: 60.0912\n","Epoch 30/50\n","537/537 [==============================] - 0s 50us/step - loss: 1183.1639 - regression_loss: 480.8751 - val_loss: 156.5452 - val_regression_loss: 60.7497\n","Epoch 31/50\n","537/537 [==============================] - 0s 48us/step - loss: 1191.4599 - regression_loss: 481.3526 - val_loss: 156.7174 - val_regression_loss: 60.8130\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 1184.3936 - regression_loss: 478.4180 - val_loss: 155.7709 - val_regression_loss: 60.3575\n","Epoch 33/50\n","537/537 [==============================] - 0s 47us/step - loss: 1159.0459 - regression_loss: 467.0551 - val_loss: 154.3444 - val_regression_loss: 59.6702\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 1181.9419 - regression_loss: 481.4227 - val_loss: 152.9750 - val_regression_loss: 59.0277\n","Epoch 35/50\n","537/537 [==============================] - 0s 47us/step - loss: 1165.7152 - regression_loss: 472.0770 - val_loss: 151.7448 - val_regression_loss: 58.4635\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 1184.6642 - regression_loss: 481.6198 - val_loss: 151.6135 - val_regression_loss: 58.3906\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 1166.5946 - regression_loss: 474.8129 - val_loss: 152.3429 - val_regression_loss: 58.6839\n","Epoch 38/50\n","537/537 [==============================] - 0s 62us/step - loss: 1170.7981 - regression_loss: 475.6780 - val_loss: 153.0602 - val_regression_loss: 58.9746\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 39/50\n","537/537 [==============================] - 0s 55us/step - loss: 1175.9017 - regression_loss: 477.6218 - val_loss: 153.2466 - val_regression_loss: 59.0417\n","Epoch 40/50\n","537/537 [==============================] - 0s 51us/step - loss: 1172.5267 - regression_loss: 477.6086 - val_loss: 153.3206 - val_regression_loss: 59.0617\n","Epoch 41/50\n","537/537 [==============================] - 0s 51us/step - loss: 1163.8881 - regression_loss: 471.0598 - val_loss: 153.1525 - val_regression_loss: 58.9738\n","Epoch 42/50\n","537/537 [==============================] - 0s 52us/step - loss: 1164.1576 - regression_loss: 471.3911 - val_loss: 152.7884 - val_regression_loss: 58.7965\n","Epoch 43/50\n","537/537 [==============================] - 0s 48us/step - loss: 1158.8345 - regression_loss: 469.0711 - val_loss: 152.5175 - val_regression_loss: 58.6605\n","***************************** elapsed_time is:  3.1868767738342285\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 38779.1227 - regression_loss: 19443.0353 - val_loss: 3414.9473 - val_regression_loss: 1705.7017\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 28724.4216 - regression_loss: 14357.8761 - val_loss: 2272.4285 - val_regression_loss: 1122.9250\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 19062.8470 - regression_loss: 9444.5332 - val_loss: 1311.2230 - val_regression_loss: 624.1593\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 11173.9981 - regression_loss: 5349.6474 - val_loss: 1077.3374 - val_regression_loss: 483.1277\n","Epoch 5/50\n","537/537 [==============================] - 0s 50us/step - loss: 8454.2705 - regression_loss: 3799.0715 - val_loss: 1087.7910 - val_regression_loss: 482.1319\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 8189.0413 - regression_loss: 3621.5208 - val_loss: 699.0032 - val_regression_loss: 300.9855\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 5252.7327 - regression_loss: 2257.9675 - val_loss: 395.9700 - val_regression_loss: 168.8978\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 2962.7079 - regression_loss: 1274.6764 - val_loss: 424.3458 - val_regression_loss: 199.3830\n","Epoch 9/50\n","537/537 [==============================] - 0s 53us/step - loss: 3427.3503 - regression_loss: 1634.9659 - val_loss: 553.5574 - val_regression_loss: 272.0543\n","Epoch 10/50\n","537/537 [==============================] - 0s 52us/step - loss: 4610.2740 - regression_loss: 2290.4722 - val_loss: 524.1227 - val_regression_loss: 257.4084\n","Epoch 11/50\n","537/537 [==============================] - 0s 57us/step - loss: 4350.7377 - regression_loss: 2158.0349 - val_loss: 388.9625 - val_regression_loss: 185.7919\n","Epoch 12/50\n","537/537 [==============================] - 0s 47us/step - loss: 3182.6974 - regression_loss: 1538.7034 - val_loss: 278.8854 - val_regression_loss: 125.4468\n","Epoch 13/50\n","537/537 [==============================] - 0s 50us/step - loss: 2327.9818 - regression_loss: 1063.8487 - val_loss: 253.4968 - val_regression_loss: 107.8222\n","Epoch 14/50\n","537/537 [==============================] - 0s 46us/step - loss: 2171.2318 - regression_loss: 944.9136 - val_loss: 281.6590 - val_regression_loss: 118.2857\n","Epoch 15/50\n","537/537 [==============================] - 0s 52us/step - loss: 2421.7125 - regression_loss: 1036.9229 - val_loss: 297.1976 - val_regression_loss: 124.2992\n","Epoch 16/50\n","537/537 [==============================] - 0s 72us/step - loss: 2565.3815 - regression_loss: 1096.6862 - val_loss: 276.5854 - val_regression_loss: 114.1687\n","Epoch 17/50\n","537/537 [==============================] - 0s 63us/step - loss: 2440.3994 - regression_loss: 1031.7257 - val_loss: 246.9419 - val_regression_loss: 101.0492\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 2172.4523 - regression_loss: 913.6755 - val_loss: 232.7078 - val_regression_loss: 96.4089\n","Epoch 19/50\n","537/537 [==============================] - 0s 61us/step - loss: 1995.5184 - regression_loss: 849.0974 - val_loss: 231.3701 - val_regression_loss: 98.3059\n","Epoch 20/50\n","537/537 [==============================] - 0s 47us/step - loss: 1957.8887 - regression_loss: 851.5276 - val_loss: 232.4443 - val_regression_loss: 101.0379\n","Epoch 21/50\n","537/537 [==============================] - 0s 53us/step - loss: 1942.2856 - regression_loss: 867.8255 - val_loss: 233.9012 - val_regression_loss: 103.2578\n","Epoch 22/50\n","537/537 [==============================] - 0s 49us/step - loss: 1889.4784 - regression_loss: 851.2770 - val_loss: 235.4482 - val_regression_loss: 104.6714\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 1916.7966 - regression_loss: 872.3581 - val_loss: 232.9204 - val_regression_loss: 103.0779\n","Epoch 24/50\n","537/537 [==============================] - 0s 53us/step - loss: 1815.0398 - regression_loss: 817.6539 - val_loss: 223.6426 - val_regression_loss: 97.2972\n","Epoch 25/50\n","537/537 [==============================] - 0s 53us/step - loss: 1795.9433 - regression_loss: 803.5398 - val_loss: 212.7270 - val_regression_loss: 90.2558\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1713.4025 - regression_loss: 743.9094 - val_loss: 205.7296 - val_regression_loss: 85.3406\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 1676.5020 - regression_loss: 710.7967 - val_loss: 202.7284 - val_regression_loss: 82.9154\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 1635.8179 - regression_loss: 684.1126 - val_loss: 200.0500 - val_regression_loss: 81.2541\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 1674.3034 - regression_loss: 700.6682 - val_loss: 197.9891 - val_regression_loss: 80.4519\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1622.2931 - regression_loss: 679.6936 - val_loss: 197.5312 - val_regression_loss: 80.8620\n","Epoch 31/50\n","537/537 [==============================] - 0s 47us/step - loss: 1599.4948 - regression_loss: 671.4411 - val_loss: 197.7777 - val_regression_loss: 81.8185\n","Epoch 32/50\n","537/537 [==============================] - 0s 59us/step - loss: 1560.6375 - regression_loss: 662.0057 - val_loss: 197.8284 - val_regression_loss: 82.6014\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 1543.4718 - regression_loss: 660.6482 - val_loss: 197.3085 - val_regression_loss: 82.8474\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 1550.1398 - regression_loss: 666.5286 - val_loss: 195.7626 - val_regression_loss: 82.2662\n","Epoch 35/50\n","537/537 [==============================] - 0s 52us/step - loss: 1509.5819 - regression_loss: 650.5693 - val_loss: 192.1882 - val_regression_loss: 80.2945\n","Epoch 36/50\n","537/537 [==============================] - 0s 49us/step - loss: 1492.9789 - regression_loss: 637.0120 - val_loss: 187.9149 - val_regression_loss: 77.7989\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 1455.5564 - regression_loss: 616.4824 - val_loss: 184.5992 - val_regression_loss: 75.6714\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 1477.4981 - regression_loss: 622.6149 - val_loss: 182.2858 - val_regression_loss: 74.1736\n","Epoch 39/50\n","537/537 [==============================] - 0s 47us/step - loss: 1410.8327 - regression_loss: 586.6878 - val_loss: 179.3507 - val_regression_loss: 72.6430\n","Epoch 40/50\n","537/537 [==============================] - 0s 53us/step - loss: 1427.7196 - regression_loss: 594.0119 - val_loss: 177.8199 - val_regression_loss: 72.0386\n","Epoch 41/50\n","537/537 [==============================] - 0s 46us/step - loss: 1436.5084 - regression_loss: 598.7769 - val_loss: 177.3457 - val_regression_loss: 72.0535\n","Epoch 42/50\n","537/537 [==============================] - 0s 51us/step - loss: 1408.7372 - regression_loss: 584.9444 - val_loss: 177.0493 - val_regression_loss: 72.1535\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 1418.4626 - regression_loss: 591.4611 - val_loss: 176.8776 - val_regression_loss: 72.2730\n","Epoch 44/50\n","537/537 [==============================] - 0s 56us/step - loss: 1406.8492 - regression_loss: 588.3472 - val_loss: 175.5006 - val_regression_loss: 71.7032\n","Epoch 45/50\n","537/537 [==============================] - 0s 47us/step - loss: 1392.9804 - regression_loss: 581.9423 - val_loss: 173.0756 - val_regression_loss: 70.5759\n","Epoch 46/50\n","537/537 [==============================] - 0s 47us/step - loss: 1354.0731 - regression_loss: 564.4643 - val_loss: 170.0446 - val_regression_loss: 69.0606\n","Epoch 47/50\n","537/537 [==============================] - 0s 49us/step - loss: 1367.1630 - regression_loss: 569.6390 - val_loss: 167.7984 - val_regression_loss: 67.8714\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 1360.1794 - regression_loss: 565.4797 - val_loss: 166.6333 - val_regression_loss: 67.0977\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 1342.0298 - regression_loss: 557.3707 - val_loss: 165.6932 - val_regression_loss: 66.5343\n","Epoch 50/50\n","537/537 [==============================] - 0s 52us/step - loss: 1340.6310 - regression_loss: 552.0423 - val_loss: 164.8893 - val_regression_loss: 66.2265\n","***************************** elapsed_time is:  3.1607844829559326\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 13410.0479 - regression_loss: 6520.8643 - val_loss: 900.3756 - val_regression_loss: 427.5844\n","Epoch 2/50\n","537/537 [==============================] - 0s 50us/step - loss: 9298.0783 - regression_loss: 4483.8401 - val_loss: 525.4353 - val_regression_loss: 242.4292\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 5836.7813 - regression_loss: 2771.9198 - val_loss: 339.6544 - val_regression_loss: 152.0235\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 3374.0209 - regression_loss: 1560.5492 - val_loss: 334.5323 - val_regression_loss: 150.2919\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 2915.4484 - regression_loss: 1338.2575 - val_loss: 289.2428 - val_regression_loss: 126.0817\n","Epoch 6/50\n","537/537 [==============================] - 0s 50us/step - loss: 2189.8691 - regression_loss: 962.9724 - val_loss: 332.7594 - val_regression_loss: 145.8925\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 2446.6592 - regression_loss: 1076.4506 - val_loss: 348.7242 - val_regression_loss: 153.6202\n","Epoch 8/50\n","537/537 [==============================] - 0s 47us/step - loss: 2689.6897 - regression_loss: 1196.9904 - val_loss: 250.1291 - val_regression_loss: 105.4172\n","Epoch 9/50\n","537/537 [==============================] - 0s 47us/step - loss: 2041.0141 - regression_loss: 882.2903 - val_loss: 172.2699 - val_regression_loss: 67.9029\n","Epoch 10/50\n","537/537 [==============================] - 0s 47us/step - loss: 1555.9382 - regression_loss: 651.5810 - val_loss: 165.9634 - val_regression_loss: 65.9115\n","Epoch 11/50\n","537/537 [==============================] - 0s 57us/step - loss: 1647.9066 - regression_loss: 706.5736 - val_loss: 184.7467 - val_regression_loss: 75.9290\n","Epoch 12/50\n","537/537 [==============================] - 0s 47us/step - loss: 1825.5936 - regression_loss: 799.9129 - val_loss: 180.2942 - val_regression_loss: 73.7768\n","Epoch 13/50\n","537/537 [==============================] - 0s 52us/step - loss: 1791.7046 - regression_loss: 783.7545 - val_loss: 162.7254 - val_regression_loss: 64.6825\n","Epoch 14/50\n","537/537 [==============================] - 0s 49us/step - loss: 1632.9103 - regression_loss: 702.9330 - val_loss: 154.9902 - val_regression_loss: 60.3002\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 1529.1273 - regression_loss: 643.7955 - val_loss: 160.2369 - val_regression_loss: 62.3523\n","Epoch 16/50\n","537/537 [==============================] - 0s 48us/step - loss: 1498.8773 - regression_loss: 624.8057 - val_loss: 169.9146 - val_regression_loss: 66.6856\n","Epoch 17/50\n","537/537 [==============================] - 0s 54us/step - loss: 1515.9043 - regression_loss: 630.8838 - val_loss: 176.0942 - val_regression_loss: 69.4961\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 1520.0062 - regression_loss: 632.5617 - val_loss: 174.6485 - val_regression_loss: 68.8318\n","Epoch 19/50\n","537/537 [==============================] - 0s 50us/step - loss: 1459.3712 - regression_loss: 601.3092 - val_loss: 167.2406 - val_regression_loss: 65.4845\n","Epoch 20/50\n","537/537 [==============================] - 0s 49us/step - loss: 1450.8949 - regression_loss: 598.2108 - val_loss: 158.8879 - val_regression_loss: 61.7504\n","Epoch 21/50\n","537/537 [==============================] - 0s 47us/step - loss: 1409.1066 - regression_loss: 583.6625 - val_loss: 154.5815 - val_regression_loss: 59.9382\n","Epoch 22/50\n","537/537 [==============================] - 0s 43us/step - loss: 1394.7414 - regression_loss: 578.7819 - val_loss: 153.3609 - val_regression_loss: 59.5271\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 1406.7834 - regression_loss: 587.8187 - val_loss: 153.8330 - val_regression_loss: 59.8112\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 1395.0405 - regression_loss: 578.9817 - val_loss: 156.2555 - val_regression_loss: 60.9703\n","Epoch 25/50\n","537/537 [==============================] - 0s 45us/step - loss: 1357.2222 - regression_loss: 557.8946 - val_loss: 160.0892 - val_regression_loss: 62.7546\n","Epoch 26/50\n","537/537 [==============================] - 0s 53us/step - loss: 1324.6157 - regression_loss: 543.5376 - val_loss: 163.3062 - val_regression_loss: 64.2114\n","Epoch 27/50\n","537/537 [==============================] - 0s 48us/step - loss: 1339.5996 - regression_loss: 548.0640 - val_loss: 164.1499 - val_regression_loss: 64.5256\n","Epoch 28/50\n","537/537 [==============================] - 0s 46us/step - loss: 1318.9604 - regression_loss: 537.4275 - val_loss: 163.7833 - val_regression_loss: 64.3166\n","Epoch 29/50\n","537/537 [==============================] - 0s 51us/step - loss: 1323.6138 - regression_loss: 542.4150 - val_loss: 162.9075 - val_regression_loss: 63.9300\n","Epoch 30/50\n","537/537 [==============================] - 0s 50us/step - loss: 1321.5744 - regression_loss: 539.5800 - val_loss: 161.3084 - val_regression_loss: 63.2433\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 1318.6203 - regression_loss: 539.2000 - val_loss: 159.6752 - val_regression_loss: 62.5304\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 1300.4094 - regression_loss: 532.8766 - val_loss: 158.0277 - val_regression_loss: 61.7750\n","Epoch 33/50\n","537/537 [==============================] - 0s 56us/step - loss: 1261.3178 - regression_loss: 511.8508 - val_loss: 157.5701 - val_regression_loss: 61.5618\n","Epoch 34/50\n","537/537 [==============================] - 0s 46us/step - loss: 1286.8953 - regression_loss: 525.9569 - val_loss: 157.2684 - val_regression_loss: 61.3671\n","Epoch 35/50\n","537/537 [==============================] - 0s 56us/step - loss: 1291.9563 - regression_loss: 528.1089 - val_loss: 157.8367 - val_regression_loss: 61.5930\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 1288.6171 - regression_loss: 524.9723 - val_loss: 158.6232 - val_regression_loss: 61.9457\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 1243.3048 - regression_loss: 504.2835 - val_loss: 159.2753 - val_regression_loss: 62.2448\n","Epoch 38/50\n","537/537 [==============================] - 0s 53us/step - loss: 1267.6741 - regression_loss: 517.8222 - val_loss: 159.0146 - val_regression_loss: 62.1141\n","Epoch 39/50\n","537/537 [==============================] - 0s 50us/step - loss: 1253.8722 - regression_loss: 509.3370 - val_loss: 157.8886 - val_regression_loss: 61.5806\n","Epoch 40/50\n","537/537 [==============================] - 0s 49us/step - loss: 1264.5759 - regression_loss: 514.7836 - val_loss: 156.7542 - val_regression_loss: 61.0558\n","Epoch 41/50\n","537/537 [==============================] - 0s 48us/step - loss: 1255.8604 - regression_loss: 511.3242 - val_loss: 156.3525 - val_regression_loss: 60.8977\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 1245.7224 - regression_loss: 507.8975 - val_loss: 156.5434 - val_regression_loss: 61.0065\n","\n","Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 1257.4371 - regression_loss: 514.5532 - val_loss: 156.5691 - val_regression_loss: 61.0081\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 1252.2638 - regression_loss: 513.1229 - val_loss: 156.6069 - val_regression_loss: 61.0020\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 1216.0060 - regression_loss: 493.6998 - val_loss: 156.9470 - val_regression_loss: 61.1510\n","Epoch 46/50\n","537/537 [==============================] - 0s 56us/step - loss: 1236.8960 - regression_loss: 503.1060 - val_loss: 157.3169 - val_regression_loss: 61.3236\n","Epoch 47/50\n","537/537 [==============================] - 0s 49us/step - loss: 1263.0703 - regression_loss: 517.1614 - val_loss: 157.6267 - val_regression_loss: 61.4729\n","Epoch 48/50\n","537/537 [==============================] - 0s 46us/step - loss: 1253.3482 - regression_loss: 511.3686 - val_loss: 157.5688 - val_regression_loss: 61.4479\n","Epoch 49/50\n","537/537 [==============================] - 0s 43us/step - loss: 1245.8400 - regression_loss: 508.3892 - val_loss: 157.4449 - val_regression_loss: 61.3845\n","Epoch 50/50\n","537/537 [==============================] - 0s 51us/step - loss: 1240.0101 - regression_loss: 508.6738 - val_loss: 157.3379 - val_regression_loss: 61.3287\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","***************************** elapsed_time is:  3.1576759815216064\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 9146.0179 - regression_loss: 4390.9005 - val_loss: 766.9752 - val_regression_loss: 359.6851\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 6523.4353 - regression_loss: 3085.8011 - val_loss: 537.3442 - val_regression_loss: 246.0602\n","Epoch 3/50\n","537/537 [==============================] - 0s 52us/step - loss: 4538.2094 - regression_loss: 2103.2494 - val_loss: 396.0497 - val_regression_loss: 177.0523\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 3145.3287 - regression_loss: 1419.5996 - val_loss: 253.6273 - val_regression_loss: 106.1886\n","Epoch 5/50\n","537/537 [==============================] - 0s 44us/step - loss: 1851.9287 - regression_loss: 774.6451 - val_loss: 215.4500 - val_regression_loss: 86.3559\n","Epoch 6/50\n","537/537 [==============================] - 0s 46us/step - loss: 1594.7331 - regression_loss: 638.5669 - val_loss: 296.9309 - val_regression_loss: 126.7055\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 2386.6119 - regression_loss: 1029.7059 - val_loss: 257.9815 - val_regression_loss: 108.4178\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 1990.7909 - regression_loss: 842.0451 - val_loss: 192.1777 - val_regression_loss: 77.2283\n","Epoch 9/50\n","537/537 [==============================] - 0s 47us/step - loss: 1476.6234 - regression_loss: 600.3461 - val_loss: 174.4049 - val_regression_loss: 69.8253\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 1323.8914 - regression_loss: 535.4506 - val_loss: 183.1140 - val_regression_loss: 75.1194\n","Epoch 11/50\n","537/537 [==============================] - 0s 56us/step - loss: 1408.0399 - regression_loss: 586.1155 - val_loss: 191.7483 - val_regression_loss: 79.9123\n","Epoch 12/50\n","537/537 [==============================] - 0s 49us/step - loss: 1534.8681 - regression_loss: 654.0483 - val_loss: 190.3364 - val_regression_loss: 79.3624\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 1553.0487 - regression_loss: 664.2573 - val_loss: 178.4476 - val_regression_loss: 73.3342\n","Epoch 14/50\n","537/537 [==============================] - 0s 45us/step - loss: 1398.9607 - regression_loss: 587.7752 - val_loss: 166.8045 - val_regression_loss: 67.1510\n","Epoch 15/50\n","537/537 [==============================] - 0s 48us/step - loss: 1293.3047 - regression_loss: 527.5073 - val_loss: 164.8479 - val_regression_loss: 65.5036\n","Epoch 16/50\n","537/537 [==============================] - 0s 54us/step - loss: 1249.6546 - regression_loss: 503.9990 - val_loss: 170.3685 - val_regression_loss: 67.5254\n","Epoch 17/50\n","537/537 [==============================] - 0s 52us/step - loss: 1286.0853 - regression_loss: 515.4061 - val_loss: 174.1828 - val_regression_loss: 69.0328\n","Epoch 18/50\n","537/537 [==============================] - 0s 56us/step - loss: 1331.3866 - regression_loss: 531.7973 - val_loss: 172.4002 - val_regression_loss: 68.2661\n","Epoch 19/50\n","537/537 [==============================] - 0s 49us/step - loss: 1294.8312 - regression_loss: 517.4372 - val_loss: 168.3302 - val_regression_loss: 66.6629\n","Epoch 20/50\n","537/537 [==============================] - 0s 53us/step - loss: 1240.2082 - regression_loss: 497.7677 - val_loss: 165.6386 - val_regression_loss: 65.8099\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 1224.2053 - regression_loss: 494.4395 - val_loss: 165.8069 - val_regression_loss: 66.2623\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 1231.8509 - regression_loss: 500.9489 - val_loss: 166.8097 - val_regression_loss: 66.9496\n","Epoch 23/50\n","537/537 [==============================] - 0s 48us/step - loss: 1235.5453 - regression_loss: 506.7995 - val_loss: 166.3559 - val_regression_loss: 66.7065\n","Epoch 24/50\n","537/537 [==============================] - 0s 60us/step - loss: 1229.9922 - regression_loss: 500.4857 - val_loss: 165.4130 - val_regression_loss: 66.1038\n","Epoch 25/50\n","537/537 [==============================] - 0s 59us/step - loss: 1239.1847 - regression_loss: 506.5682 - val_loss: 165.2402 - val_regression_loss: 65.8121\n","Epoch 26/50\n","537/537 [==============================] - 0s 50us/step - loss: 1188.7869 - regression_loss: 476.9373 - val_loss: 166.4107 - val_regression_loss: 66.1848\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 1219.4703 - regression_loss: 493.4400 - val_loss: 168.5143 - val_regression_loss: 67.0845\n","Epoch 28/50\n","537/537 [==============================] - 0s 70us/step - loss: 1202.7481 - regression_loss: 484.5288 - val_loss: 170.3171 - val_regression_loss: 67.9185\n","Epoch 29/50\n","537/537 [==============================] - 0s 52us/step - loss: 1211.9222 - regression_loss: 489.6661 - val_loss: 170.6403 - val_regression_loss: 68.0813\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1193.2318 - regression_loss: 482.2645 - val_loss: 170.3154 - val_regression_loss: 67.9797\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 1194.4766 - regression_loss: 482.6211 - val_loss: 169.5936 - val_regression_loss: 67.7092\n","\n","Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 32/50\n","537/537 [==============================] - 0s 67us/step - loss: 1200.8661 - regression_loss: 487.3396 - val_loss: 169.2223 - val_regression_loss: 67.5585\n","Epoch 33/50\n","537/537 [==============================] - 0s 50us/step - loss: 1186.3276 - regression_loss: 479.4385 - val_loss: 168.9220 - val_regression_loss: 67.4340\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 1202.1024 - regression_loss: 487.4396 - val_loss: 168.6882 - val_regression_loss: 67.3268\n","Epoch 35/50\n","537/537 [==============================] - 0s 50us/step - loss: 1188.0596 - regression_loss: 481.9399 - val_loss: 168.5374 - val_regression_loss: 67.2423\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 1184.2157 - regression_loss: 478.1212 - val_loss: 168.5484 - val_regression_loss: 67.2169\n","***************************** elapsed_time is:  3.046459436416626\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 461586.8389 - regression_loss: 230339.4551 - val_loss: 45501.5391 - val_regression_loss: 22700.2539\n","Epoch 2/50\n","537/537 [==============================] - 0s 60us/step - loss: 426246.2781 - regression_loss: 212699.8750 - val_loss: 41004.2656 - val_regression_loss: 20457.0410\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 390174.6496 - regression_loss: 194717.4034 - val_loss: 34850.7070 - val_regression_loss: 17387.5605\n","Epoch 4/50\n","537/537 [==============================] - 0s 48us/step - loss: 336860.2099 - regression_loss: 168119.0638 - val_loss: 26953.8008 - val_regression_loss: 13447.8721\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 263890.3075 - regression_loss: 131706.9867 - val_loss: 17967.4785 - val_regression_loss: 8964.0957\n","Epoch 6/50\n","537/537 [==============================] - 0s 45us/step - loss: 173602.9946 - regression_loss: 86642.7960 - val_loss: 9571.8438 - val_regression_loss: 4775.1631\n","Epoch 7/50\n","537/537 [==============================] - 0s 50us/step - loss: 102320.0948 - regression_loss: 51073.3983 - val_loss: 4858.8384 - val_regression_loss: 2425.9165\n","Epoch 8/50\n","537/537 [==============================] - 0s 49us/step - loss: 47962.7087 - regression_loss: 23952.2728 - val_loss: 7305.2686 - val_regression_loss: 3651.8872\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 51974.5122 - regression_loss: 25975.2468 - val_loss: 11291.6562 - val_regression_loss: 5635.9243\n","Epoch 10/50\n","537/537 [==============================] - 0s 48us/step - loss: 74624.5993 - regression_loss: 37208.3418 - val_loss: 9858.4346 - val_regression_loss: 4902.2505\n","Epoch 11/50\n","537/537 [==============================] - 0s 47us/step - loss: 65629.0759 - regression_loss: 32568.6922 - val_loss: 6033.2510 - val_regression_loss: 2982.6750\n","Epoch 12/50\n","537/537 [==============================] - 0s 47us/step - loss: 42782.4165 - regression_loss: 21082.0239 - val_loss: 3299.4099 - val_regression_loss: 1619.4679\n","Epoch 13/50\n","537/537 [==============================] - 0s 46us/step - loss: 28294.5985 - regression_loss: 13879.9944 - val_loss: 2449.2051 - val_regression_loss: 1200.0862\n","Epoch 14/50\n","537/537 [==============================] - 0s 51us/step - loss: 27502.2791 - regression_loss: 13534.2886 - val_loss: 2647.1814 - val_regression_loss: 1303.0410\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 32625.6225 - regression_loss: 16133.8191 - val_loss: 2977.5042 - val_regression_loss: 1470.3956\n","Epoch 16/50\n","537/537 [==============================] - 0s 52us/step - loss: 37005.4573 - regression_loss: 18342.1839 - val_loss: 3008.4568 - val_regression_loss: 1487.0505\n","Epoch 17/50\n","537/537 [==============================] - 0s 54us/step - loss: 36208.4143 - regression_loss: 17956.7022 - val_loss: 2738.4695 - val_regression_loss: 1352.7847\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 31995.5716 - regression_loss: 15857.4917 - val_loss: 2395.2012 - val_regression_loss: 1181.7122\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 26792.4549 - regression_loss: 13262.7139 - val_loss: 2264.7810 - val_regression_loss: 1117.0148\n","Epoch 20/50\n","537/537 [==============================] - 0s 51us/step - loss: 22497.8179 - regression_loss: 11120.5791 - val_loss: 2501.5100 - val_regression_loss: 1235.8727\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 20769.1202 - regression_loss: 10261.7777 - val_loss: 2985.5415 - val_regression_loss: 1478.3136\n","Epoch 22/50\n","537/537 [==============================] - 0s 49us/step - loss: 21877.0703 - regression_loss: 10818.4645 - val_loss: 3363.6165 - val_regression_loss: 1667.6379\n","Epoch 23/50\n","537/537 [==============================] - 0s 47us/step - loss: 22567.6052 - regression_loss: 11166.8594 - val_loss: 3362.0603 - val_regression_loss: 1666.9196\n","Epoch 24/50\n","537/537 [==============================] - 0s 47us/step - loss: 22838.9605 - regression_loss: 11303.5280 - val_loss: 3037.6289 - val_regression_loss: 1504.5245\n","Epoch 25/50\n","537/537 [==============================] - 0s 51us/step - loss: 20680.9476 - regression_loss: 10223.3064 - val_loss: 2629.8389 - val_regression_loss: 1300.2623\n","Epoch 26/50\n","537/537 [==============================] - 0s 44us/step - loss: 18784.4122 - regression_loss: 9270.8599 - val_loss: 2326.5474 - val_regression_loss: 1148.1646\n","Epoch 27/50\n","537/537 [==============================] - 0s 50us/step - loss: 18516.3711 - regression_loss: 9132.6042 - val_loss: 2149.7424 - val_regression_loss: 1059.3389\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 17978.8582 - regression_loss: 8859.8712 - val_loss: 2034.1486 - val_regression_loss: 1001.2064\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 18564.2279 - regression_loss: 9152.2625 - val_loss: 1929.4174 - val_regression_loss: 948.6436\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 18132.8072 - regression_loss: 8933.5403 - val_loss: 1850.7069 - val_regression_loss: 909.2310\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 17494.2284 - regression_loss: 8615.4439 - val_loss: 1829.8765 - val_regression_loss: 898.8777\n","Epoch 32/50\n","537/537 [==============================] - 0s 48us/step - loss: 15967.3311 - regression_loss: 7851.9760 - val_loss: 1863.0094 - val_regression_loss: 915.5833\n","Epoch 33/50\n","537/537 [==============================] - 0s 50us/step - loss: 16497.2569 - regression_loss: 8119.4080 - val_loss: 1912.8932 - val_regression_loss: 940.7083\n","Epoch 34/50\n","537/537 [==============================] - 0s 51us/step - loss: 16285.5250 - regression_loss: 8014.6026 - val_loss: 1924.9513 - val_regression_loss: 946.9137\n","Epoch 35/50\n","537/537 [==============================] - 0s 58us/step - loss: 15869.8826 - regression_loss: 7808.5809 - val_loss: 1873.0621 - val_regression_loss: 921.0784\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 15294.7659 - regression_loss: 7523.1864 - val_loss: 1791.9065 - val_regression_loss: 880.5447\n","Epoch 37/50\n","537/537 [==============================] - 0s 49us/step - loss: 14324.5312 - regression_loss: 7037.8621 - val_loss: 1699.4159 - val_regression_loss: 834.2692\n","Epoch 38/50\n","537/537 [==============================] - 0s 48us/step - loss: 14488.5210 - regression_loss: 7118.3058 - val_loss: 1626.4208 - val_regression_loss: 797.7232\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 14075.7901 - regression_loss: 6910.5648 - val_loss: 1563.5481 - val_regression_loss: 766.2408\n","Epoch 40/50\n","537/537 [==============================] - 0s 52us/step - loss: 13835.7569 - regression_loss: 6790.5394 - val_loss: 1506.2750 - val_regression_loss: 737.5658\n","Epoch 41/50\n","537/537 [==============================] - 0s 57us/step - loss: 13295.1794 - regression_loss: 6521.2887 - val_loss: 1454.1095 - val_regression_loss: 711.4454\n","Epoch 42/50\n","537/537 [==============================] - 0s 47us/step - loss: 12877.6083 - regression_loss: 6311.0806 - val_loss: 1411.9550 - val_regression_loss: 690.3468\n","Epoch 43/50\n","537/537 [==============================] - 0s 48us/step - loss: 12293.8152 - regression_loss: 6019.3143 - val_loss: 1366.7307 - val_regression_loss: 667.7134\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 11883.8805 - regression_loss: 5814.3183 - val_loss: 1304.9020 - val_regression_loss: 636.7778\n","Epoch 45/50\n","537/537 [==============================] - 0s 48us/step - loss: 11572.8856 - regression_loss: 5660.0453 - val_loss: 1220.7963 - val_regression_loss: 594.7010\n","Epoch 46/50\n","537/537 [==============================] - 0s 48us/step - loss: 11386.1232 - regression_loss: 5565.9790 - val_loss: 1139.1824 - val_regression_loss: 553.8846\n","Epoch 47/50\n","537/537 [==============================] - 0s 51us/step - loss: 10688.7101 - regression_loss: 5216.8739 - val_loss: 1066.9708 - val_regression_loss: 517.7803\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 10521.1776 - regression_loss: 5132.2893 - val_loss: 1005.9538 - val_regression_loss: 487.2868\n","Epoch 49/50\n","537/537 [==============================] - 0s 53us/step - loss: 10187.5287 - regression_loss: 4969.4130 - val_loss: 953.7783 - val_regression_loss: 461.2286\n","Epoch 50/50\n","537/537 [==============================] - 0s 50us/step - loss: 9718.6390 - regression_loss: 4732.2471 - val_loss: 908.3043 - val_regression_loss: 438.5189\n","***************************** elapsed_time is:  3.1116209030151367\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 35983.8371 - regression_loss: 17784.4426 - val_loss: 3488.9617 - val_regression_loss: 1720.0375\n","Epoch 2/50\n","537/537 [==============================] - 0s 49us/step - loss: 27409.2355 - regression_loss: 13524.4751 - val_loss: 2356.5242 - val_regression_loss: 1157.1215\n","Epoch 3/50\n","537/537 [==============================] - 0s 50us/step - loss: 18409.4667 - regression_loss: 9050.6203 - val_loss: 1312.9147 - val_regression_loss: 638.6985\n","Epoch 4/50\n","537/537 [==============================] - 0s 48us/step - loss: 10195.9492 - regression_loss: 4970.3248 - val_loss: 972.9165 - val_regression_loss: 471.9232\n","Epoch 5/50\n","537/537 [==============================] - 0s 53us/step - loss: 7958.1962 - regression_loss: 3876.4291 - val_loss: 932.9008 - val_regression_loss: 452.7585\n","Epoch 6/50\n","537/537 [==============================] - 0s 44us/step - loss: 7707.8341 - regression_loss: 3757.5614 - val_loss: 498.1674 - val_regression_loss: 234.0470\n","Epoch 7/50\n","537/537 [==============================] - 0s 49us/step - loss: 4293.4345 - regression_loss: 2036.5565 - val_loss: 233.5905 - val_regression_loss: 99.5307\n","Epoch 8/50\n","537/537 [==============================] - 0s 48us/step - loss: 2065.2995 - regression_loss: 903.0070 - val_loss: 342.8935 - val_regression_loss: 152.1602\n","Epoch 9/50\n","537/537 [==============================] - 0s 48us/step - loss: 2887.8486 - regression_loss: 1300.0998 - val_loss: 511.6171 - val_regression_loss: 235.5512\n","Epoch 10/50\n","537/537 [==============================] - 0s 53us/step - loss: 4166.5403 - regression_loss: 1932.6096 - val_loss: 500.2553 - val_regression_loss: 229.9643\n","Epoch 11/50\n","537/537 [==============================] - 0s 53us/step - loss: 3981.4121 - regression_loss: 1841.5141 - val_loss: 369.2150 - val_regression_loss: 165.1373\n","Epoch 12/50\n","537/537 [==============================] - 0s 49us/step - loss: 2923.5071 - regression_loss: 1322.2438 - val_loss: 241.7853 - val_regression_loss: 102.3372\n","Epoch 13/50\n","537/537 [==============================] - 0s 54us/step - loss: 1968.8380 - regression_loss: 853.1882 - val_loss: 191.8085 - val_regression_loss: 78.2622\n","Epoch 14/50\n","537/537 [==============================] - 0s 54us/step - loss: 1728.3859 - regression_loss: 737.7728 - val_loss: 206.4942 - val_regression_loss: 86.3534\n","Epoch 15/50\n","537/537 [==============================] - 0s 51us/step - loss: 1923.6281 - regression_loss: 845.3550 - val_loss: 222.4386 - val_regression_loss: 94.7619\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 2081.3359 - regression_loss: 926.0192 - val_loss: 216.6301 - val_regression_loss: 91.9365\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 1978.4134 - regression_loss: 877.4453 - val_loss: 209.2192 - val_regression_loss: 88.0130\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 1869.7020 - regression_loss: 819.8973 - val_loss: 206.9314 - val_regression_loss: 86.4864\n","Epoch 19/50\n","537/537 [==============================] - 0s 49us/step - loss: 1744.3659 - regression_loss: 754.9262 - val_loss: 200.2196 - val_regression_loss: 82.6982\n","\n","Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 20/50\n","537/537 [==============================] - 0s 57us/step - loss: 1652.0808 - regression_loss: 704.7350 - val_loss: 193.3969 - val_regression_loss: 79.0728\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 1552.7735 - regression_loss: 653.8624 - val_loss: 186.2254 - val_regression_loss: 75.2768\n","Epoch 22/50\n","537/537 [==============================] - 0s 57us/step - loss: 1545.1499 - regression_loss: 649.6124 - val_loss: 182.7484 - val_regression_loss: 73.3473\n","Epoch 23/50\n","537/537 [==============================] - 0s 51us/step - loss: 1534.3008 - regression_loss: 642.5037 - val_loss: 183.1150 - val_regression_loss: 73.3703\n","Epoch 24/50\n","537/537 [==============================] - 0s 46us/step - loss: 1541.9616 - regression_loss: 648.0632 - val_loss: 184.5044 - val_regression_loss: 73.9558\n","Epoch 25/50\n","537/537 [==============================] - 0s 45us/step - loss: 1564.1313 - regression_loss: 656.2981 - val_loss: 184.5617 - val_regression_loss: 73.9344\n","Epoch 26/50\n","537/537 [==============================] - 0s 48us/step - loss: 1531.6254 - regression_loss: 640.8799 - val_loss: 183.2308 - val_regression_loss: 73.2805\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 1538.5819 - regression_loss: 643.5032 - val_loss: 181.9722 - val_regression_loss: 72.7057\n","Epoch 28/50\n","537/537 [==============================] - 0s 48us/step - loss: 1531.6938 - regression_loss: 641.2061 - val_loss: 181.8805 - val_regression_loss: 72.7390\n","Epoch 29/50\n","537/537 [==============================] - 0s 45us/step - loss: 1508.3128 - regression_loss: 631.1056 - val_loss: 182.1366 - val_regression_loss: 72.9522\n","Epoch 30/50\n","537/537 [==============================] - 0s 47us/step - loss: 1501.9149 - regression_loss: 629.1392 - val_loss: 181.3905 - val_regression_loss: 72.6643\n","Epoch 31/50\n","537/537 [==============================] - 0s 63us/step - loss: 1464.3372 - regression_loss: 611.9249 - val_loss: 179.4059 - val_regression_loss: 71.7488\n","Epoch 32/50\n","537/537 [==============================] - 0s 50us/step - loss: 1473.2908 - regression_loss: 615.7492 - val_loss: 177.1598 - val_regression_loss: 70.6818\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 1481.5847 - regression_loss: 620.5784 - val_loss: 175.1096 - val_regression_loss: 69.6898\n","Epoch 34/50\n","537/537 [==============================] - 0s 49us/step - loss: 1473.7276 - regression_loss: 620.5123 - val_loss: 173.6140 - val_regression_loss: 68.9492\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 1480.9342 - regression_loss: 619.8508 - val_loss: 172.6647 - val_regression_loss: 68.4562\n","Epoch 36/50\n","537/537 [==============================] - 0s 45us/step - loss: 1453.1732 - regression_loss: 606.9532 - val_loss: 172.2027 - val_regression_loss: 68.1857\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 1462.7345 - regression_loss: 610.0558 - val_loss: 172.2257 - val_regression_loss: 68.1482\n","Epoch 38/50\n","537/537 [==============================] - 0s 44us/step - loss: 1457.6039 - regression_loss: 607.8532 - val_loss: 172.7003 - val_regression_loss: 68.3396\n","Epoch 39/50\n","537/537 [==============================] - 0s 50us/step - loss: 1458.6702 - regression_loss: 610.3823 - val_loss: 173.2742 - val_regression_loss: 68.5844\n","Epoch 40/50\n","537/537 [==============================] - 0s 48us/step - loss: 1440.2963 - regression_loss: 598.5606 - val_loss: 173.4257 - val_regression_loss: 68.6287\n","Epoch 41/50\n","537/537 [==============================] - 0s 48us/step - loss: 1421.7793 - regression_loss: 591.6838 - val_loss: 172.7069 - val_regression_loss: 68.2534\n","Epoch 42/50\n","537/537 [==============================] - 0s 52us/step - loss: 1413.7644 - regression_loss: 586.6705 - val_loss: 171.4251 - val_regression_loss: 67.6126\n","Epoch 43/50\n","537/537 [==============================] - 0s 52us/step - loss: 1417.4623 - regression_loss: 587.4250 - val_loss: 170.1312 - val_regression_loss: 66.9827\n","Epoch 44/50\n","537/537 [==============================] - 0s 53us/step - loss: 1439.6154 - regression_loss: 596.7811 - val_loss: 169.0274 - val_regression_loss: 66.4516\n","Epoch 45/50\n","537/537 [==============================] - 0s 46us/step - loss: 1435.5594 - regression_loss: 596.4614 - val_loss: 168.1664 - val_regression_loss: 66.0448\n","Epoch 46/50\n","537/537 [==============================] - 0s 50us/step - loss: 1404.1713 - regression_loss: 583.5017 - val_loss: 167.5977 - val_regression_loss: 65.7826\n","Epoch 47/50\n","537/537 [==============================] - 0s 50us/step - loss: 1369.1651 - regression_loss: 562.8092 - val_loss: 167.4686 - val_regression_loss: 65.7365\n","Epoch 48/50\n","537/537 [==============================] - 0s 55us/step - loss: 1401.5282 - regression_loss: 580.1001 - val_loss: 167.3468 - val_regression_loss: 65.6864\n","Epoch 49/50\n","537/537 [==============================] - 0s 57us/step - loss: 1398.1065 - regression_loss: 578.3155 - val_loss: 167.1802 - val_regression_loss: 65.6124\n","Epoch 50/50\n","537/537 [==============================] - 0s 58us/step - loss: 1391.6089 - regression_loss: 577.5906 - val_loss: 166.7510 - val_regression_loss: 65.4003\n","***************************** elapsed_time is:  3.181396245956421\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 16703.7310 - regression_loss: 8165.5984 - val_loss: 1327.7512 - val_regression_loss: 642.0801\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 10536.2493 - regression_loss: 5112.0785 - val_loss: 746.6022 - val_regression_loss: 355.0010\n","Epoch 3/50\n","537/537 [==============================] - 0s 48us/step - loss: 5886.9137 - regression_loss: 2814.2778 - val_loss: 459.1672 - val_regression_loss: 214.2650\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 3522.0949 - regression_loss: 1653.3071 - val_loss: 382.5445 - val_regression_loss: 176.0843\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 3046.0942 - regression_loss: 1412.5609 - val_loss: 298.6825 - val_regression_loss: 131.6580\n","Epoch 6/50\n","537/537 [==============================] - 0s 45us/step - loss: 2579.5041 - regression_loss: 1153.3756 - val_loss: 275.1767 - val_regression_loss: 117.9235\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 2376.2487 - regression_loss: 1038.4049 - val_loss: 310.2756 - val_regression_loss: 135.1445\n","Epoch 8/50\n","537/537 [==============================] - 0s 45us/step - loss: 2464.9937 - regression_loss: 1084.0450 - val_loss: 290.2094 - val_regression_loss: 125.9037\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 2300.3852 - regression_loss: 1006.6113 - val_loss: 228.0565 - val_regression_loss: 95.9997\n","Epoch 10/50\n","537/537 [==============================] - 0s 48us/step - loss: 1786.6036 - regression_loss: 761.5197 - val_loss: 182.5447 - val_regression_loss: 74.3884\n","Epoch 11/50\n","537/537 [==============================] - 0s 55us/step - loss: 1551.2103 - regression_loss: 652.8468 - val_loss: 175.5010 - val_regression_loss: 71.7304\n","Epoch 12/50\n","537/537 [==============================] - 0s 48us/step - loss: 1630.1616 - regression_loss: 700.7125 - val_loss: 188.6420 - val_regression_loss: 78.7061\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 1764.8415 - regression_loss: 774.2138 - val_loss: 186.7407 - val_regression_loss: 77.6837\n","Epoch 14/50\n","537/537 [==============================] - 0s 67us/step - loss: 1746.5700 - regression_loss: 762.3057 - val_loss: 169.0379 - val_regression_loss: 68.4120\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 1570.2984 - regression_loss: 669.0114 - val_loss: 158.3902 - val_regression_loss: 62.5155\n","Epoch 16/50\n","537/537 [==============================] - 0s 52us/step - loss: 1408.1416 - regression_loss: 583.1398 - val_loss: 163.3585 - val_regression_loss: 64.4587\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 1430.7913 - regression_loss: 591.8175 - val_loss: 173.2688 - val_regression_loss: 69.0236\n","Epoch 18/50\n","537/537 [==============================] - 0s 48us/step - loss: 1470.1306 - regression_loss: 608.9298 - val_loss: 175.6146 - val_regression_loss: 70.0378\n","Epoch 19/50\n","537/537 [==============================] - 0s 54us/step - loss: 1464.1514 - regression_loss: 603.4381 - val_loss: 169.5136 - val_regression_loss: 67.0327\n","Epoch 20/50\n","537/537 [==============================] - 0s 52us/step - loss: 1453.7266 - regression_loss: 599.7990 - val_loss: 159.6205 - val_regression_loss: 62.3072\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 1424.3591 - regression_loss: 589.2898 - val_loss: 152.5534 - val_regression_loss: 59.0802\n","\n","Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 1377.9795 - regression_loss: 569.5156 - val_loss: 150.6345 - val_regression_loss: 58.2723\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 1371.9158 - regression_loss: 567.0808 - val_loss: 149.3374 - val_regression_loss: 57.7581\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 1377.0062 - regression_loss: 569.5289 - val_loss: 148.3662 - val_regression_loss: 57.3726\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 1358.5359 - regression_loss: 561.1954 - val_loss: 147.7452 - val_regression_loss: 57.1191\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1402.7461 - regression_loss: 583.1990 - val_loss: 147.4451 - val_regression_loss: 56.9766\n","Epoch 27/50\n","537/537 [==============================] - 0s 44us/step - loss: 1357.9363 - regression_loss: 561.7439 - val_loss: 147.2792 - val_regression_loss: 56.8665\n","Epoch 28/50\n","537/537 [==============================] - 0s 46us/step - loss: 1360.4416 - regression_loss: 561.9661 - val_loss: 147.2256 - val_regression_loss: 56.7844\n","Epoch 29/50\n","537/537 [==============================] - 0s 52us/step - loss: 1365.8231 - regression_loss: 564.0952 - val_loss: 147.0983 - val_regression_loss: 56.6526\n","Epoch 30/50\n","537/537 [==============================] - 0s 47us/step - loss: 1338.7588 - regression_loss: 551.3647 - val_loss: 146.9067 - val_regression_loss: 56.4857\n","Epoch 31/50\n","537/537 [==============================] - 0s 50us/step - loss: 1340.8629 - regression_loss: 553.9652 - val_loss: 146.6033 - val_regression_loss: 56.2772\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 1331.1131 - regression_loss: 544.1988 - val_loss: 146.1176 - val_regression_loss: 55.9990\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 1323.1628 - regression_loss: 543.6073 - val_loss: 145.4475 - val_regression_loss: 55.6470\n","Epoch 34/50\n","537/537 [==============================] - 0s 51us/step - loss: 1337.9033 - regression_loss: 548.1650 - val_loss: 144.3679 - val_regression_loss: 55.1167\n","Epoch 35/50\n","537/537 [==============================] - 0s 57us/step - loss: 1316.8086 - regression_loss: 537.6297 - val_loss: 143.1728 - val_regression_loss: 54.5453\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 1309.1337 - regression_loss: 536.4733 - val_loss: 141.9045 - val_regression_loss: 53.9483\n","Epoch 37/50\n","537/537 [==============================] - 0s 56us/step - loss: 1333.2785 - regression_loss: 548.2633 - val_loss: 140.8012 - val_regression_loss: 53.4402\n","Epoch 38/50\n","537/537 [==============================] - 0s 49us/step - loss: 1326.2975 - regression_loss: 545.9755 - val_loss: 139.9009 - val_regression_loss: 53.0293\n","Epoch 39/50\n","537/537 [==============================] - 0s 48us/step - loss: 1330.9108 - regression_loss: 548.7424 - val_loss: 139.0890 - val_regression_loss: 52.6578\n","Epoch 40/50\n","537/537 [==============================] - 0s 45us/step - loss: 1295.2163 - regression_loss: 529.5730 - val_loss: 138.4443 - val_regression_loss: 52.3554\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 1311.4189 - regression_loss: 537.7599 - val_loss: 137.8715 - val_regression_loss: 52.0775\n","Epoch 42/50\n","537/537 [==============================] - 0s 46us/step - loss: 1311.1704 - regression_loss: 537.1405 - val_loss: 137.3756 - val_regression_loss: 51.8278\n","Epoch 43/50\n","537/537 [==============================] - 0s 44us/step - loss: 1317.5861 - regression_loss: 538.9891 - val_loss: 136.9185 - val_regression_loss: 51.5927\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 1296.2322 - regression_loss: 530.9155 - val_loss: 136.3929 - val_regression_loss: 51.3186\n","Epoch 45/50\n","537/537 [==============================] - 0s 48us/step - loss: 1293.0450 - regression_loss: 527.9649 - val_loss: 135.8682 - val_regression_loss: 51.0502\n","Epoch 46/50\n","537/537 [==============================] - 0s 53us/step - loss: 1275.9019 - regression_loss: 521.9873 - val_loss: 135.2124 - val_regression_loss: 50.7231\n","Epoch 47/50\n","537/537 [==============================] - 0s 49us/step - loss: 1262.8203 - regression_loss: 513.4186 - val_loss: 134.5462 - val_regression_loss: 50.4009\n","Epoch 48/50\n","537/537 [==============================] - 0s 56us/step - loss: 1288.8548 - regression_loss: 526.1608 - val_loss: 133.8424 - val_regression_loss: 50.0609\n","Epoch 49/50\n","537/537 [==============================] - 0s 56us/step - loss: 1238.8221 - regression_loss: 501.0455 - val_loss: 133.1264 - val_regression_loss: 49.7195\n","Epoch 50/50\n","537/537 [==============================] - 0s 48us/step - loss: 1273.4868 - regression_loss: 521.6290 - val_loss: 132.4424 - val_regression_loss: 49.3975\n","***************************** elapsed_time is:  3.1297590732574463\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 20409.6046 - regression_loss: 9967.6582 - val_loss: 2049.1516 - val_regression_loss: 994.7975\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 15058.0373 - regression_loss: 7327.9365 - val_loss: 1347.9528 - val_regression_loss: 650.0172\n","Epoch 3/50\n","537/537 [==============================] - 0s 45us/step - loss: 9778.2844 - regression_loss: 4728.2925 - val_loss: 760.3413 - val_regression_loss: 363.6482\n","Epoch 4/50\n","537/537 [==============================] - 0s 44us/step - loss: 6266.6648 - regression_loss: 3033.7955 - val_loss: 558.5809 - val_regression_loss: 268.5767\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 5121.9066 - regression_loss: 2502.6089 - val_loss: 378.2655 - val_regression_loss: 176.4576\n","Epoch 6/50\n","537/537 [==============================] - 0s 45us/step - loss: 3647.4380 - regression_loss: 1745.8134 - val_loss: 243.6315 - val_regression_loss: 103.0825\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 2312.6654 - regression_loss: 1025.5529 - val_loss: 313.4882 - val_regression_loss: 131.9705\n","Epoch 8/50\n","537/537 [==============================] - 0s 48us/step - loss: 2539.2844 - regression_loss: 1087.4690 - val_loss: 434.4252 - val_regression_loss: 189.6213\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 3099.0640 - regression_loss: 1346.3859 - val_loss: 431.7126 - val_regression_loss: 189.0120\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 2911.3442 - regression_loss: 1258.1773 - val_loss: 333.5840 - val_regression_loss: 142.7455\n","Epoch 11/50\n","537/537 [==============================] - 0s 58us/step - loss: 2328.8569 - regression_loss: 990.8102 - val_loss: 230.1644 - val_regression_loss: 94.5965\n","Epoch 12/50\n","537/537 [==============================] - 0s 53us/step - loss: 1753.2578 - regression_loss: 732.3692 - val_loss: 180.5053 - val_regression_loss: 73.1141\n","Epoch 13/50\n","537/537 [==============================] - 0s 50us/step - loss: 1620.2659 - regression_loss: 694.0362 - val_loss: 180.5405 - val_regression_loss: 75.4893\n","Epoch 14/50\n","537/537 [==============================] - 0s 49us/step - loss: 1861.7336 - regression_loss: 834.6966 - val_loss: 185.8859 - val_regression_loss: 79.0842\n","Epoch 15/50\n","537/537 [==============================] - 0s 50us/step - loss: 1976.2279 - regression_loss: 900.5077 - val_loss: 176.7830 - val_regression_loss: 74.1149\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 1840.1312 - regression_loss: 828.5795 - val_loss: 171.9182 - val_regression_loss: 70.4536\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 1580.7187 - regression_loss: 686.4234 - val_loss: 182.1480 - val_regression_loss: 74.0521\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 1536.4717 - regression_loss: 651.1312 - val_loss: 195.8979 - val_regression_loss: 79.4831\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 1539.5747 - regression_loss: 639.5935 - val_loss: 199.9410 - val_regression_loss: 80.3396\n","Epoch 20/50\n","537/537 [==============================] - 0s 56us/step - loss: 1583.3275 - regression_loss: 649.1758 - val_loss: 193.0066 - val_regression_loss: 76.2567\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 1523.2253 - regression_loss: 612.5789 - val_loss: 180.5971 - val_regression_loss: 70.2406\n","Epoch 22/50\n","537/537 [==============================] - 0s 51us/step - loss: 1457.1652 - regression_loss: 587.6642 - val_loss: 168.4377 - val_regression_loss: 65.0305\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 1458.3183 - regression_loss: 588.6347 - val_loss: 160.1289 - val_regression_loss: 62.1048\n","Epoch 24/50\n","537/537 [==============================] - 0s 50us/step - loss: 1401.3520 - regression_loss: 572.6532 - val_loss: 158.2498 - val_regression_loss: 62.1483\n","Epoch 25/50\n","537/537 [==============================] - 0s 51us/step - loss: 1397.9674 - regression_loss: 582.2822 - val_loss: 161.1995 - val_regression_loss: 64.1255\n","Epoch 26/50\n","537/537 [==============================] - 0s 47us/step - loss: 1404.9739 - regression_loss: 590.6107 - val_loss: 163.1830 - val_regression_loss: 65.2303\n","Epoch 27/50\n","537/537 [==============================] - 0s 57us/step - loss: 1381.7640 - regression_loss: 581.7950 - val_loss: 160.7282 - val_regression_loss: 63.8226\n","Epoch 28/50\n","537/537 [==============================] - 0s 52us/step - loss: 1381.2477 - regression_loss: 578.6554 - val_loss: 156.4210 - val_regression_loss: 61.3234\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 1351.2101 - regression_loss: 561.3055 - val_loss: 154.5677 - val_regression_loss: 59.9215\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 1345.1502 - regression_loss: 551.0166 - val_loss: 155.7535 - val_regression_loss: 60.0793\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 1316.6576 - regression_loss: 533.6977 - val_loss: 158.2717 - val_regression_loss: 61.0825\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 1331.1590 - regression_loss: 537.3420 - val_loss: 161.0312 - val_regression_loss: 62.4396\n","Epoch 33/50\n","537/537 [==============================] - 0s 60us/step - loss: 1314.5560 - regression_loss: 529.4077 - val_loss: 160.6270 - val_regression_loss: 62.4707\n","Epoch 34/50\n","537/537 [==============================] - 0s 58us/step - loss: 1288.8162 - regression_loss: 517.2351 - val_loss: 156.8404 - val_regression_loss: 60.9614\n","Epoch 35/50\n","537/537 [==============================] - 0s 53us/step - loss: 1312.4260 - regression_loss: 535.4875 - val_loss: 154.1140 - val_regression_loss: 59.9305\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 1290.2962 - regression_loss: 526.5558 - val_loss: 152.9379 - val_regression_loss: 59.5611\n","Epoch 37/50\n","537/537 [==============================] - 0s 48us/step - loss: 1290.2975 - regression_loss: 528.0147 - val_loss: 153.9037 - val_regression_loss: 60.0707\n","Epoch 38/50\n","537/537 [==============================] - 0s 50us/step - loss: 1287.7419 - regression_loss: 527.9808 - val_loss: 155.0649 - val_regression_loss: 60.5252\n","Epoch 39/50\n","537/537 [==============================] - 0s 50us/step - loss: 1270.1371 - regression_loss: 516.9559 - val_loss: 154.6115 - val_regression_loss: 60.1651\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 1268.7797 - regression_loss: 515.2823 - val_loss: 152.3950 - val_regression_loss: 59.0004\n","Epoch 41/50\n","537/537 [==============================] - 0s 52us/step - loss: 1237.7343 - regression_loss: 500.6917 - val_loss: 150.5641 - val_regression_loss: 58.0593\n","Epoch 42/50\n","537/537 [==============================] - 0s 51us/step - loss: 1254.3275 - regression_loss: 508.6032 - val_loss: 150.6716 - val_regression_loss: 58.0881\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 1251.1249 - regression_loss: 506.6939 - val_loss: 150.7194 - val_regression_loss: 58.1683\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 1261.1157 - regression_loss: 510.8147 - val_loss: 151.3672 - val_regression_loss: 58.5905\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 1268.3764 - regression_loss: 518.3699 - val_loss: 149.9223 - val_regression_loss: 58.0119\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 1264.6655 - regression_loss: 516.0657 - val_loss: 147.7719 - val_regression_loss: 57.0703\n","\n","Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 47/50\n","537/537 [==============================] - 0s 56us/step - loss: 1266.3957 - regression_loss: 519.1744 - val_loss: 147.6165 - val_regression_loss: 56.9794\n","Epoch 48/50\n","537/537 [==============================] - 0s 52us/step - loss: 1272.7804 - regression_loss: 520.2739 - val_loss: 147.9057 - val_regression_loss: 57.0653\n","Epoch 49/50\n","537/537 [==============================] - 0s 67us/step - loss: 1254.3355 - regression_loss: 512.8577 - val_loss: 148.1703 - val_regression_loss: 57.1254\n","Epoch 50/50\n","537/537 [==============================] - 0s 55us/step - loss: 1255.9511 - regression_loss: 511.6929 - val_loss: 147.9437 - val_regression_loss: 56.9575\n","***************************** elapsed_time is:  3.4663314819335938\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 11907.3884 - regression_loss: 5815.7405 - val_loss: 1103.2190 - val_regression_loss: 527.3777\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 8439.6891 - regression_loss: 4054.7715 - val_loss: 752.4985 - val_regression_loss: 347.2031\n","Epoch 3/50\n","537/537 [==============================] - 0s 45us/step - loss: 5122.7294 - regression_loss: 2357.9836 - val_loss: 577.1509 - val_regression_loss: 253.8689\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 3480.6791 - regression_loss: 1494.4510 - val_loss: 440.9420 - val_regression_loss: 187.6479\n","Epoch 5/50\n","537/537 [==============================] - 0s 46us/step - loss: 2605.9166 - regression_loss: 1072.7243 - val_loss: 295.9137 - val_regression_loss: 122.9145\n","Epoch 6/50\n","537/537 [==============================] - 0s 45us/step - loss: 2167.7268 - regression_loss: 917.3667 - val_loss: 271.6485 - val_regression_loss: 117.7454\n","Epoch 7/50\n","537/537 [==============================] - 0s 43us/step - loss: 2423.3302 - regression_loss: 1099.5281 - val_loss: 265.8892 - val_regression_loss: 116.7695\n","Epoch 8/50\n","537/537 [==============================] - 0s 48us/step - loss: 2295.5896 - regression_loss: 1048.9542 - val_loss: 222.6602 - val_regression_loss: 93.8020\n","Epoch 9/50\n","537/537 [==============================] - 0s 54us/step - loss: 1771.6576 - regression_loss: 775.2601 - val_loss: 203.5764 - val_regression_loss: 82.0091\n","Epoch 10/50\n","537/537 [==============================] - 0s 48us/step - loss: 1462.0275 - regression_loss: 601.0394 - val_loss: 227.5271 - val_regression_loss: 92.0956\n","Epoch 11/50\n","537/537 [==============================] - 0s 49us/step - loss: 1497.0478 - regression_loss: 604.7769 - val_loss: 256.5175 - val_regression_loss: 105.7642\n","Epoch 12/50\n","537/537 [==============================] - 0s 49us/step - loss: 1694.2283 - regression_loss: 696.8428 - val_loss: 245.1627 - val_regression_loss: 100.5336\n","Epoch 13/50\n","537/537 [==============================] - 0s 52us/step - loss: 1654.5782 - regression_loss: 679.7624 - val_loss: 210.8592 - val_regression_loss: 84.6901\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 1439.5444 - regression_loss: 583.4573 - val_loss: 187.0659 - val_regression_loss: 74.4107\n","Epoch 15/50\n","537/537 [==============================] - 0s 72us/step - loss: 1338.6930 - regression_loss: 546.4575 - val_loss: 178.9239 - val_regression_loss: 71.8214\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 1391.4171 - regression_loss: 585.6348 - val_loss: 176.3206 - val_regression_loss: 71.5320\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 1424.0740 - regression_loss: 610.7345 - val_loss: 173.4429 - val_regression_loss: 70.3606\n","Epoch 18/50\n","537/537 [==============================] - 0s 54us/step - loss: 1390.5738 - regression_loss: 597.3774 - val_loss: 171.6058 - val_regression_loss: 68.8993\n","Epoch 19/50\n","537/537 [==============================] - 0s 56us/step - loss: 1370.5210 - regression_loss: 583.0248 - val_loss: 174.0194 - val_regression_loss: 69.0016\n","Epoch 20/50\n","537/537 [==============================] - 0s 54us/step - loss: 1311.8087 - regression_loss: 546.6730 - val_loss: 179.9639 - val_regression_loss: 70.7955\n","Epoch 21/50\n","537/537 [==============================] - 0s 54us/step - loss: 1302.0869 - regression_loss: 528.9820 - val_loss: 185.4613 - val_regression_loss: 72.7601\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 1286.3396 - regression_loss: 516.6156 - val_loss: 186.3613 - val_regression_loss: 73.0076\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 1282.0804 - regression_loss: 510.0081 - val_loss: 181.7510 - val_regression_loss: 71.0278\n","Epoch 24/50\n","537/537 [==============================] - 0s 52us/step - loss: 1289.4166 - regression_loss: 518.5959 - val_loss: 174.9346 - val_regression_loss: 68.2276\n","Epoch 25/50\n","537/537 [==============================] - 0s 53us/step - loss: 1254.8140 - regression_loss: 505.3127 - val_loss: 169.0646 - val_regression_loss: 65.9717\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 1246.3831 - regression_loss: 510.3526 - val_loss: 165.9987 - val_regression_loss: 64.8544\n","Epoch 27/50\n","537/537 [==============================] - 0s 55us/step - loss: 1280.3717 - regression_loss: 528.7388 - val_loss: 164.5562 - val_regression_loss: 64.2054\n","Epoch 28/50\n","537/537 [==============================] - 0s 50us/step - loss: 1247.6435 - regression_loss: 515.4625 - val_loss: 164.9375 - val_regression_loss: 64.1635\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 1256.3945 - regression_loss: 515.5667 - val_loss: 166.8214 - val_regression_loss: 64.7312\n","Epoch 30/50\n","537/537 [==============================] - 0s 50us/step - loss: 1221.7106 - regression_loss: 496.7811 - val_loss: 169.5468 - val_regression_loss: 65.7259\n","Epoch 31/50\n","537/537 [==============================] - 0s 50us/step - loss: 1223.3366 - regression_loss: 491.0808 - val_loss: 171.3872 - val_regression_loss: 66.4300\n","Epoch 32/50\n","537/537 [==============================] - 0s 60us/step - loss: 1240.7521 - regression_loss: 499.9621 - val_loss: 172.0919 - val_regression_loss: 66.7073\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 1216.7218 - regression_loss: 489.1366 - val_loss: 170.9643 - val_regression_loss: 66.2668\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 1192.0061 - regression_loss: 478.4969 - val_loss: 169.0678 - val_regression_loss: 65.5583\n","Epoch 35/50\n","537/537 [==============================] - 0s 52us/step - loss: 1226.7076 - regression_loss: 495.7889 - val_loss: 167.4083 - val_regression_loss: 64.9445\n","Epoch 36/50\n","537/537 [==============================] - 0s 60us/step - loss: 1199.8071 - regression_loss: 484.5800 - val_loss: 166.4488 - val_regression_loss: 64.5664\n","Epoch 37/50\n","537/537 [==============================] - 0s 51us/step - loss: 1225.7012 - regression_loss: 499.8652 - val_loss: 166.3576 - val_regression_loss: 64.4566\n","Epoch 38/50\n","537/537 [==============================] - 0s 54us/step - loss: 1208.1839 - regression_loss: 488.2793 - val_loss: 166.8444 - val_regression_loss: 64.6157\n","Epoch 39/50\n","537/537 [==============================] - 0s 50us/step - loss: 1212.8275 - regression_loss: 490.4034 - val_loss: 167.4596 - val_regression_loss: 64.8216\n","\n","Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 40/50\n","537/537 [==============================] - 0s 55us/step - loss: 1192.7673 - regression_loss: 481.0938 - val_loss: 167.5890 - val_regression_loss: 64.8635\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 1199.9395 - regression_loss: 485.5507 - val_loss: 167.7699 - val_regression_loss: 64.9368\n","Epoch 42/50\n","537/537 [==============================] - 0s 53us/step - loss: 1204.7822 - regression_loss: 487.7418 - val_loss: 167.9017 - val_regression_loss: 65.0031\n","Epoch 43/50\n","537/537 [==============================] - 0s 57us/step - loss: 1205.9536 - regression_loss: 489.0369 - val_loss: 167.7400 - val_regression_loss: 64.9360\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 1201.9304 - regression_loss: 487.0208 - val_loss: 167.2799 - val_regression_loss: 64.7370\n","\n","Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","***************************** elapsed_time is:  3.0287766456604004\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 2111758.4257 - regression_loss: 1055766.1212 - val_loss: 268758.2812 - val_regression_loss: 134368.1406\n","Epoch 2/50\n","537/537 [==============================] - 0s 51us/step - loss: 2073661.3456 - regression_loss: 1036740.5822 - val_loss: 256383.6875 - val_regression_loss: 128184.0938\n","Epoch 3/50\n","537/537 [==============================] - 0s 45us/step - loss: 1984943.8381 - regression_loss: 992400.9968 - val_loss: 239031.2031 - val_regression_loss: 119510.4062\n","Epoch 4/50\n","537/537 [==============================] - 0s 44us/step - loss: 1867652.9522 - regression_loss: 933775.1797 - val_loss: 216165.2188 - val_regression_loss: 108079.2812\n","Epoch 5/50\n","537/537 [==============================] - 0s 50us/step - loss: 1683360.1771 - regression_loss: 841636.1573 - val_loss: 188298.8750 - val_regression_loss: 94147.3359\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 1499296.6460 - regression_loss: 749609.1362 - val_loss: 156830.3906 - val_regression_loss: 78413.6562\n","Epoch 7/50\n","537/537 [==============================] - 0s 54us/step - loss: 1264473.1018 - regression_loss: 632201.4050 - val_loss: 123487.0859 - val_regression_loss: 61741.6484\n","Epoch 8/50\n","537/537 [==============================] - 0s 50us/step - loss: 1008754.2723 - regression_loss: 504333.3930 - val_loss: 90202.9375 - val_regression_loss: 45097.1562\n","Epoch 9/50\n","537/537 [==============================] - 0s 44us/step - loss: 768685.7838 - regression_loss: 384270.7018 - val_loss: 60357.1328 - val_regression_loss: 30166.8613\n","Epoch 10/50\n","537/537 [==============================] - 0s 53us/step - loss: 562875.4952 - regression_loss: 281301.1107 - val_loss: 39897.9570 - val_regression_loss: 19918.0332\n","Epoch 11/50\n","537/537 [==============================] - 0s 53us/step - loss: 431913.5290 - regression_loss: 215648.6055 - val_loss: 36525.8359 - val_regression_loss: 18198.6172\n","Epoch 12/50\n","537/537 [==============================] - 0s 50us/step - loss: 439921.9580 - regression_loss: 219397.5364 - val_loss: 43841.1406 - val_regression_loss: 21844.9766\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 512251.5258 - regression_loss: 255483.3005 - val_loss: 43481.5859 - val_regression_loss: 21681.9395\n","Epoch 14/50\n","537/537 [==============================] - 0s 50us/step - loss: 492513.5568 - regression_loss: 245762.5278 - val_loss: 36101.5547 - val_regression_loss: 18012.0684\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 424612.5369 - regression_loss: 211991.0144 - val_loss: 30815.9453 - val_regression_loss: 15382.9863\n","Epoch 16/50\n","537/537 [==============================] - 0s 46us/step - loss: 364097.4899 - regression_loss: 181843.5259 - val_loss: 30838.2246 - val_regression_loss: 15401.5781\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 361953.8635 - regression_loss: 180835.9969 - val_loss: 32977.0898 - val_regression_loss: 16474.0215\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 339838.8624 - regression_loss: 169797.5296 - val_loss: 34490.2773 - val_regression_loss: 17231.0742\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 374055.1087 - regression_loss: 186911.8783 - val_loss: 33902.3984 - val_regression_loss: 16936.1152\n","Epoch 20/50\n","537/537 [==============================] - 0s 51us/step - loss: 354990.1288 - regression_loss: 177364.9394 - val_loss: 31477.5996 - val_regression_loss: 15722.0566\n","Epoch 21/50\n","537/537 [==============================] - 0s 46us/step - loss: 346666.6947 - regression_loss: 173189.7414 - val_loss: 28120.1055 - val_regression_loss: 14041.6162\n","Epoch 22/50\n","537/537 [==============================] - 0s 49us/step - loss: 320169.2626 - regression_loss: 159925.5572 - val_loss: 25146.6602 - val_regression_loss: 12553.7666\n","Epoch 23/50\n","537/537 [==============================] - 0s 57us/step - loss: 301667.3461 - regression_loss: 150668.3525 - val_loss: 23528.5586 - val_regression_loss: 11744.5420\n","Epoch 24/50\n","537/537 [==============================] - 0s 57us/step - loss: 280515.9973 - regression_loss: 140086.4223 - val_loss: 23231.6328 - val_regression_loss: 11596.9336\n","Epoch 25/50\n","537/537 [==============================] - 0s 49us/step - loss: 286818.9520 - regression_loss: 143249.5096 - val_loss: 23174.9922 - val_regression_loss: 11569.9814\n","Epoch 26/50\n","537/537 [==============================] - 0s 49us/step - loss: 284475.6399 - regression_loss: 142094.5007 - val_loss: 22408.9023 - val_regression_loss: 11188.0479\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 260416.4078 - regression_loss: 130075.0237 - val_loss: 21086.2949 - val_regression_loss: 10527.3223\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 258685.6350 - regression_loss: 129217.9138 - val_loss: 19945.3145 - val_regression_loss: 9956.9023\n","Epoch 29/50\n","537/537 [==============================] - 0s 48us/step - loss: 243726.1387 - regression_loss: 121732.9705 - val_loss: 19334.2539 - val_regression_loss: 9651.2646\n","Epoch 30/50\n","537/537 [==============================] - 0s 53us/step - loss: 231403.0644 - regression_loss: 115571.6261 - val_loss: 19042.3105 - val_regression_loss: 9505.3447\n","Epoch 31/50\n","537/537 [==============================] - 0s 45us/step - loss: 229672.0916 - regression_loss: 114710.1258 - val_loss: 18688.4062 - val_regression_loss: 9328.8574\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 193307.8257 - regression_loss: 96531.2359 - val_loss: 18186.1133 - val_regression_loss: 9078.8066\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 191952.9498 - regression_loss: 95861.3907 - val_loss: 17771.3926 - val_regression_loss: 8873.3672\n","Epoch 34/50\n","537/537 [==============================] - 0s 51us/step - loss: 194239.7608 - regression_loss: 97020.7366 - val_loss: 17693.4824 - val_regression_loss: 8836.6963\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 185710.8962 - regression_loss: 92775.0858 - val_loss: 17536.2285 - val_regression_loss: 8759.7080\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 176464.4118 - regression_loss: 88163.9952 - val_loss: 16678.4102 - val_regression_loss: 8330.7285\n","Epoch 37/50\n","537/537 [==============================] - 0s 47us/step - loss: 147884.1882 - regression_loss: 73853.5388 - val_loss: 15056.2021 - val_regression_loss: 7518.0293\n","Epoch 38/50\n","537/537 [==============================] - 0s 48us/step - loss: 155979.3554 - regression_loss: 77906.7294 - val_loss: 13588.4180 - val_regression_loss: 6782.3262\n","Epoch 39/50\n","537/537 [==============================] - 0s 51us/step - loss: 147213.7562 - regression_loss: 73496.6419 - val_loss: 12309.4785 - val_regression_loss: 6142.2285\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 128548.2974 - regression_loss: 64163.1829 - val_loss: 11199.6025 - val_regression_loss: 5588.1772\n","Epoch 41/50\n","537/537 [==============================] - 0s 67us/step - loss: 125265.7024 - regression_loss: 62519.9289 - val_loss: 10419.9375 - val_regression_loss: 5200.8569\n","Epoch 42/50\n","537/537 [==============================] - 0s 53us/step - loss: 115320.2751 - regression_loss: 57555.4588 - val_loss: 9912.3564 - val_regression_loss: 4949.9238\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 106415.4087 - regression_loss: 53122.5203 - val_loss: 9220.9727 - val_regression_loss: 4605.7773\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 98608.5036 - regression_loss: 49248.9193 - val_loss: 8100.5088 - val_regression_loss: 4045.3091\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 92380.4396 - regression_loss: 46116.6631 - val_loss: 6879.3374 - val_regression_loss: 3433.5867\n","Epoch 46/50\n","537/537 [==============================] - 0s 47us/step - loss: 84920.4444 - regression_loss: 42402.6715 - val_loss: 6053.0645 - val_regression_loss: 3018.4900\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 77749.1669 - regression_loss: 38798.5212 - val_loss: 5681.1143 - val_regression_loss: 2830.0928\n","Epoch 48/50\n","537/537 [==============================] - 0s 49us/step - loss: 73142.4883 - regression_loss: 36477.8164 - val_loss: 5690.0757 - val_regression_loss: 2831.6682\n","Epoch 49/50\n","537/537 [==============================] - 0s 52us/step - loss: 67131.0462 - regression_loss: 33447.2662 - val_loss: 5545.7725 - val_regression_loss: 2757.1267\n","Epoch 50/50\n","537/537 [==============================] - 0s 49us/step - loss: 63037.8265 - regression_loss: 31387.0271 - val_loss: 5019.4561 - val_regression_loss: 2492.4517\n","***************************** elapsed_time is:  3.0863938331604004\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 530067.9008 - regression_loss: 266218.5600 - val_loss: 58272.9688 - val_regression_loss: 29263.0625\n","Epoch 2/50\n","537/537 [==============================] - 0s 51us/step - loss: 522511.1551 - regression_loss: 262373.4695 - val_loss: 53352.8125 - val_regression_loss: 26785.3203\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 470413.9979 - regression_loss: 236165.6378 - val_loss: 46613.3086 - val_regression_loss: 23394.3828\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 411372.2552 - regression_loss: 206446.8444 - val_loss: 37773.0508 - val_regression_loss: 18948.4336\n","Epoch 5/50\n","537/537 [==============================] - 0s 48us/step - loss: 338328.6796 - regression_loss: 169687.3022 - val_loss: 27260.2305 - val_regression_loss: 13660.8877\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 252965.2694 - regression_loss: 126783.5095 - val_loss: 16653.7812 - val_regression_loss: 8322.4414\n","Epoch 7/50\n","537/537 [==============================] - 0s 42us/step - loss: 153499.5490 - regression_loss: 76733.9731 - val_loss: 8690.2227 - val_regression_loss: 4302.1992\n","Epoch 8/50\n","537/537 [==============================] - 0s 53us/step - loss: 92940.7724 - regression_loss: 46154.8651 - val_loss: 7342.3979 - val_regression_loss: 3589.1550\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 78446.0749 - regression_loss: 38566.8342 - val_loss: 11560.3838 - val_regression_loss: 5678.9644\n","Epoch 10/50\n","537/537 [==============================] - 0s 52us/step - loss: 111352.4540 - regression_loss: 54840.4093 - val_loss: 11606.7139 - val_regression_loss: 5722.0171\n","Epoch 11/50\n","537/537 [==============================] - 0s 50us/step - loss: 110954.5202 - regression_loss: 54778.1903 - val_loss: 8110.5205 - val_regression_loss: 4010.0908\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 78445.1973 - regression_loss: 38795.5238 - val_loss: 5379.9707 - val_regression_loss: 2678.1279\n","Epoch 13/50\n","537/537 [==============================] - 0s 46us/step - loss: 59867.6736 - regression_loss: 29793.4102 - val_loss: 4843.6499 - val_regression_loss: 2432.2991\n","Epoch 14/50\n","537/537 [==============================] - 0s 48us/step - loss: 56713.5048 - regression_loss: 28365.5582 - val_loss: 5551.7075 - val_regression_loss: 2797.8528\n","Epoch 15/50\n","537/537 [==============================] - 0s 53us/step - loss: 64220.1639 - regression_loss: 32219.3104 - val_loss: 6119.7393 - val_regression_loss: 3083.1558\n","Epoch 16/50\n","537/537 [==============================] - 0s 57us/step - loss: 69480.3693 - regression_loss: 34861.3644 - val_loss: 5882.0312 - val_regression_loss: 2957.8193\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 63562.0408 - regression_loss: 31847.7050 - val_loss: 5034.8418 - val_regression_loss: 2523.0576\n","Epoch 18/50\n","537/537 [==============================] - 0s 50us/step - loss: 59600.8238 - regression_loss: 29772.4079 - val_loss: 4098.9761 - val_regression_loss: 2041.7256\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 52043.2544 - regression_loss: 25880.2543 - val_loss: 3614.2839 - val_regression_loss: 1787.0677\n","Epoch 20/50\n","537/537 [==============================] - 0s 45us/step - loss: 48132.0160 - regression_loss: 23845.2156 - val_loss: 3680.2817 - val_regression_loss: 1811.4658\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 47402.4879 - regression_loss: 23401.1685 - val_loss: 3893.4653 - val_regression_loss: 1915.3879\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 49961.9712 - regression_loss: 24675.7340 - val_loss: 3816.4724 - val_regression_loss: 1879.8765\n","Epoch 23/50\n","537/537 [==============================] - 0s 47us/step - loss: 48533.9612 - regression_loss: 23979.7042 - val_loss: 3439.6250 - val_regression_loss: 1698.6454\n","Epoch 24/50\n","537/537 [==============================] - 0s 51us/step - loss: 40920.2827 - regression_loss: 20229.3031 - val_loss: 3086.3525 - val_regression_loss: 1530.8286\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 39616.8998 - regression_loss: 19661.7698 - val_loss: 2954.1565 - val_regression_loss: 1472.9187\n","Epoch 26/50\n","537/537 [==============================] - 0s 49us/step - loss: 39616.0309 - regression_loss: 19728.6254 - val_loss: 2952.1985 - val_regression_loss: 1478.4854\n","Epoch 27/50\n","537/537 [==============================] - 0s 47us/step - loss: 38304.6187 - regression_loss: 19133.1061 - val_loss: 2920.0945 - val_regression_loss: 1466.5688\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 35851.8075 - regression_loss: 17918.1782 - val_loss: 2772.0842 - val_regression_loss: 1394.3645\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 36258.0538 - regression_loss: 18156.6470 - val_loss: 2552.5828 - val_regression_loss: 1283.6379\n","Epoch 30/50\n","537/537 [==============================] - 0s 53us/step - loss: 33518.9379 - regression_loss: 16765.7035 - val_loss: 2389.4514 - val_regression_loss: 1198.9373\n","Epoch 31/50\n","537/537 [==============================] - 0s 49us/step - loss: 32013.6444 - regression_loss: 15988.1825 - val_loss: 2312.1526 - val_regression_loss: 1155.4187\n","Epoch 32/50\n","537/537 [==============================] - 0s 47us/step - loss: 28723.3973 - regression_loss: 14299.5505 - val_loss: 2249.3555 - val_regression_loss: 1118.8248\n","Epoch 33/50\n","537/537 [==============================] - 0s 45us/step - loss: 28600.0420 - regression_loss: 14185.1715 - val_loss: 2132.1025 - val_regression_loss: 1056.3479\n","Epoch 34/50\n","537/537 [==============================] - 0s 50us/step - loss: 27967.9817 - regression_loss: 13850.2759 - val_loss: 1990.5646 - val_regression_loss: 983.8250\n","Epoch 35/50\n","537/537 [==============================] - 0s 47us/step - loss: 25881.5425 - regression_loss: 12795.8443 - val_loss: 1876.3590 - val_regression_loss: 926.9125\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 24161.9620 - regression_loss: 11954.1187 - val_loss: 1798.1216 - val_regression_loss: 888.3522\n","Epoch 37/50\n","537/537 [==============================] - 0s 51us/step - loss: 23359.5719 - regression_loss: 11557.6242 - val_loss: 1717.9497 - val_regression_loss: 848.0119\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 19906.9857 - regression_loss: 9835.2533 - val_loss: 1631.1768 - val_regression_loss: 803.0676\n","Epoch 39/50\n","537/537 [==============================] - 0s 51us/step - loss: 19655.0823 - regression_loss: 9706.9258 - val_loss: 1570.1317 - val_regression_loss: 769.7067\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 16806.6816 - regression_loss: 8261.4713 - val_loss: 1536.9443 - val_regression_loss: 750.7389\n","Epoch 41/50\n","537/537 [==============================] - 0s 53us/step - loss: 17066.9164 - regression_loss: 8381.0970 - val_loss: 1505.2646 - val_regression_loss: 733.7056\n","Epoch 42/50\n","537/537 [==============================] - 0s 44us/step - loss: 16181.6661 - regression_loss: 7938.1734 - val_loss: 1444.4998 - val_regression_loss: 703.8152\n","Epoch 43/50\n","537/537 [==============================] - 0s 53us/step - loss: 15751.4662 - regression_loss: 7735.2754 - val_loss: 1375.1488 - val_regression_loss: 670.5441\n","Epoch 44/50\n","537/537 [==============================] - 0s 53us/step - loss: 14927.5373 - regression_loss: 7337.9059 - val_loss: 1318.4534 - val_regression_loss: 643.4793\n","Epoch 45/50\n","537/537 [==============================] - 0s 49us/step - loss: 12928.2117 - regression_loss: 6348.7256 - val_loss: 1275.8763 - val_regression_loss: 622.3929\n","Epoch 46/50\n","537/537 [==============================] - 0s 47us/step - loss: 13055.2281 - regression_loss: 6421.7703 - val_loss: 1262.5284 - val_regression_loss: 614.0986\n","Epoch 47/50\n","537/537 [==============================] - 0s 44us/step - loss: 12472.5903 - regression_loss: 6122.2132 - val_loss: 1266.8961 - val_regression_loss: 614.5161\n","Epoch 48/50\n","537/537 [==============================] - 0s 49us/step - loss: 10542.9490 - regression_loss: 5140.1475 - val_loss: 1232.1414 - val_regression_loss: 597.0435\n","Epoch 49/50\n","537/537 [==============================] - 0s 52us/step - loss: 9747.0895 - regression_loss: 4744.4813 - val_loss: 1180.6364 - val_regression_loss: 572.1163\n","Epoch 50/50\n","537/537 [==============================] - 0s 47us/step - loss: 10445.4419 - regression_loss: 5108.2637 - val_loss: 1148.6998 - val_regression_loss: 556.3727\n","***************************** elapsed_time is:  3.3095149993896484\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 259829.9126 - regression_loss: 129500.6336 - val_loss: 27421.1621 - val_regression_loss: 13664.2559\n","Epoch 2/50\n","537/537 [==============================] - 0s 51us/step - loss: 235661.5809 - regression_loss: 117459.2057 - val_loss: 23744.1914 - val_regression_loss: 11832.3721\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 205737.3112 - regression_loss: 102554.5063 - val_loss: 18832.9980 - val_regression_loss: 9384.8652\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 159453.8878 - regression_loss: 79480.5446 - val_loss: 12804.9795 - val_regression_loss: 6380.2178\n","Epoch 5/50\n","537/537 [==============================] - 0s 45us/step - loss: 112298.2287 - regression_loss: 55981.2850 - val_loss: 6911.6421 - val_regression_loss: 3443.0796\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 61642.1186 - regression_loss: 30732.8923 - val_loss: 3515.8860 - val_regression_loss: 1753.5938\n","Epoch 7/50\n","537/537 [==============================] - 0s 47us/step - loss: 31220.7563 - regression_loss: 15587.9704 - val_loss: 4635.5532 - val_regression_loss: 2318.0161\n","Epoch 8/50\n","537/537 [==============================] - 0s 50us/step - loss: 38807.4529 - regression_loss: 19417.7332 - val_loss: 5661.6714 - val_regression_loss: 2826.6084\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 45510.6629 - regression_loss: 22727.5182 - val_loss: 4168.6543 - val_regression_loss: 2068.8252\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 34201.3184 - regression_loss: 16982.1850 - val_loss: 2309.6616 - val_regression_loss: 1129.6287\n","Epoch 11/50\n","537/537 [==============================] - 0s 51us/step - loss: 20483.9872 - regression_loss: 10044.0987 - val_loss: 1419.6174 - val_regression_loss: 681.6548\n","Epoch 12/50\n","537/537 [==============================] - 0s 49us/step - loss: 14935.8090 - regression_loss: 7247.6480 - val_loss: 1440.0306 - val_regression_loss: 693.0171\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 15930.6911 - regression_loss: 7756.2390 - val_loss: 1773.7511 - val_regression_loss: 862.0162\n","Epoch 14/50\n","537/537 [==============================] - 0s 47us/step - loss: 19646.9817 - regression_loss: 9632.7176 - val_loss: 1937.0155 - val_regression_loss: 945.6735\n","Epoch 15/50\n","537/537 [==============================] - 0s 53us/step - loss: 20633.3061 - regression_loss: 10139.6941 - val_loss: 1798.4130 - val_regression_loss: 878.1828\n","Epoch 16/50\n","537/537 [==============================] - 0s 57us/step - loss: 19926.9491 - regression_loss: 9805.3408 - val_loss: 1474.3235 - val_regression_loss: 717.8165\n","Epoch 17/50\n","537/537 [==============================] - 0s 60us/step - loss: 16776.7851 - regression_loss: 8240.9373 - val_loss: 1169.5874 - val_regression_loss: 567.0271\n","\n","Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 13857.6687 - regression_loss: 6795.6930 - val_loss: 1081.2267 - val_regression_loss: 523.5532\n","Epoch 19/50\n","537/537 [==============================] - 0s 46us/step - loss: 12555.9291 - regression_loss: 6148.5042 - val_loss: 1051.7787 - val_regression_loss: 509.4532\n","Epoch 20/50\n","537/537 [==============================] - 0s 51us/step - loss: 11578.1495 - regression_loss: 5666.3238 - val_loss: 1076.6316 - val_regression_loss: 522.4072\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 11556.3574 - regression_loss: 5657.8434 - val_loss: 1133.2621 - val_regression_loss: 551.1503\n","Epoch 22/50\n","537/537 [==============================] - 0s 54us/step - loss: 11661.8907 - regression_loss: 5715.3679 - val_loss: 1188.3035 - val_regression_loss: 578.9827\n","Epoch 23/50\n","537/537 [==============================] - 0s 47us/step - loss: 11595.6032 - regression_loss: 5682.4819 - val_loss: 1214.9479 - val_regression_loss: 592.5030\n","Epoch 24/50\n","537/537 [==============================] - 0s 45us/step - loss: 11800.5067 - regression_loss: 5791.4108 - val_loss: 1204.6470 - val_regression_loss: 587.4330\n","Epoch 25/50\n","537/537 [==============================] - 0s 50us/step - loss: 11475.5065 - regression_loss: 5626.1033 - val_loss: 1161.0393 - val_regression_loss: 565.5973\n","Epoch 26/50\n","537/537 [==============================] - 0s 50us/step - loss: 11220.8416 - regression_loss: 5497.9008 - val_loss: 1102.5243 - val_regression_loss: 536.2115\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 10952.5543 - regression_loss: 5364.3118 - val_loss: 1045.6121 - val_regression_loss: 507.5542\n","Epoch 28/50\n","537/537 [==============================] - 0s 46us/step - loss: 10295.6715 - regression_loss: 5036.5485 - val_loss: 998.2290 - val_regression_loss: 483.6106\n","Epoch 29/50\n","537/537 [==============================] - 0s 41us/step - loss: 10448.4793 - regression_loss: 5107.8386 - val_loss: 963.2881 - val_regression_loss: 465.8717\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 10228.6900 - regression_loss: 5000.2277 - val_loss: 938.5090 - val_regression_loss: 453.2354\n","Epoch 31/50\n","537/537 [==============================] - 0s 52us/step - loss: 10303.2122 - regression_loss: 5030.4890 - val_loss: 919.4459 - val_regression_loss: 443.5034\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 9863.9691 - regression_loss: 4809.1082 - val_loss: 902.7457 - val_regression_loss: 435.0070\n","Epoch 33/50\n","537/537 [==============================] - 0s 48us/step - loss: 9764.1285 - regression_loss: 4759.3879 - val_loss: 887.9078 - val_regression_loss: 427.5143\n","Epoch 34/50\n","537/537 [==============================] - 0s 60us/step - loss: 9702.5189 - regression_loss: 4727.7419 - val_loss: 875.7985 - val_regression_loss: 421.4508\n","Epoch 35/50\n","537/537 [==============================] - 0s 45us/step - loss: 9368.8248 - regression_loss: 4558.8822 - val_loss: 866.2946 - val_regression_loss: 416.7426\n","Epoch 36/50\n","537/537 [==============================] - 0s 43us/step - loss: 9340.7411 - regression_loss: 4546.8693 - val_loss: 860.3469 - val_regression_loss: 413.8579\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 8975.7196 - regression_loss: 4365.3234 - val_loss: 855.8593 - val_regression_loss: 411.7349\n","Epoch 38/50\n","537/537 [==============================] - 0s 48us/step - loss: 8831.8521 - regression_loss: 4291.5286 - val_loss: 850.3164 - val_regression_loss: 409.1036\n","Epoch 39/50\n","537/537 [==============================] - 0s 44us/step - loss: 8546.3398 - regression_loss: 4149.6139 - val_loss: 842.4167 - val_regression_loss: 405.2906\n","Epoch 40/50\n","537/537 [==============================] - 0s 53us/step - loss: 8266.6972 - regression_loss: 4012.4083 - val_loss: 831.2451 - val_regression_loss: 399.8183\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 7953.9072 - regression_loss: 3855.5685 - val_loss: 818.0268 - val_regression_loss: 393.2950\n","Epoch 42/50\n","537/537 [==============================] - 0s 54us/step - loss: 7940.9013 - regression_loss: 3847.5983 - val_loss: 803.2019 - val_regression_loss: 385.9305\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 7827.7875 - regression_loss: 3792.3197 - val_loss: 788.2790 - val_regression_loss: 378.4865\n","Epoch 44/50\n","537/537 [==============================] - 0s 52us/step - loss: 7499.4566 - regression_loss: 3627.3482 - val_loss: 772.4376 - val_regression_loss: 370.5515\n","Epoch 45/50\n","537/537 [==============================] - 0s 48us/step - loss: 7171.3353 - regression_loss: 3464.2328 - val_loss: 757.6541 - val_regression_loss: 363.1371\n","Epoch 46/50\n","537/537 [==============================] - 0s 49us/step - loss: 7096.5818 - regression_loss: 3428.3363 - val_loss: 742.8710 - val_regression_loss: 355.7059\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 7108.4709 - regression_loss: 3430.5929 - val_loss: 730.4348 - val_regression_loss: 349.4517\n","Epoch 48/50\n","537/537 [==============================] - 0s 50us/step - loss: 6727.8462 - regression_loss: 3241.5137 - val_loss: 719.2095 - val_regression_loss: 343.8083\n","Epoch 49/50\n","537/537 [==============================] - 0s 51us/step - loss: 6503.0695 - regression_loss: 3128.3192 - val_loss: 709.4864 - val_regression_loss: 338.9230\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 6381.9295 - regression_loss: 3064.7337 - val_loss: 700.9177 - val_regression_loss: 334.6232\n","***************************** elapsed_time is:  3.1664793491363525\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 582195.8647 - regression_loss: 290931.0571 - val_loss: 51651.5195 - val_regression_loss: 25804.8008\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 531965.5682 - regression_loss: 265832.2247 - val_loss: 47296.6875 - val_regression_loss: 23628.9004\n","Epoch 3/50\n","537/537 [==============================] - 0s 50us/step - loss: 508858.3781 - regression_loss: 254293.4626 - val_loss: 41326.9844 - val_regression_loss: 20645.1426\n","Epoch 4/50\n","537/537 [==============================] - 0s 48us/step - loss: 465250.0258 - regression_loss: 232500.2665 - val_loss: 33736.1406 - val_regression_loss: 16850.1621\n","Epoch 5/50\n","537/537 [==============================] - 0s 46us/step - loss: 390718.5413 - regression_loss: 195241.1433 - val_loss: 25207.9395 - val_regression_loss: 12585.7061\n","Epoch 6/50\n","537/537 [==============================] - 0s 47us/step - loss: 307580.5883 - regression_loss: 153668.8696 - val_loss: 17173.0684 - val_regression_loss: 8567.0664\n","Epoch 7/50\n","537/537 [==============================] - 0s 51us/step - loss: 228725.7782 - regression_loss: 114234.8941 - val_loss: 11822.3096 - val_regression_loss: 5889.7856\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 155716.1399 - regression_loss: 77713.0307 - val_loss: 11698.7588 - val_regression_loss: 5825.9824\n","Epoch 9/50\n","537/537 [==============================] - 0s 44us/step - loss: 147599.2949 - regression_loss: 73639.1542 - val_loss: 16193.6455 - val_regression_loss: 8072.8774\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 167082.3051 - regression_loss: 83377.5353 - val_loss: 17214.2793 - val_regression_loss: 8585.6191\n","Epoch 11/50\n","537/537 [==============================] - 0s 55us/step - loss: 171007.0331 - regression_loss: 85357.3452 - val_loss: 13942.0723 - val_regression_loss: 6952.8633\n","Epoch 12/50\n","537/537 [==============================] - 0s 42us/step - loss: 146490.2231 - regression_loss: 73121.5911 - val_loss: 10043.4609 - val_regression_loss: 5005.9644\n","Epoch 13/50\n","537/537 [==============================] - 0s 52us/step - loss: 124668.2008 - regression_loss: 62229.0170 - val_loss: 7718.8135 - val_regression_loss: 3844.7192\n","Epoch 14/50\n","537/537 [==============================] - 0s 52us/step - loss: 112279.4994 - regression_loss: 56041.7481 - val_loss: 6958.2856 - val_regression_loss: 3464.4612\n","Epoch 15/50\n","537/537 [==============================] - 0s 43us/step - loss: 115737.1430 - regression_loss: 57772.2277 - val_loss: 6864.5229 - val_regression_loss: 3416.9995\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 112209.3781 - regression_loss: 56004.3111 - val_loss: 6651.3491 - val_regression_loss: 3309.5347\n","Epoch 17/50\n","537/537 [==============================] - 0s 52us/step - loss: 111948.5291 - regression_loss: 55866.5819 - val_loss: 6094.6841 - val_regression_loss: 3030.1978\n","Epoch 18/50\n","537/537 [==============================] - 0s 54us/step - loss: 107333.8050 - regression_loss: 53549.7974 - val_loss: 5548.6890 - val_regression_loss: 2756.4707\n","Epoch 19/50\n","537/537 [==============================] - 0s 50us/step - loss: 87139.9044 - regression_loss: 43440.4824 - val_loss: 5484.1489 - val_regression_loss: 2724.0466\n","Epoch 20/50\n","537/537 [==============================] - 0s 43us/step - loss: 85208.0276 - regression_loss: 42463.9000 - val_loss: 5934.1333 - val_regression_loss: 2949.4187\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 83619.4005 - regression_loss: 41666.0991 - val_loss: 6253.6973 - val_regression_loss: 3109.7334\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 80203.5128 - regression_loss: 39958.8572 - val_loss: 5766.9731 - val_regression_loss: 2866.8640\n","Epoch 23/50\n","537/537 [==============================] - 0s 50us/step - loss: 72089.6368 - regression_loss: 35900.0155 - val_loss: 4695.3921 - val_regression_loss: 2331.7524\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 68302.1250 - regression_loss: 34019.2920 - val_loss: 3776.3740 - val_regression_loss: 1873.2657\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 62818.4011 - regression_loss: 31281.3135 - val_loss: 3302.4937 - val_regression_loss: 1637.6068\n","Epoch 26/50\n","537/537 [==============================] - 0s 47us/step - loss: 56416.0687 - regression_loss: 28090.9210 - val_loss: 3087.5239 - val_regression_loss: 1531.2786\n","Epoch 27/50\n","537/537 [==============================] - 0s 50us/step - loss: 55801.6037 - regression_loss: 27794.2486 - val_loss: 2893.4800 - val_regression_loss: 1435.0745\n","Epoch 28/50\n","537/537 [==============================] - 0s 46us/step - loss: 49587.0101 - regression_loss: 24687.4411 - val_loss: 2808.5476 - val_regression_loss: 1392.9147\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 43928.6793 - regression_loss: 21857.9562 - val_loss: 2905.9290 - val_regression_loss: 1441.2275\n","Epoch 30/50\n","537/537 [==============================] - 0s 45us/step - loss: 43068.4263 - regression_loss: 21425.0125 - val_loss: 2861.5576 - val_regression_loss: 1417.8865\n","Epoch 31/50\n","537/537 [==============================] - 0s 41us/step - loss: 41748.8998 - regression_loss: 20764.7659 - val_loss: 2423.4609 - val_regression_loss: 1197.0472\n","Epoch 32/50\n","537/537 [==============================] - 0s 59us/step - loss: 38131.4208 - regression_loss: 18955.3806 - val_loss: 1969.7614 - val_regression_loss: 967.8528\n","Epoch 33/50\n","537/537 [==============================] - 0s 44us/step - loss: 32247.9496 - regression_loss: 16012.9213 - val_loss: 1690.9597 - val_regression_loss: 825.4951\n","Epoch 34/50\n","537/537 [==============================] - 0s 49us/step - loss: 32248.7630 - regression_loss: 16011.2916 - val_loss: 1511.4066 - val_regression_loss: 731.8940\n","Epoch 35/50\n","537/537 [==============================] - 0s 55us/step - loss: 29529.9975 - regression_loss: 14646.2576 - val_loss: 1420.4951 - val_regression_loss: 682.0078\n","Epoch 36/50\n","537/537 [==============================] - 0s 47us/step - loss: 27523.7967 - regression_loss: 13626.7828 - val_loss: 1452.1595 - val_regression_loss: 693.8248\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 25410.0501 - regression_loss: 12550.7388 - val_loss: 1413.0042 - val_regression_loss: 672.1661\n","Epoch 38/50\n","537/537 [==============================] - 0s 56us/step - loss: 23464.4518 - regression_loss: 11575.2619 - val_loss: 1261.3811 - val_regression_loss: 596.4554\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 21147.0491 - regression_loss: 10423.0893 - val_loss: 1142.2411 - val_regression_loss: 538.3763\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 18370.7074 - regression_loss: 9040.6878 - val_loss: 1095.9246 - val_regression_loss: 517.0680\n","Epoch 41/50\n","537/537 [==============================] - 0s 50us/step - loss: 17717.0748 - regression_loss: 8720.1585 - val_loss: 1145.0322 - val_regression_loss: 543.1086\n","Epoch 42/50\n","537/537 [==============================] - 0s 51us/step - loss: 15056.3642 - regression_loss: 7391.2792 - val_loss: 1259.7906 - val_regression_loss: 602.1569\n","Epoch 43/50\n","537/537 [==============================] - 0s 46us/step - loss: 15257.9651 - regression_loss: 7492.3533 - val_loss: 1272.3799 - val_regression_loss: 610.7876\n","Epoch 44/50\n","537/537 [==============================] - 0s 44us/step - loss: 13880.2048 - regression_loss: 6805.5632 - val_loss: 1149.7657 - val_regression_loss: 552.0244\n","Epoch 45/50\n","537/537 [==============================] - 0s 56us/step - loss: 12423.6048 - regression_loss: 6087.2161 - val_loss: 1085.0393 - val_regression_loss: 521.4041\n","Epoch 46/50\n","537/537 [==============================] - 0s 57us/step - loss: 10947.0428 - regression_loss: 5348.8365 - val_loss: 1066.0192 - val_regression_loss: 512.9630\n","Epoch 47/50\n","537/537 [==============================] - 0s 55us/step - loss: 10422.3249 - regression_loss: 5088.2566 - val_loss: 1126.6694 - val_regression_loss: 543.7104\n","Epoch 48/50\n","537/537 [==============================] - 0s 51us/step - loss: 10066.2767 - regression_loss: 4910.5959 - val_loss: 1176.5551 - val_regression_loss: 568.9691\n","Epoch 49/50\n","537/537 [==============================] - 0s 46us/step - loss: 9458.1987 - regression_loss: 4603.8251 - val_loss: 1101.9902 - val_regression_loss: 532.2803\n","Epoch 50/50\n","537/537 [==============================] - 0s 46us/step - loss: 8956.7795 - regression_loss: 4353.5513 - val_loss: 989.4070 - val_regression_loss: 476.6738\n","***************************** elapsed_time is:  3.0528416633605957\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 1252737.6359 - regression_loss: 626120.8740 - val_loss: 84904.0703 - val_regression_loss: 42423.7578\n","Epoch 2/50\n","537/537 [==============================] - 0s 60us/step - loss: 1204557.7321 - regression_loss: 602051.2109 - val_loss: 79293.8750 - val_regression_loss: 39620.7188\n","Epoch 3/50\n","537/537 [==============================] - 0s 46us/step - loss: 1099579.0243 - regression_loss: 549585.1675 - val_loss: 71372.7578 - val_regression_loss: 35662.4375\n","Epoch 4/50\n","537/537 [==============================] - 0s 52us/step - loss: 1041260.0004 - regression_loss: 520448.8166 - val_loss: 60658.5352 - val_regression_loss: 30307.5195\n","Epoch 5/50\n","537/537 [==============================] - 0s 43us/step - loss: 917730.7722 - regression_loss: 458708.0006 - val_loss: 47443.5742 - val_regression_loss: 23701.7812\n","Epoch 6/50\n","537/537 [==============================] - 0s 54us/step - loss: 806280.3356 - regression_loss: 403003.1690 - val_loss: 33134.8828 - val_regression_loss: 16548.3125\n","Epoch 7/50\n","537/537 [==============================] - 0s 51us/step - loss: 635619.4847 - regression_loss: 317683.1470 - val_loss: 20567.1445 - val_regression_loss: 10264.1104\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 493850.4408 - regression_loss: 246799.9724 - val_loss: 14695.8662 - val_regression_loss: 7326.7783\n","Epoch 9/50\n","537/537 [==============================] - 0s 46us/step - loss: 352222.8860 - regression_loss: 175971.8910 - val_loss: 21769.9648 - val_regression_loss: 10861.4531\n","Epoch 10/50\n","537/537 [==============================] - 0s 52us/step - loss: 337983.3695 - regression_loss: 168831.9030 - val_loss: 36519.5430 - val_regression_loss: 18236.2598\n","Epoch 11/50\n","537/537 [==============================] - 0s 50us/step - loss: 382887.0913 - regression_loss: 191288.2678 - val_loss: 36900.0117 - val_regression_loss: 18429.6328\n","Epoch 12/50\n","537/537 [==============================] - 0s 45us/step - loss: 377339.1048 - regression_loss: 188541.0916 - val_loss: 26132.1934 - val_regression_loss: 13048.7334\n","Epoch 13/50\n","537/537 [==============================] - 0s 43us/step - loss: 301837.1521 - regression_loss: 150814.7036 - val_loss: 15759.5879 - val_regression_loss: 7864.1211\n","Epoch 14/50\n","537/537 [==============================] - 0s 45us/step - loss: 257786.6225 - regression_loss: 128798.1927 - val_loss: 10414.8457 - val_regression_loss: 5192.4688\n","Epoch 15/50\n","537/537 [==============================] - 0s 53us/step - loss: 255708.7401 - regression_loss: 127766.3779 - val_loss: 9271.8262 - val_regression_loss: 4621.2783\n","Epoch 16/50\n","537/537 [==============================] - 0s 46us/step - loss: 247043.8866 - regression_loss: 123434.6572 - val_loss: 9779.0264 - val_regression_loss: 4875.0273\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 264161.0291 - regression_loss: 131992.7470 - val_loss: 9743.7109 - val_regression_loss: 4857.3818\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 257446.3957 - regression_loss: 128626.9357 - val_loss: 9183.7900 - val_regression_loss: 4577.3643\n","Epoch 19/50\n","537/537 [==============================] - 0s 54us/step - loss: 234407.6120 - regression_loss: 117098.2663 - val_loss: 9563.7090 - val_regression_loss: 4767.3491\n","Epoch 20/50\n","537/537 [==============================] - 0s 54us/step - loss: 209468.1235 - regression_loss: 104617.1757 - val_loss: 11473.9551 - val_regression_loss: 5722.5537\n","Epoch 21/50\n","537/537 [==============================] - 0s 51us/step - loss: 200704.7957 - regression_loss: 100226.9638 - val_loss: 13351.9268 - val_regression_loss: 6661.4741\n","Epoch 22/50\n","537/537 [==============================] - 0s 50us/step - loss: 186113.7001 - regression_loss: 92926.9598 - val_loss: 12958.8213 - val_regression_loss: 6464.5977\n","Epoch 23/50\n","537/537 [==============================] - 0s 76us/step - loss: 190697.8448 - regression_loss: 95205.5160 - val_loss: 10441.2676 - val_regression_loss: 5205.9316\n","Epoch 24/50\n","537/537 [==============================] - 0s 51us/step - loss: 176608.2224 - regression_loss: 88167.4002 - val_loss: 7731.3955 - val_regression_loss: 3851.8762\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 163650.4492 - regression_loss: 81688.0897 - val_loss: 6058.1631 - val_regression_loss: 3016.7969\n","Epoch 26/50\n","537/537 [==============================] - 0s 53us/step - loss: 156407.7976 - regression_loss: 78089.8201 - val_loss: 5370.4243 - val_regression_loss: 2674.5925\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 139461.6298 - regression_loss: 69627.2474 - val_loss: 5180.1274 - val_regression_loss: 2580.8542\n","Epoch 28/50\n","537/537 [==============================] - 0s 54us/step - loss: 126716.4038 - regression_loss: 63263.1862 - val_loss: 5415.5884 - val_regression_loss: 2699.4021\n","Epoch 29/50\n","537/537 [==============================] - 0s 51us/step - loss: 126029.8508 - regression_loss: 62928.4138 - val_loss: 6210.6948 - val_regression_loss: 3096.8215\n","Epoch 30/50\n","537/537 [==============================] - 0s 50us/step - loss: 118501.5307 - regression_loss: 59164.1593 - val_loss: 7052.1631 - val_regression_loss: 3516.2048\n","Epoch 31/50\n","537/537 [==============================] - 0s 46us/step - loss: 108921.7845 - regression_loss: 54376.1086 - val_loss: 6875.8086 - val_regression_loss: 3425.5586\n","Epoch 32/50\n","537/537 [==============================] - 0s 48us/step - loss: 109221.1597 - regression_loss: 54521.7393 - val_loss: 5873.3926 - val_regression_loss: 2921.2915\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 99150.6531 - regression_loss: 49485.1803 - val_loss: 4839.4233 - val_regression_loss: 2400.8760\n","Epoch 34/50\n","537/537 [==============================] - 0s 48us/step - loss: 93457.0274 - regression_loss: 46630.3716 - val_loss: 4407.3286 - val_regression_loss: 2180.4604\n","Epoch 35/50\n","537/537 [==============================] - 0s 45us/step - loss: 87028.0786 - regression_loss: 43401.7778 - val_loss: 4383.8623 - val_regression_loss: 2163.3813\n","Epoch 36/50\n","537/537 [==============================] - 0s 57us/step - loss: 81714.0805 - regression_loss: 40718.4737 - val_loss: 4756.3154 - val_regression_loss: 2343.6646\n","Epoch 37/50\n","537/537 [==============================] - 0s 49us/step - loss: 79133.9005 - regression_loss: 39401.8689 - val_loss: 5212.2070 - val_regression_loss: 2566.8215\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 73482.0023 - regression_loss: 36545.4551 - val_loss: 5103.6890 - val_regression_loss: 2510.7810\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 68243.6319 - regression_loss: 33937.1995 - val_loss: 4390.6670 - val_regression_loss: 2155.8445\n","Epoch 40/50\n","537/537 [==============================] - 0s 48us/step - loss: 59226.2186 - regression_loss: 29411.1086 - val_loss: 3726.5745 - val_regression_loss: 1827.4884\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 62122.2144 - regression_loss: 30882.7691 - val_loss: 3537.5686 - val_regression_loss: 1736.2410\n","Epoch 42/50\n","537/537 [==============================] - 0s 44us/step - loss: 57183.8546 - regression_loss: 28419.9687 - val_loss: 3747.5686 - val_regression_loss: 1843.6648\n","Epoch 43/50\n","537/537 [==============================] - 0s 47us/step - loss: 54576.2145 - regression_loss: 27125.8290 - val_loss: 3996.5505 - val_regression_loss: 1970.3671\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 50752.7436 - regression_loss: 25216.0415 - val_loss: 3823.8555 - val_regression_loss: 1886.3201\n","Epoch 45/50\n","537/537 [==============================] - 0s 51us/step - loss: 48681.3247 - regression_loss: 24191.2173 - val_loss: 3344.2043 - val_regression_loss: 1648.3948\n","Epoch 46/50\n","537/537 [==============================] - 0s 50us/step - loss: 42217.3753 - regression_loss: 20966.8433 - val_loss: 2980.7676 - val_regression_loss: 1467.7688\n","Epoch 47/50\n","537/537 [==============================] - 0s 65us/step - loss: 40534.7160 - regression_loss: 20132.1598 - val_loss: 3057.9246 - val_regression_loss: 1506.3882\n","Epoch 48/50\n","537/537 [==============================] - 0s 63us/step - loss: 38604.0300 - regression_loss: 19164.5965 - val_loss: 3255.7781 - val_regression_loss: 1605.1646\n","Epoch 49/50\n","537/537 [==============================] - 0s 50us/step - loss: 36888.6717 - regression_loss: 18305.1547 - val_loss: 3139.7341 - val_regression_loss: 1547.5461\n","Epoch 50/50\n","537/537 [==============================] - 0s 50us/step - loss: 33440.8388 - regression_loss: 16582.1957 - val_loss: 2677.0149 - val_regression_loss: 1317.1394\n","***************************** elapsed_time is:  3.4217007160186768\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 1883142.3744 - regression_loss: 940989.1264 - val_loss: 211483.2188 - val_regression_loss: 105676.0547\n","Epoch 2/50\n","537/537 [==============================] - 0s 49us/step - loss: 1817569.6142 - regression_loss: 908273.0449 - val_loss: 202725.5469 - val_regression_loss: 101305.2969\n","Epoch 3/50\n","537/537 [==============================] - 0s 48us/step - loss: 1764000.7942 - regression_loss: 881553.9540 - val_loss: 190272.3594 - val_regression_loss: 95087.2969\n","Epoch 4/50\n","537/537 [==============================] - 0s 47us/step - loss: 1640647.0388 - regression_loss: 819943.5505 - val_loss: 172904.9062 - val_regression_loss: 86412.3516\n","Epoch 5/50\n","537/537 [==============================] - 0s 45us/step - loss: 1503940.1978 - regression_loss: 751663.1247 - val_loss: 150202.3906 - val_regression_loss: 75069.7031\n","Epoch 6/50\n","537/537 [==============================] - 0s 45us/step - loss: 1322825.2171 - regression_loss: 661174.0521 - val_loss: 122607.9922 - val_regression_loss: 61280.4336\n","Epoch 7/50\n","537/537 [==============================] - 0s 48us/step - loss: 1088238.5541 - regression_loss: 543944.7505 - val_loss: 91295.2031 - val_regression_loss: 45630.7461\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 832798.3072 - regression_loss: 416276.0402 - val_loss: 58868.8594 - val_regression_loss: 29422.4512\n","Epoch 9/50\n","537/537 [==============================] - 0s 55us/step - loss: 576535.2964 - regression_loss: 288184.1982 - val_loss: 31339.7598 - val_regression_loss: 15659.7178\n","Epoch 10/50\n","537/537 [==============================] - 0s 53us/step - loss: 341834.7135 - regression_loss: 170845.1346 - val_loss: 20339.3555 - val_regression_loss: 10151.6670\n","Epoch 11/50\n","537/537 [==============================] - 0s 51us/step - loss: 238524.4373 - regression_loss: 119112.2412 - val_loss: 35335.2930 - val_regression_loss: 17620.4766\n","Epoch 12/50\n","537/537 [==============================] - 0s 55us/step - loss: 341752.7372 - regression_loss: 170446.1834 - val_loss: 45836.7969 - val_regression_loss: 22862.0586\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 416360.6349 - regression_loss: 207649.7439 - val_loss: 35949.3086 - val_regression_loss: 17935.2520\n","Epoch 14/50\n","537/537 [==============================] - 0s 43us/step - loss: 336367.7698 - regression_loss: 167841.3987 - val_loss: 23359.1758 - val_regression_loss: 11654.3076\n","Epoch 15/50\n","537/537 [==============================] - 0s 56us/step - loss: 244593.9063 - regression_loss: 122090.3442 - val_loss: 17755.7148 - val_regression_loss: 8857.7666\n","Epoch 16/50\n","537/537 [==============================] - 0s 43us/step - loss: 207790.2787 - regression_loss: 103736.1390 - val_loss: 17886.8398 - val_regression_loss: 8924.5352\n","Epoch 17/50\n","537/537 [==============================] - 0s 58us/step - loss: 211538.4226 - regression_loss: 105620.7321 - val_loss: 19596.2715 - val_regression_loss: 9779.5518\n","Epoch 18/50\n","537/537 [==============================] - 0s 44us/step - loss: 229920.4902 - regression_loss: 114817.4743 - val_loss: 20188.6582 - val_regression_loss: 10075.8916\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 239744.5544 - regression_loss: 119735.6264 - val_loss: 19093.9004 - val_regression_loss: 9528.5947\n","Epoch 20/50\n","537/537 [==============================] - 0s 53us/step - loss: 228317.0091 - regression_loss: 114018.2633 - val_loss: 16967.9922 - val_regression_loss: 8465.6855\n","Epoch 21/50\n","537/537 [==============================] - 0s 47us/step - loss: 207875.8937 - regression_loss: 103801.2683 - val_loss: 15036.9170 - val_regression_loss: 7500.1509\n","\n","Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 22/50\n","537/537 [==============================] - 0s 58us/step - loss: 187463.8237 - regression_loss: 93597.8008 - val_loss: 14517.3818 - val_regression_loss: 7240.3872\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 177549.2392 - regression_loss: 88641.7259 - val_loss: 14407.8789 - val_regression_loss: 7185.6504\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 176099.7937 - regression_loss: 87915.3437 - val_loss: 14632.5283 - val_regression_loss: 7298.0098\n","Epoch 25/50\n","537/537 [==============================] - 0s 48us/step - loss: 171784.9515 - regression_loss: 85758.0572 - val_loss: 15002.4297 - val_regression_loss: 7483.0083\n","Epoch 26/50\n","537/537 [==============================] - 0s 46us/step - loss: 168836.5140 - regression_loss: 84284.1883 - val_loss: 15270.6182 - val_regression_loss: 7617.1504\n","Epoch 27/50\n","537/537 [==============================] - 0s 59us/step - loss: 167500.1240 - regression_loss: 83616.7356 - val_loss: 15241.3018 - val_regression_loss: 7602.5391\n","Epoch 28/50\n","537/537 [==============================] - 0s 51us/step - loss: 169537.9225 - regression_loss: 84636.6800 - val_loss: 14910.9082 - val_regression_loss: 7437.3828\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 162028.8859 - regression_loss: 80882.2573 - val_loss: 14360.4033 - val_regression_loss: 7162.1592\n","Epoch 30/50\n","537/537 [==============================] - 0s 46us/step - loss: 148518.1617 - regression_loss: 74126.2835 - val_loss: 13701.3809 - val_regression_loss: 6832.6768\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 153178.9489 - regression_loss: 76458.4179 - val_loss: 13130.7900 - val_regression_loss: 6547.3965\n","Epoch 32/50\n","537/537 [==============================] - 0s 52us/step - loss: 153897.9497 - regression_loss: 76817.5131 - val_loss: 12673.3896 - val_regression_loss: 6318.6963\n","Epoch 33/50\n","537/537 [==============================] - 0s 48us/step - loss: 147935.4236 - regression_loss: 73836.1640 - val_loss: 12306.4570 - val_regression_loss: 6135.2119\n","Epoch 34/50\n","537/537 [==============================] - 0s 44us/step - loss: 146588.5661 - regression_loss: 73162.5305 - val_loss: 12015.3691 - val_regression_loss: 5989.6377\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 137946.5522 - regression_loss: 68840.7073 - val_loss: 11766.3818 - val_regression_loss: 5865.1069\n","Epoch 36/50\n","537/537 [==============================] - 0s 45us/step - loss: 142818.4377 - regression_loss: 71277.5179 - val_loss: 11565.1982 - val_regression_loss: 5764.4658\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 135175.9437 - regression_loss: 67458.0627 - val_loss: 11398.0107 - val_regression_loss: 5680.8193\n","Epoch 38/50\n","537/537 [==============================] - 0s 46us/step - loss: 134114.3421 - regression_loss: 66924.1868 - val_loss: 11273.2598 - val_regression_loss: 5618.3911\n","Epoch 39/50\n","537/537 [==============================] - 0s 46us/step - loss: 132919.5587 - regression_loss: 66324.5855 - val_loss: 11179.5977 - val_regression_loss: 5571.5200\n","Epoch 40/50\n","537/537 [==============================] - 0s 50us/step - loss: 130905.7774 - regression_loss: 65317.4983 - val_loss: 11093.2803 - val_regression_loss: 5528.3345\n","Epoch 41/50\n","537/537 [==============================] - 0s 53us/step - loss: 127775.8085 - regression_loss: 63753.0086 - val_loss: 10975.7148 - val_regression_loss: 5469.5435\n","Epoch 42/50\n","537/537 [==============================] - 0s 47us/step - loss: 118737.4318 - regression_loss: 59235.0091 - val_loss: 10776.2217 - val_regression_loss: 5369.8105\n","Epoch 43/50\n","537/537 [==============================] - 0s 55us/step - loss: 122147.4950 - regression_loss: 60938.0276 - val_loss: 10541.6143 - val_regression_loss: 5252.5317\n","Epoch 44/50\n","537/537 [==============================] - 0s 50us/step - loss: 118998.6375 - regression_loss: 59363.7086 - val_loss: 10241.7842 - val_regression_loss: 5102.6577\n","Epoch 45/50\n","537/537 [==============================] - 0s 47us/step - loss: 117238.2555 - regression_loss: 58485.4308 - val_loss: 9913.1768 - val_regression_loss: 4938.3979\n","Epoch 46/50\n","537/537 [==============================] - 0s 47us/step - loss: 113366.6963 - regression_loss: 56548.5746 - val_loss: 9578.7939 - val_regression_loss: 4771.2539\n","Epoch 47/50\n","537/537 [==============================] - 0s 52us/step - loss: 109688.2981 - regression_loss: 54711.6428 - val_loss: 9246.6631 - val_regression_loss: 4605.2314\n","Epoch 48/50\n","537/537 [==============================] - 0s 53us/step - loss: 105576.6221 - regression_loss: 52656.4507 - val_loss: 8924.4863 - val_regression_loss: 4444.1802\n","Epoch 49/50\n","537/537 [==============================] - 0s 48us/step - loss: 99374.4650 - regression_loss: 49554.1054 - val_loss: 8616.6729 - val_regression_loss: 4290.2954\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 96778.7393 - regression_loss: 48256.8064 - val_loss: 8325.7998 - val_regression_loss: 4144.8765\n","***************************** elapsed_time is:  3.1606194972991943\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 55831.3588 - regression_loss: 27685.0022 - val_loss: 6025.1772 - val_regression_loss: 2984.3469\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 45772.9915 - regression_loss: 22685.0349 - val_loss: 4398.1733 - val_regression_loss: 2175.1992\n","Epoch 3/50\n","537/537 [==============================] - 0s 46us/step - loss: 33874.8867 - regression_loss: 16768.2808 - val_loss: 2690.0442 - val_regression_loss: 1325.8796\n","Epoch 4/50\n","537/537 [==============================] - 0s 43us/step - loss: 20937.3232 - regression_loss: 10335.5629 - val_loss: 1535.7780 - val_regression_loss: 753.5046\n","Epoch 5/50\n","537/537 [==============================] - 0s 50us/step - loss: 12645.4177 - regression_loss: 6225.2640 - val_loss: 1495.7874 - val_regression_loss: 736.1788\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 13393.1856 - regression_loss: 6617.8825 - val_loss: 1193.4290 - val_regression_loss: 583.3818\n","Epoch 7/50\n","537/537 [==============================] - 0s 44us/step - loss: 10775.2099 - regression_loss: 5293.0785 - val_loss: 762.4176 - val_regression_loss: 364.4972\n","Epoch 8/50\n","537/537 [==============================] - 0s 49us/step - loss: 6719.0289 - regression_loss: 3234.1582 - val_loss: 739.8690 - val_regression_loss: 350.0228\n","Epoch 9/50\n","537/537 [==============================] - 0s 54us/step - loss: 5823.1785 - regression_loss: 2757.6715 - val_loss: 976.0883 - val_regression_loss: 466.2356\n","Epoch 10/50\n","537/537 [==============================] - 0s 58us/step - loss: 7358.9783 - regression_loss: 3510.8118 - val_loss: 1046.2136 - val_regression_loss: 500.9859\n","Epoch 11/50\n","537/537 [==============================] - 0s 52us/step - loss: 7933.9988 - regression_loss: 3794.8659 - val_loss: 887.1182 - val_regression_loss: 422.2845\n","Epoch 12/50\n","537/537 [==============================] - 0s 53us/step - loss: 6814.9798 - regression_loss: 3242.0927 - val_loss: 655.3853 - val_regression_loss: 307.8602\n","Epoch 13/50\n","537/537 [==============================] - 0s 43us/step - loss: 5074.1713 - regression_loss: 2383.8786 - val_loss: 503.2503 - val_regression_loss: 233.3611\n","Epoch 14/50\n","537/537 [==============================] - 0s 51us/step - loss: 3975.6790 - regression_loss: 1848.8926 - val_loss: 483.8378 - val_regression_loss: 224.9764\n","Epoch 15/50\n","537/537 [==============================] - 0s 47us/step - loss: 4184.3092 - regression_loss: 1967.1871 - val_loss: 525.8049 - val_regression_loss: 246.7783\n","Epoch 16/50\n","537/537 [==============================] - 0s 45us/step - loss: 4553.6681 - regression_loss: 2156.5746 - val_loss: 531.8962 - val_regression_loss: 250.0026\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 4343.8150 - regression_loss: 2050.8953 - val_loss: 502.6670 - val_regression_loss: 234.9918\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 3999.3813 - regression_loss: 1874.9747 - val_loss: 489.6834 - val_regression_loss: 227.7918\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 3804.0075 - regression_loss: 1773.2621 - val_loss: 492.9381 - val_regression_loss: 228.7125\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 3722.0558 - regression_loss: 1725.6840 - val_loss: 481.9981 - val_regression_loss: 222.7287\n","Epoch 21/50\n","537/537 [==============================] - 0s 54us/step - loss: 3566.0237 - regression_loss: 1642.3426 - val_loss: 449.0076 - val_regression_loss: 205.9248\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 3293.7896 - regression_loss: 1505.0395 - val_loss: 416.1030 - val_regression_loss: 189.3128\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 3136.9008 - regression_loss: 1426.0707 - val_loss: 396.1710 - val_regression_loss: 179.3460\n","Epoch 24/50\n","537/537 [==============================] - 0s 45us/step - loss: 3209.3074 - regression_loss: 1462.6615 - val_loss: 378.8465 - val_regression_loss: 170.8398\n","Epoch 25/50\n","537/537 [==============================] - 0s 50us/step - loss: 3154.7421 - regression_loss: 1435.6101 - val_loss: 357.5860 - val_regression_loss: 160.4363\n","Epoch 26/50\n","537/537 [==============================] - 0s 48us/step - loss: 2892.1383 - regression_loss: 1308.5444 - val_loss: 343.4361 - val_regression_loss: 153.5053\n","Epoch 27/50\n","537/537 [==============================] - 0s 55us/step - loss: 2860.7886 - regression_loss: 1292.3811 - val_loss: 339.2958 - val_regression_loss: 151.5345\n","Epoch 28/50\n","537/537 [==============================] - 0s 54us/step - loss: 2845.2701 - regression_loss: 1286.9561 - val_loss: 336.3981 - val_regression_loss: 150.2219\n","Epoch 29/50\n","537/537 [==============================] - 0s 54us/step - loss: 2759.3399 - regression_loss: 1245.9944 - val_loss: 329.1650 - val_regression_loss: 146.8015\n","Epoch 30/50\n","537/537 [==============================] - 0s 45us/step - loss: 2611.3214 - regression_loss: 1173.3392 - val_loss: 322.2624 - val_regression_loss: 143.5462\n","Epoch 31/50\n","537/537 [==============================] - 0s 50us/step - loss: 2597.4400 - regression_loss: 1168.8698 - val_loss: 316.7174 - val_regression_loss: 140.8147\n","Epoch 32/50\n","537/537 [==============================] - 0s 50us/step - loss: 2503.8245 - regression_loss: 1122.2357 - val_loss: 309.0596 - val_regression_loss: 136.8463\n","Epoch 33/50\n","537/537 [==============================] - 0s 45us/step - loss: 2502.1328 - regression_loss: 1118.0057 - val_loss: 300.8164 - val_regression_loss: 132.5040\n","Epoch 34/50\n","537/537 [==============================] - 0s 53us/step - loss: 2389.6027 - regression_loss: 1060.2518 - val_loss: 295.2198 - val_regression_loss: 129.5465\n","Epoch 35/50\n","537/537 [==============================] - 0s 49us/step - loss: 2418.3235 - regression_loss: 1075.5207 - val_loss: 286.4710 - val_regression_loss: 125.1903\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 2306.9532 - regression_loss: 1018.9475 - val_loss: 277.5521 - val_regression_loss: 120.8630\n","Epoch 37/50\n","537/537 [==============================] - 0s 63us/step - loss: 2253.7786 - regression_loss: 994.1803 - val_loss: 269.9018 - val_regression_loss: 117.2266\n","Epoch 38/50\n","537/537 [==============================] - 0s 58us/step - loss: 2231.9120 - regression_loss: 984.6267 - val_loss: 261.1825 - val_regression_loss: 112.9872\n","Epoch 39/50\n","537/537 [==============================] - 0s 46us/step - loss: 2165.8258 - regression_loss: 954.1488 - val_loss: 253.5932 - val_regression_loss: 109.2517\n","Epoch 40/50\n","537/537 [==============================] - 0s 47us/step - loss: 2163.0908 - regression_loss: 956.2816 - val_loss: 247.7009 - val_regression_loss: 106.3355\n","Epoch 41/50\n","537/537 [==============================] - 0s 48us/step - loss: 2067.3068 - regression_loss: 905.6347 - val_loss: 242.6830 - val_regression_loss: 103.9291\n","Epoch 42/50\n","537/537 [==============================] - 0s 45us/step - loss: 2099.0401 - regression_loss: 924.8245 - val_loss: 239.4619 - val_regression_loss: 102.4524\n","Epoch 43/50\n","537/537 [==============================] - 0s 52us/step - loss: 2009.8261 - regression_loss: 880.6163 - val_loss: 237.4213 - val_regression_loss: 101.4990\n","Epoch 44/50\n","537/537 [==============================] - 0s 49us/step - loss: 1991.8260 - regression_loss: 873.5123 - val_loss: 232.9329 - val_regression_loss: 99.2240\n","Epoch 45/50\n","537/537 [==============================] - 0s 48us/step - loss: 1941.6608 - regression_loss: 847.6513 - val_loss: 228.1888 - val_regression_loss: 96.7958\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 1869.2421 - regression_loss: 813.0120 - val_loss: 221.8537 - val_regression_loss: 93.6281\n","Epoch 47/50\n","537/537 [==============================] - 0s 50us/step - loss: 1905.5003 - regression_loss: 830.1715 - val_loss: 216.0164 - val_regression_loss: 90.8127\n","Epoch 48/50\n","537/537 [==============================] - 0s 47us/step - loss: 1828.9884 - regression_loss: 794.9068 - val_loss: 210.8659 - val_regression_loss: 88.2865\n","Epoch 49/50\n","537/537 [==============================] - 0s 46us/step - loss: 1844.3104 - regression_loss: 800.1935 - val_loss: 206.3056 - val_regression_loss: 85.9911\n","Epoch 50/50\n","537/537 [==============================] - 0s 52us/step - loss: 1788.7504 - regression_loss: 775.8872 - val_loss: 202.9412 - val_regression_loss: 84.3099\n","***************************** elapsed_time is:  3.058241844177246\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 7502.3887 - regression_loss: 3578.8422 - val_loss: 657.2430 - val_regression_loss: 305.3394\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 5327.8615 - regression_loss: 2489.2523 - val_loss: 445.1410 - val_regression_loss: 199.2864\n","Epoch 3/50\n","537/537 [==============================] - 0s 45us/step - loss: 3751.7643 - regression_loss: 1701.6860 - val_loss: 305.4851 - val_regression_loss: 130.5379\n","Epoch 4/50\n","537/537 [==============================] - 0s 45us/step - loss: 2554.4406 - regression_loss: 1112.1730 - val_loss: 185.8828 - val_regression_loss: 72.5954\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 1439.9359 - regression_loss: 570.2816 - val_loss: 244.7399 - val_regression_loss: 103.8886\n","Epoch 6/50\n","537/537 [==============================] - 0s 47us/step - loss: 1833.7257 - regression_loss: 781.0991 - val_loss: 260.2256 - val_regression_loss: 112.6474\n","Epoch 7/50\n","537/537 [==============================] - 0s 54us/step - loss: 2032.8115 - regression_loss: 889.8813 - val_loss: 205.5742 - val_regression_loss: 85.6654\n","Epoch 8/50\n","537/537 [==============================] - 0s 47us/step - loss: 1710.5012 - regression_loss: 728.6968 - val_loss: 171.0363 - val_regression_loss: 68.3773\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 1416.3730 - regression_loss: 582.5816 - val_loss: 165.0637 - val_regression_loss: 65.3095\n","Epoch 10/50\n","537/537 [==============================] - 0s 50us/step - loss: 1290.0276 - regression_loss: 518.1314 - val_loss: 188.4458 - val_regression_loss: 76.9796\n","Epoch 11/50\n","537/537 [==============================] - 0s 50us/step - loss: 1425.4951 - regression_loss: 588.8620 - val_loss: 191.6236 - val_regression_loss: 78.6358\n","Epoch 12/50\n","537/537 [==============================] - 0s 50us/step - loss: 1486.1245 - regression_loss: 618.8302 - val_loss: 173.1118 - val_regression_loss: 69.5037\n","Epoch 13/50\n","537/537 [==============================] - 0s 51us/step - loss: 1400.4052 - regression_loss: 576.9094 - val_loss: 158.9195 - val_regression_loss: 62.5576\n","Epoch 14/50\n","537/537 [==============================] - 0s 45us/step - loss: 1285.9428 - regression_loss: 519.8909 - val_loss: 152.4098 - val_regression_loss: 59.4583\n","Epoch 15/50\n","537/537 [==============================] - 0s 54us/step - loss: 1214.8899 - regression_loss: 485.4742 - val_loss: 156.1697 - val_regression_loss: 61.4646\n","Epoch 16/50\n","537/537 [==============================] - 0s 57us/step - loss: 1201.0905 - regression_loss: 479.9189 - val_loss: 166.1727 - val_regression_loss: 66.5355\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 1279.0443 - regression_loss: 519.7991 - val_loss: 164.9017 - val_regression_loss: 65.9023\n","Epoch 18/50\n","537/537 [==============================] - 0s 57us/step - loss: 1257.3159 - regression_loss: 508.8010 - val_loss: 156.7157 - val_regression_loss: 61.7655\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 1221.5689 - regression_loss: 492.6233 - val_loss: 153.1559 - val_regression_loss: 59.9315\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 1180.8670 - regression_loss: 471.5942 - val_loss: 154.3860 - val_regression_loss: 60.5064\n","Epoch 21/50\n","537/537 [==============================] - 0s 48us/step - loss: 1184.3782 - regression_loss: 473.3089 - val_loss: 159.8158 - val_regression_loss: 63.1964\n","Epoch 22/50\n","537/537 [==============================] - 0s 51us/step - loss: 1191.1412 - regression_loss: 478.0732 - val_loss: 161.9564 - val_regression_loss: 64.2497\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 1204.1069 - regression_loss: 482.1497 - val_loss: 158.2033 - val_regression_loss: 62.3740\n","Epoch 24/50\n","537/537 [==============================] - 0s 57us/step - loss: 1187.5507 - regression_loss: 476.0814 - val_loss: 156.0058 - val_regression_loss: 61.2990\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 1188.9578 - regression_loss: 477.0181 - val_loss: 156.8131 - val_regression_loss: 61.7535\n","\n","Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 1144.3899 - regression_loss: 457.6573 - val_loss: 158.0319 - val_regression_loss: 62.3838\n","Epoch 27/50\n","537/537 [==============================] - 0s 47us/step - loss: 1157.4554 - regression_loss: 463.7641 - val_loss: 159.6351 - val_regression_loss: 63.1939\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 1177.2166 - regression_loss: 474.4325 - val_loss: 160.2382 - val_regression_loss: 63.4901\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 1166.8912 - regression_loss: 466.5746 - val_loss: 159.7607 - val_regression_loss: 63.2353\n","Epoch 30/50\n","537/537 [==============================] - 0s 46us/step - loss: 1168.9585 - regression_loss: 470.9371 - val_loss: 158.7849 - val_regression_loss: 62.7292\n","Epoch 31/50\n","537/537 [==============================] - 0s 45us/step - loss: 1179.4135 - regression_loss: 474.7179 - val_loss: 158.3734 - val_regression_loss: 62.5058\n","\n","Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 32/50\n","537/537 [==============================] - 0s 41us/step - loss: 1165.5820 - regression_loss: 466.0516 - val_loss: 158.3474 - val_regression_loss: 62.4865\n","Epoch 33/50\n","537/537 [==============================] - 0s 53us/step - loss: 1135.0072 - regression_loss: 453.5463 - val_loss: 158.4631 - val_regression_loss: 62.5408\n","Epoch 34/50\n","537/537 [==============================] - 0s 53us/step - loss: 1166.9244 - regression_loss: 468.1033 - val_loss: 158.8105 - val_regression_loss: 62.7132\n","Epoch 35/50\n","537/537 [==============================] - 0s 49us/step - loss: 1157.9451 - regression_loss: 466.0021 - val_loss: 159.1392 - val_regression_loss: 62.8766\n","Epoch 36/50\n","537/537 [==============================] - 0s 55us/step - loss: 1157.2451 - regression_loss: 465.7555 - val_loss: 159.2613 - val_regression_loss: 62.9377\n","Epoch 37/50\n","537/537 [==============================] - 0s 54us/step - loss: 1155.3599 - regression_loss: 463.4662 - val_loss: 159.0930 - val_regression_loss: 62.8533\n","Epoch 38/50\n","537/537 [==============================] - 0s 43us/step - loss: 1116.7953 - regression_loss: 444.8625 - val_loss: 158.8604 - val_regression_loss: 62.7379\n","Epoch 39/50\n","537/537 [==============================] - 0s 44us/step - loss: 1168.9585 - regression_loss: 468.7733 - val_loss: 158.5264 - val_regression_loss: 62.5708\n","Epoch 40/50\n","537/537 [==============================] - 0s 43us/step - loss: 1153.0675 - regression_loss: 464.3389 - val_loss: 158.3281 - val_regression_loss: 62.4738\n","Epoch 41/50\n","537/537 [==============================] - 0s 52us/step - loss: 1139.8940 - regression_loss: 456.3700 - val_loss: 158.4862 - val_regression_loss: 62.5549\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 1147.9178 - regression_loss: 461.0620 - val_loss: 158.7802 - val_regression_loss: 62.7033\n","Epoch 43/50\n","537/537 [==============================] - 0s 48us/step - loss: 1151.4644 - regression_loss: 461.5974 - val_loss: 159.2119 - val_regression_loss: 62.9231\n","\n","Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 44/50\n","537/537 [==============================] - 0s 56us/step - loss: 1145.2451 - regression_loss: 459.6902 - val_loss: 159.2862 - val_regression_loss: 62.9615\n","Epoch 45/50\n","537/537 [==============================] - 0s 51us/step - loss: 1113.3073 - regression_loss: 443.1436 - val_loss: 159.2963 - val_regression_loss: 62.9670\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 1178.0786 - regression_loss: 474.1644 - val_loss: 159.2570 - val_regression_loss: 62.9473\n","Epoch 47/50\n","537/537 [==============================] - 0s 53us/step - loss: 1158.4383 - regression_loss: 464.6002 - val_loss: 159.2064 - val_regression_loss: 62.9213\n","Epoch 48/50\n","537/537 [==============================] - 0s 58us/step - loss: 1154.2580 - regression_loss: 465.3981 - val_loss: 159.1299 - val_regression_loss: 62.8831\n","Epoch 49/50\n","537/537 [==============================] - 0s 52us/step - loss: 1169.4677 - regression_loss: 470.3436 - val_loss: 159.0438 - val_regression_loss: 62.8395\n","Epoch 50/50\n","537/537 [==============================] - 0s 49us/step - loss: 1152.1291 - regression_loss: 463.3187 - val_loss: 158.9859 - val_regression_loss: 62.8095\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","***************************** elapsed_time is:  3.1190648078918457\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 60006.1394 - regression_loss: 30045.3734 - val_loss: 7119.9873 - val_regression_loss: 3561.7673\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 50439.4568 - regression_loss: 25219.5979 - val_loss: 5598.0469 - val_regression_loss: 2793.6802\n","Epoch 3/50\n","537/537 [==============================] - 0s 44us/step - loss: 39105.6426 - regression_loss: 19495.6742 - val_loss: 3780.3286 - val_regression_loss: 1874.5188\n","Epoch 4/50\n","537/537 [==============================] - 0s 45us/step - loss: 25121.0186 - regression_loss: 12421.0565 - val_loss: 2197.8247 - val_regression_loss: 1069.3254\n","Epoch 5/50\n","537/537 [==============================] - 0s 52us/step - loss: 14660.4962 - regression_loss: 7077.4470 - val_loss: 1865.9856 - val_regression_loss: 888.8824\n","Epoch 6/50\n","537/537 [==============================] - 0s 49us/step - loss: 14156.5467 - regression_loss: 6705.4154 - val_loss: 1827.5416 - val_regression_loss: 871.0297\n","Epoch 7/50\n","537/537 [==============================] - 0s 49us/step - loss: 13994.9890 - regression_loss: 6641.7880 - val_loss: 1389.7554 - val_regression_loss: 664.3885\n","Epoch 8/50\n","537/537 [==============================] - 0s 54us/step - loss: 9789.3528 - regression_loss: 4642.2841 - val_loss: 1150.9988 - val_regression_loss: 558.1736\n","Epoch 9/50\n","537/537 [==============================] - 0s 57us/step - loss: 6890.7594 - regression_loss: 3300.7604 - val_loss: 1279.2615 - val_regression_loss: 631.4417\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 7098.9756 - regression_loss: 3476.3760 - val_loss: 1481.0233 - val_regression_loss: 736.0704\n","Epoch 11/50\n","537/537 [==============================] - 0s 49us/step - loss: 8628.7068 - regression_loss: 4273.6186 - val_loss: 1446.6288 - val_regression_loss: 717.9866\n","Epoch 12/50\n","537/537 [==============================] - 0s 52us/step - loss: 8472.3969 - regression_loss: 4189.4449 - val_loss: 1215.7008 - val_regression_loss: 598.9499\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 6829.9146 - regression_loss: 3345.3995 - val_loss: 988.5239 - val_regression_loss: 480.7174\n","Epoch 14/50\n","537/537 [==============================] - 0s 45us/step - loss: 5376.6797 - regression_loss: 2581.4584 - val_loss: 903.6129 - val_regression_loss: 433.7657\n","Epoch 15/50\n","537/537 [==============================] - 0s 49us/step - loss: 4986.9615 - regression_loss: 2353.7126 - val_loss: 925.9958 - val_regression_loss: 441.4675\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 5300.6562 - regression_loss: 2482.4927 - val_loss: 923.8082 - val_regression_loss: 438.5509\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 5433.0573 - regression_loss: 2532.1424 - val_loss: 849.9353 - val_regression_loss: 401.7411\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 5014.0633 - regression_loss: 2325.5737 - val_loss: 778.5510 - val_regression_loss: 367.8726\n","Epoch 19/50\n","537/537 [==============================] - 0s 58us/step - loss: 4246.7080 - regression_loss: 1956.4233 - val_loss: 765.2341 - val_regression_loss: 363.9278\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 3978.1899 - regression_loss: 1846.0170 - val_loss: 779.2625 - val_regression_loss: 373.6270\n","Epoch 21/50\n","537/537 [==============================] - 0s 45us/step - loss: 3899.7963 - regression_loss: 1829.5658 - val_loss: 769.3030 - val_regression_loss: 370.5784\n","Epoch 22/50\n","537/537 [==============================] - 0s 50us/step - loss: 3655.2584 - regression_loss: 1725.1302 - val_loss: 733.6401 - val_regression_loss: 353.6254\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 3413.0820 - regression_loss: 1610.1485 - val_loss: 699.5521 - val_regression_loss: 336.3432\n","Epoch 24/50\n","537/537 [==============================] - 0s 47us/step - loss: 3286.1289 - regression_loss: 1551.6766 - val_loss: 674.9196 - val_regression_loss: 322.7072\n","Epoch 25/50\n","537/537 [==============================] - 0s 49us/step - loss: 3114.0021 - regression_loss: 1454.1127 - val_loss: 650.6953 - val_regression_loss: 308.7522\n","Epoch 26/50\n","537/537 [==============================] - 0s 42us/step - loss: 2994.7369 - regression_loss: 1381.6554 - val_loss: 627.3904 - val_regression_loss: 295.4586\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 2722.0560 - regression_loss: 1230.1204 - val_loss: 616.7997 - val_regression_loss: 289.2723\n","Epoch 28/50\n","537/537 [==============================] - 0s 47us/step - loss: 2584.6702 - regression_loss: 1156.5098 - val_loss: 614.0919 - val_regression_loss: 287.7663\n","Epoch 29/50\n","537/537 [==============================] - 0s 51us/step - loss: 2517.1828 - regression_loss: 1120.2267 - val_loss: 599.2715 - val_regression_loss: 280.5955\n","Epoch 30/50\n","537/537 [==============================] - 0s 49us/step - loss: 2493.9276 - regression_loss: 1114.3975 - val_loss: 573.4626 - val_regression_loss: 268.1331\n","Epoch 31/50\n","537/537 [==============================] - 0s 53us/step - loss: 2294.9516 - regression_loss: 1015.8821 - val_loss: 548.3707 - val_regression_loss: 256.2358\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 2104.2575 - regression_loss: 923.4270 - val_loss: 529.3043 - val_regression_loss: 247.5736\n","Epoch 33/50\n","537/537 [==============================] - 0s 46us/step - loss: 2099.9640 - regression_loss: 928.1176 - val_loss: 511.5991 - val_regression_loss: 239.5811\n","Epoch 34/50\n","537/537 [==============================] - 0s 44us/step - loss: 2055.8753 - regression_loss: 909.8548 - val_loss: 493.8223 - val_regression_loss: 231.2981\n","Epoch 35/50\n","537/537 [==============================] - 0s 50us/step - loss: 1928.2784 - regression_loss: 850.8285 - val_loss: 476.1527 - val_regression_loss: 222.5166\n","Epoch 36/50\n","537/537 [==============================] - 0s 49us/step - loss: 1886.3283 - regression_loss: 828.8322 - val_loss: 457.7829 - val_regression_loss: 212.9386\n","Epoch 37/50\n","537/537 [==============================] - 0s 53us/step - loss: 1830.6328 - regression_loss: 798.2114 - val_loss: 440.8366 - val_regression_loss: 203.8502\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 1708.8722 - regression_loss: 728.6067 - val_loss: 428.4117 - val_regression_loss: 197.2135\n","Epoch 39/50\n","537/537 [==============================] - 0s 46us/step - loss: 1729.7338 - regression_loss: 734.0534 - val_loss: 420.5125 - val_regression_loss: 193.2784\n","Epoch 40/50\n","537/537 [==============================] - 0s 49us/step - loss: 1709.4618 - regression_loss: 724.2196 - val_loss: 411.5096 - val_regression_loss: 189.2017\n","Epoch 41/50\n","537/537 [==============================] - 0s 47us/step - loss: 1670.9994 - regression_loss: 711.6830 - val_loss: 403.1765 - val_regression_loss: 185.6435\n","Epoch 42/50\n","537/537 [==============================] - 0s 49us/step - loss: 1596.1798 - regression_loss: 676.4754 - val_loss: 394.6749 - val_regression_loss: 181.8982\n","Epoch 43/50\n","537/537 [==============================] - 0s 47us/step - loss: 1620.3687 - regression_loss: 693.2393 - val_loss: 384.4924 - val_regression_loss: 176.9129\n","Epoch 44/50\n","537/537 [==============================] - 0s 53us/step - loss: 1625.5075 - regression_loss: 696.0076 - val_loss: 372.2817 - val_regression_loss: 170.5693\n","Epoch 45/50\n","537/537 [==============================] - 0s 49us/step - loss: 1597.4410 - regression_loss: 682.2577 - val_loss: 363.8906 - val_regression_loss: 166.0250\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 1568.3840 - regression_loss: 662.7786 - val_loss: 361.4468 - val_regression_loss: 164.6891\n","Epoch 47/50\n","537/537 [==============================] - 0s 48us/step - loss: 1559.8751 - regression_loss: 657.8188 - val_loss: 359.5575 - val_regression_loss: 163.7607\n","Epoch 48/50\n","537/537 [==============================] - 0s 48us/step - loss: 1467.1266 - regression_loss: 610.3188 - val_loss: 357.9774 - val_regression_loss: 162.9915\n","Epoch 49/50\n","537/537 [==============================] - 0s 50us/step - loss: 1530.2683 - regression_loss: 647.1304 - val_loss: 354.0286 - val_regression_loss: 160.9484\n","Epoch 50/50\n","537/537 [==============================] - 0s 49us/step - loss: 1512.6708 - regression_loss: 634.2697 - val_loss: 348.4226 - val_regression_loss: 158.1851\n","***************************** elapsed_time is:  3.3605191707611084\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 93518.2282 - regression_loss: 46722.6141 - val_loss: 8853.3936 - val_regression_loss: 4416.6729\n","Epoch 2/50\n","537/537 [==============================] - 0s 47us/step - loss: 77181.6417 - regression_loss: 38541.7643 - val_loss: 6869.8438 - val_regression_loss: 3422.0786\n","Epoch 3/50\n","537/537 [==============================] - 0s 54us/step - loss: 59907.6121 - regression_loss: 29881.3979 - val_loss: 4492.0386 - val_regression_loss: 2227.9346\n","Epoch 4/50\n","537/537 [==============================] - 0s 49us/step - loss: 38694.4840 - regression_loss: 19230.6382 - val_loss: 2350.7517 - val_regression_loss: 1147.7742\n","Epoch 5/50\n","537/537 [==============================] - 0s 44us/step - loss: 18997.4312 - regression_loss: 9308.0396 - val_loss: 1781.3002 - val_regression_loss: 849.2526\n","Epoch 6/50\n","537/537 [==============================] - 0s 44us/step - loss: 11924.5732 - regression_loss: 5664.7436 - val_loss: 2489.9485 - val_regression_loss: 1197.5275\n","Epoch 7/50\n","537/537 [==============================] - 0s 42us/step - loss: 16764.7448 - regression_loss: 8035.9248 - val_loss: 1957.0670 - val_regression_loss: 938.7722\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 13367.6110 - regression_loss: 6401.4698 - val_loss: 1024.5895 - val_regression_loss: 483.5446\n","Epoch 9/50\n","537/537 [==============================] - 0s 57us/step - loss: 7907.7297 - regression_loss: 3759.5749 - val_loss: 553.7543 - val_regression_loss: 256.6102\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 5684.4708 - regression_loss: 2716.0248 - val_loss: 563.7789 - val_regression_loss: 266.2981\n","Epoch 11/50\n","537/537 [==============================] - 0s 48us/step - loss: 6818.8479 - regression_loss: 3319.8596 - val_loss: 689.0837 - val_regression_loss: 330.4603\n","Epoch 12/50\n","537/537 [==============================] - 0s 48us/step - loss: 8056.8226 - regression_loss: 3950.0984 - val_loss: 698.3177 - val_regression_loss: 334.5934\n","Epoch 13/50\n","537/537 [==============================] - 0s 47us/step - loss: 7880.0257 - regression_loss: 3855.3102 - val_loss: 589.4435 - val_regression_loss: 278.6100\n","Epoch 14/50\n","537/537 [==============================] - 0s 45us/step - loss: 6456.8407 - regression_loss: 3131.8913 - val_loss: 467.5994 - val_regression_loss: 215.6900\n","Epoch 15/50\n","537/537 [==============================] - 0s 47us/step - loss: 4762.1598 - regression_loss: 2265.4580 - val_loss: 439.2062 - val_regression_loss: 199.4855\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 3806.3408 - regression_loss: 1772.3351 - val_loss: 529.5831 - val_regression_loss: 243.0367\n","Epoch 17/50\n","537/537 [==============================] - 0s 49us/step - loss: 3926.6649 - regression_loss: 1819.5709 - val_loss: 648.0079 - val_regression_loss: 301.3095\n","Epoch 18/50\n","537/537 [==============================] - 0s 56us/step - loss: 4600.8056 - regression_loss: 2152.3622 - val_loss: 671.9794 - val_regression_loss: 313.1907\n","Epoch 19/50\n","537/537 [==============================] - 0s 54us/step - loss: 4615.8349 - regression_loss: 2157.1896 - val_loss: 587.3928 - val_regression_loss: 271.4955\n","Epoch 20/50\n","537/537 [==============================] - 0s 58us/step - loss: 4203.5257 - regression_loss: 1956.5890 - val_loss: 470.9121 - val_regression_loss: 214.2579\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 3599.8468 - regression_loss: 1662.7221 - val_loss: 396.7082 - val_regression_loss: 178.2437\n","Epoch 22/50\n","537/537 [==============================] - 0s 50us/step - loss: 3361.1985 - regression_loss: 1551.1212 - val_loss: 367.9308 - val_regression_loss: 164.8015\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 3452.6165 - regression_loss: 1606.3433 - val_loss: 356.3393 - val_regression_loss: 159.6846\n","Epoch 24/50\n","537/537 [==============================] - 0s 46us/step - loss: 3426.7239 - regression_loss: 1597.3274 - val_loss: 342.4152 - val_regression_loss: 153.0887\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 3414.9306 - regression_loss: 1596.1280 - val_loss: 328.0021 - val_regression_loss: 145.9096\n","Epoch 26/50\n","537/537 [==============================] - 0s 50us/step - loss: 3277.2147 - regression_loss: 1530.3668 - val_loss: 325.4756 - val_regression_loss: 144.3573\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 3017.3742 - regression_loss: 1396.3894 - val_loss: 336.8385 - val_regression_loss: 149.5068\n","Epoch 28/50\n","537/537 [==============================] - 0s 55us/step - loss: 2873.8331 - regression_loss: 1319.7538 - val_loss: 351.2194 - val_regression_loss: 156.0411\n","Epoch 29/50\n","537/537 [==============================] - 0s 56us/step - loss: 2809.6460 - regression_loss: 1280.8732 - val_loss: 355.7685 - val_regression_loss: 157.6958\n","Epoch 30/50\n","537/537 [==============================] - 0s 53us/step - loss: 2731.5249 - regression_loss: 1240.2285 - val_loss: 348.3317 - val_regression_loss: 153.5716\n","Epoch 31/50\n","537/537 [==============================] - 0s 53us/step - loss: 2665.9695 - regression_loss: 1202.3981 - val_loss: 335.8781 - val_regression_loss: 147.2314\n","Epoch 32/50\n","537/537 [==============================] - 0s 52us/step - loss: 2529.1156 - regression_loss: 1130.8896 - val_loss: 321.8629 - val_regression_loss: 140.3847\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 2516.7656 - regression_loss: 1126.5523 - val_loss: 307.9624 - val_regression_loss: 133.7474\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 2411.3227 - regression_loss: 1077.0231 - val_loss: 293.7990 - val_regression_loss: 127.0441\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 2399.6370 - regression_loss: 1077.1648 - val_loss: 281.5428 - val_regression_loss: 121.2447\n","Epoch 36/50\n","537/537 [==============================] - 0s 46us/step - loss: 2309.2049 - regression_loss: 1034.3917 - val_loss: 272.7228 - val_regression_loss: 117.0757\n","Epoch 37/50\n","537/537 [==============================] - 0s 48us/step - loss: 2287.2063 - regression_loss: 1024.8367 - val_loss: 264.1618 - val_regression_loss: 112.8822\n","Epoch 38/50\n","537/537 [==============================] - 0s 49us/step - loss: 2235.6505 - regression_loss: 997.7682 - val_loss: 255.4000 - val_regression_loss: 108.4711\n","Epoch 39/50\n","537/537 [==============================] - 0s 45us/step - loss: 2169.4258 - regression_loss: 965.5027 - val_loss: 249.1401 - val_regression_loss: 105.2092\n","Epoch 40/50\n","537/537 [==============================] - 0s 47us/step - loss: 2139.3106 - regression_loss: 950.9463 - val_loss: 247.1555 - val_regression_loss: 104.0696\n","Epoch 41/50\n","537/537 [==============================] - 0s 54us/step - loss: 2074.2663 - regression_loss: 916.9574 - val_loss: 248.2565 - val_regression_loss: 104.4694\n","Epoch 42/50\n","537/537 [==============================] - 0s 47us/step - loss: 2081.7886 - regression_loss: 919.8489 - val_loss: 248.7335 - val_regression_loss: 104.6210\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 1996.3444 - regression_loss: 875.3488 - val_loss: 247.7398 - val_regression_loss: 104.0933\n","Epoch 44/50\n","537/537 [==============================] - 0s 51us/step - loss: 1960.7356 - regression_loss: 857.3290 - val_loss: 241.7480 - val_regression_loss: 101.1542\n","Epoch 45/50\n","537/537 [==============================] - 0s 46us/step - loss: 1902.7173 - regression_loss: 831.1583 - val_loss: 233.6506 - val_regression_loss: 97.2095\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 1939.6826 - regression_loss: 847.1570 - val_loss: 226.0318 - val_regression_loss: 93.5291\n","Epoch 47/50\n","537/537 [==============================] - 0s 52us/step - loss: 1875.4102 - regression_loss: 816.4360 - val_loss: 221.1086 - val_regression_loss: 91.2002\n","Epoch 48/50\n","537/537 [==============================] - 0s 48us/step - loss: 1851.7149 - regression_loss: 806.1165 - val_loss: 219.7203 - val_regression_loss: 90.5475\n","Epoch 49/50\n","537/537 [==============================] - 0s 53us/step - loss: 1805.7192 - regression_loss: 782.3434 - val_loss: 219.4232 - val_regression_loss: 90.3835\n","Epoch 50/50\n","537/537 [==============================] - 0s 56us/step - loss: 1787.9765 - regression_loss: 774.1857 - val_loss: 219.4861 - val_regression_loss: 90.3175\n","***************************** elapsed_time is:  3.074779510498047\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 10544.7997 - regression_loss: 5154.1474 - val_loss: 1023.7789 - val_regression_loss: 492.3807\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 7188.7429 - regression_loss: 3418.4469 - val_loss: 648.0208 - val_regression_loss: 296.0276\n","Epoch 3/50\n","537/537 [==============================] - 0s 47us/step - loss: 4659.2692 - regression_loss: 2084.1012 - val_loss: 448.3972 - val_regression_loss: 187.7569\n","Epoch 4/50\n","537/537 [==============================] - 0s 44us/step - loss: 3499.6451 - regression_loss: 1436.9088 - val_loss: 311.4758 - val_regression_loss: 123.8045\n","Epoch 5/50\n","537/537 [==============================] - 0s 46us/step - loss: 2390.8291 - regression_loss: 924.7072 - val_loss: 274.6605 - val_regression_loss: 117.4758\n","Epoch 6/50\n","537/537 [==============================] - 0s 49us/step - loss: 1814.8029 - regression_loss: 742.9523 - val_loss: 374.5936 - val_regression_loss: 177.7091\n","Epoch 7/50\n","537/537 [==============================] - 0s 44us/step - loss: 2382.1208 - regression_loss: 1112.1326 - val_loss: 377.3185 - val_regression_loss: 180.7260\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 2326.6481 - regression_loss: 1098.5821 - val_loss: 291.3625 - val_regression_loss: 134.1993\n","Epoch 9/50\n","537/537 [==============================] - 0s 47us/step - loss: 1727.7322 - regression_loss: 766.8112 - val_loss: 228.3454 - val_regression_loss: 97.9026\n","Epoch 10/50\n","537/537 [==============================] - 0s 51us/step - loss: 1446.8392 - regression_loss: 586.3737 - val_loss: 214.8904 - val_regression_loss: 87.2632\n","Epoch 11/50\n","537/537 [==============================] - 0s 47us/step - loss: 1550.0817 - regression_loss: 608.7787 - val_loss: 221.8592 - val_regression_loss: 88.9092\n","Epoch 12/50\n","537/537 [==============================] - 0s 44us/step - loss: 1705.8762 - regression_loss: 670.7952 - val_loss: 220.7337 - val_regression_loss: 88.8015\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 1650.8052 - regression_loss: 649.6827 - val_loss: 216.1547 - val_regression_loss: 88.5246\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 1484.7505 - regression_loss: 584.5173 - val_loss: 217.7288 - val_regression_loss: 92.0317\n","Epoch 15/50\n","537/537 [==============================] - 0s 54us/step - loss: 1369.2695 - regression_loss: 549.8534 - val_loss: 224.8075 - val_regression_loss: 98.2311\n","Epoch 16/50\n","537/537 [==============================] - 0s 50us/step - loss: 1375.5032 - regression_loss: 575.4252 - val_loss: 228.5001 - val_regression_loss: 101.9974\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 1423.5201 - regression_loss: 616.7556 - val_loss: 222.8636 - val_regression_loss: 99.7519\n","Epoch 18/50\n","537/537 [==============================] - 0s 47us/step - loss: 1406.9192 - regression_loss: 613.3363 - val_loss: 208.6736 - val_regression_loss: 91.7979\n","Epoch 19/50\n","537/537 [==============================] - 0s 47us/step - loss: 1341.2739 - regression_loss: 576.3085 - val_loss: 194.4659 - val_regression_loss: 82.9120\n","Epoch 20/50\n","537/537 [==============================] - 0s 47us/step - loss: 1299.3612 - regression_loss: 538.0636 - val_loss: 187.7903 - val_regression_loss: 77.8075\n","Epoch 21/50\n","537/537 [==============================] - 0s 58us/step - loss: 1307.2122 - regression_loss: 525.6900 - val_loss: 187.0957 - val_regression_loss: 76.2907\n","Epoch 22/50\n","537/537 [==============================] - 0s 56us/step - loss: 1318.2625 - regression_loss: 525.0190 - val_loss: 188.9457 - val_regression_loss: 76.8639\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 1295.1413 - regression_loss: 510.2972 - val_loss: 190.6086 - val_regression_loss: 78.1149\n","Epoch 24/50\n","537/537 [==============================] - 0s 54us/step - loss: 1287.8596 - regression_loss: 511.3007 - val_loss: 190.8915 - val_regression_loss: 79.1427\n","Epoch 25/50\n","537/537 [==============================] - 0s 48us/step - loss: 1261.7935 - regression_loss: 508.9578 - val_loss: 189.6274 - val_regression_loss: 79.5250\n","Epoch 26/50\n","537/537 [==============================] - 0s 50us/step - loss: 1248.5941 - regression_loss: 509.3691 - val_loss: 186.8468 - val_regression_loss: 78.8380\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 1244.3227 - regression_loss: 511.3591 - val_loss: 182.9704 - val_regression_loss: 77.1316\n","Epoch 28/50\n","537/537 [==============================] - 0s 54us/step - loss: 1257.7490 - regression_loss: 517.1169 - val_loss: 179.6149 - val_regression_loss: 75.2895\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 1265.7445 - regression_loss: 520.0403 - val_loss: 177.5410 - val_regression_loss: 73.8111\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1249.7174 - regression_loss: 511.1067 - val_loss: 177.5822 - val_regression_loss: 73.3658\n","Epoch 31/50\n","537/537 [==============================] - 0s 45us/step - loss: 1240.6769 - regression_loss: 502.7879 - val_loss: 180.1304 - val_regression_loss: 74.4091\n","Epoch 32/50\n","537/537 [==============================] - 0s 54us/step - loss: 1229.8436 - regression_loss: 493.4930 - val_loss: 183.3696 - val_regression_loss: 76.0591\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 1232.0202 - regression_loss: 497.1679 - val_loss: 184.1870 - val_regression_loss: 76.6803\n","Epoch 34/50\n","537/537 [==============================] - 0s 47us/step - loss: 1205.6782 - regression_loss: 484.7214 - val_loss: 182.5089 - val_regression_loss: 76.1801\n","Epoch 35/50\n","537/537 [==============================] - 0s 50us/step - loss: 1227.0091 - regression_loss: 495.8288 - val_loss: 181.0334 - val_regression_loss: 75.7209\n","Epoch 36/50\n","537/537 [==============================] - 0s 45us/step - loss: 1206.0356 - regression_loss: 489.1033 - val_loss: 178.1197 - val_regression_loss: 74.3601\n","Epoch 37/50\n","537/537 [==============================] - 0s 51us/step - loss: 1235.3567 - regression_loss: 503.6052 - val_loss: 177.1706 - val_regression_loss: 73.8436\n","Epoch 38/50\n","537/537 [==============================] - 0s 55us/step - loss: 1201.3359 - regression_loss: 488.0004 - val_loss: 178.0607 - val_regression_loss: 74.1656\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 1200.6599 - regression_loss: 486.9012 - val_loss: 179.3114 - val_regression_loss: 74.5863\n","Epoch 40/50\n","537/537 [==============================] - 0s 49us/step - loss: 1196.4825 - regression_loss: 484.6422 - val_loss: 180.0546 - val_regression_loss: 74.8199\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 1194.8363 - regression_loss: 484.0490 - val_loss: 178.6612 - val_regression_loss: 74.0305\n","Epoch 42/50\n","537/537 [==============================] - 0s 56us/step - loss: 1178.2565 - regression_loss: 473.9145 - val_loss: 177.8764 - val_regression_loss: 73.7326\n","Epoch 43/50\n","537/537 [==============================] - 0s 49us/step - loss: 1196.2989 - regression_loss: 483.3790 - val_loss: 176.6702 - val_regression_loss: 73.2818\n","Epoch 44/50\n","537/537 [==============================] - 0s 54us/step - loss: 1180.8804 - regression_loss: 476.2080 - val_loss: 177.8621 - val_regression_loss: 74.0457\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 1174.8788 - regression_loss: 474.7446 - val_loss: 177.5893 - val_regression_loss: 73.9111\n","Epoch 46/50\n","537/537 [==============================] - 0s 64us/step - loss: 1182.1331 - regression_loss: 477.4565 - val_loss: 177.9195 - val_regression_loss: 73.9762\n","Epoch 47/50\n","537/537 [==============================] - 0s 50us/step - loss: 1165.1628 - regression_loss: 470.9532 - val_loss: 177.7030 - val_regression_loss: 73.6690\n","Epoch 48/50\n","537/537 [==============================] - 0s 52us/step - loss: 1179.9487 - regression_loss: 476.2732 - val_loss: 177.4244 - val_regression_loss: 73.3984\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 1171.9532 - regression_loss: 472.4973 - val_loss: 177.7217 - val_regression_loss: 73.4596\n","Epoch 50/50\n","537/537 [==============================] - 0s 52us/step - loss: 1167.9295 - regression_loss: 472.0978 - val_loss: 176.5164 - val_regression_loss: 72.8559\n","***************************** elapsed_time is:  3.0906379222869873\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 98115.2178 - regression_loss: 48962.5974 - val_loss: 10295.7549 - val_regression_loss: 5134.2964\n","Epoch 2/50\n","537/537 [==============================] - 0s 52us/step - loss: 83937.6400 - regression_loss: 41870.8272 - val_loss: 8266.6172 - val_regression_loss: 4118.4277\n","Epoch 3/50\n","537/537 [==============================] - 0s 49us/step - loss: 66420.7806 - regression_loss: 33103.2258 - val_loss: 5877.9795 - val_regression_loss: 2921.3760\n","Epoch 4/50\n","537/537 [==============================] - 0s 50us/step - loss: 45586.7118 - regression_loss: 22661.9005 - val_loss: 3416.7969 - val_regression_loss: 1685.7489\n","Epoch 5/50\n","537/537 [==============================] - 0s 42us/step - loss: 26708.7013 - regression_loss: 13184.0151 - val_loss: 2099.9851 - val_regression_loss: 1019.8030\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 16554.9958 - regression_loss: 8046.0032 - val_loss: 2784.3074 - val_regression_loss: 1356.2164\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 21494.9845 - regression_loss: 10470.7811 - val_loss: 2695.6646 - val_regression_loss: 1314.5121\n","Epoch 8/50\n","537/537 [==============================] - 0s 52us/step - loss: 20183.0303 - regression_loss: 9836.7493 - val_loss: 1808.6215 - val_regression_loss: 877.2373\n","Epoch 9/50\n","537/537 [==============================] - 0s 54us/step - loss: 13170.0463 - regression_loss: 6381.1906 - val_loss: 1269.6062 - val_regression_loss: 613.6971\n","Epoch 10/50\n","537/537 [==============================] - 0s 57us/step - loss: 9167.8910 - regression_loss: 4427.6886 - val_loss: 1319.6239 - val_regression_loss: 642.8712\n","Epoch 11/50\n","537/537 [==============================] - 0s 47us/step - loss: 9987.4886 - regression_loss: 4871.3797 - val_loss: 1516.1959 - val_regression_loss: 743.2856\n","Epoch 12/50\n","537/537 [==============================] - 0s 53us/step - loss: 12207.3231 - regression_loss: 5996.5288 - val_loss: 1485.4667 - val_regression_loss: 728.4554\n","Epoch 13/50\n","537/537 [==============================] - 0s 46us/step - loss: 11912.1650 - regression_loss: 5853.4005 - val_loss: 1231.0240 - val_regression_loss: 600.7699\n","Epoch 14/50\n","537/537 [==============================] - 0s 52us/step - loss: 9995.8819 - regression_loss: 4890.6768 - val_loss: 960.4393 - val_regression_loss: 464.4750\n","Epoch 15/50\n","537/537 [==============================] - 0s 49us/step - loss: 8062.8755 - regression_loss: 3916.1506 - val_loss: 850.9496 - val_regression_loss: 408.5222\n","Epoch 16/50\n","537/537 [==============================] - 0s 51us/step - loss: 7127.2559 - regression_loss: 3437.8026 - val_loss: 909.1204 - val_regression_loss: 436.4854\n","Epoch 17/50\n","537/537 [==============================] - 0s 45us/step - loss: 7547.1552 - regression_loss: 3641.1086 - val_loss: 976.9880 - val_regression_loss: 469.6323\n","Epoch 18/50\n","537/537 [==============================] - 0s 53us/step - loss: 7834.9111 - regression_loss: 3776.1308 - val_loss: 934.8975 - val_regression_loss: 448.3502\n","Epoch 19/50\n","537/537 [==============================] - 0s 45us/step - loss: 7243.7744 - regression_loss: 3479.2137 - val_loss: 825.7797 - val_regression_loss: 394.1275\n","Epoch 20/50\n","537/537 [==============================] - 0s 56us/step - loss: 6452.8775 - regression_loss: 3089.1615 - val_loss: 753.1394 - val_regression_loss: 358.5092\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 5956.1594 - regression_loss: 2846.3081 - val_loss: 727.0891 - val_regression_loss: 346.2874\n","Epoch 22/50\n","537/537 [==============================] - 0s 49us/step - loss: 5810.8855 - regression_loss: 2784.3629 - val_loss: 700.3952 - val_regression_loss: 333.6208\n","Epoch 23/50\n","537/537 [==============================] - 0s 50us/step - loss: 5491.0190 - regression_loss: 2627.3071 - val_loss: 648.9606 - val_regression_loss: 308.3328\n","Epoch 24/50\n","537/537 [==============================] - 0s 45us/step - loss: 5142.6056 - regression_loss: 2456.9843 - val_loss: 594.9979 - val_regression_loss: 281.5077\n","Epoch 25/50\n","537/537 [==============================] - 0s 44us/step - loss: 4822.0892 - regression_loss: 2297.8478 - val_loss: 569.0685 - val_regression_loss: 268.4504\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 4217.7980 - regression_loss: 1996.6277 - val_loss: 555.2912 - val_regression_loss: 261.2527\n","Epoch 27/50\n","537/537 [==============================] - 0s 45us/step - loss: 4387.0565 - regression_loss: 2078.3601 - val_loss: 521.6343 - val_regression_loss: 244.0097\n","Epoch 28/50\n","537/537 [==============================] - 0s 50us/step - loss: 4027.4299 - regression_loss: 1894.9047 - val_loss: 469.3578 - val_regression_loss: 217.5009\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 3778.6291 - regression_loss: 1769.9426 - val_loss: 429.3379 - val_regression_loss: 197.2466\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 3495.1788 - regression_loss: 1624.3480 - val_loss: 407.5376 - val_regression_loss: 186.1715\n","Epoch 31/50\n","537/537 [==============================] - 0s 50us/step - loss: 3500.0668 - regression_loss: 1628.8541 - val_loss: 387.5284 - val_regression_loss: 175.9041\n","Epoch 32/50\n","537/537 [==============================] - 0s 56us/step - loss: 3291.9407 - regression_loss: 1522.5328 - val_loss: 370.9591 - val_regression_loss: 167.3414\n","Epoch 33/50\n","537/537 [==============================] - 0s 49us/step - loss: 2906.8034 - regression_loss: 1329.2522 - val_loss: 364.1961 - val_regression_loss: 163.8629\n","Epoch 34/50\n","537/537 [==============================] - 0s 56us/step - loss: 2973.5499 - regression_loss: 1362.7527 - val_loss: 353.5118 - val_regression_loss: 158.6780\n","Epoch 35/50\n","537/537 [==============================] - 0s 50us/step - loss: 2936.1095 - regression_loss: 1346.1065 - val_loss: 329.5150 - val_regression_loss: 147.0708\n","Epoch 36/50\n","537/537 [==============================] - 0s 46us/step - loss: 2798.5664 - regression_loss: 1277.7489 - val_loss: 310.7904 - val_regression_loss: 138.0900\n","Epoch 37/50\n","537/537 [==============================] - 0s 51us/step - loss: 2669.4268 - regression_loss: 1215.1630 - val_loss: 301.4156 - val_regression_loss: 133.5346\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 2692.4990 - regression_loss: 1227.4991 - val_loss: 299.4083 - val_regression_loss: 132.3289\n","Epoch 39/50\n","537/537 [==============================] - 0s 45us/step - loss: 2496.0707 - regression_loss: 1129.7943 - val_loss: 301.4824 - val_regression_loss: 133.0504\n","Epoch 40/50\n","537/537 [==============================] - 0s 44us/step - loss: 2488.4974 - regression_loss: 1124.8703 - val_loss: 295.8571 - val_regression_loss: 130.0468\n","Epoch 41/50\n","537/537 [==============================] - 0s 46us/step - loss: 2348.8386 - regression_loss: 1051.7583 - val_loss: 275.7397 - val_regression_loss: 120.0279\n","Epoch 42/50\n","537/537 [==============================] - 0s 54us/step - loss: 2341.8802 - regression_loss: 1047.8809 - val_loss: 258.1154 - val_regression_loss: 111.3730\n","Epoch 43/50\n","537/537 [==============================] - 0s 56us/step - loss: 2310.4839 - regression_loss: 1035.1087 - val_loss: 251.1228 - val_regression_loss: 107.9504\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 2108.1803 - regression_loss: 933.0306 - val_loss: 251.7667 - val_regression_loss: 108.3076\n","Epoch 45/50\n","537/537 [==============================] - 0s 53us/step - loss: 2143.2460 - regression_loss: 953.3098 - val_loss: 255.2972 - val_regression_loss: 110.0621\n","Epoch 46/50\n","537/537 [==============================] - 0s 43us/step - loss: 1991.6066 - regression_loss: 873.7113 - val_loss: 249.0780 - val_regression_loss: 107.0879\n","Epoch 47/50\n","537/537 [==============================] - 0s 45us/step - loss: 2013.4759 - regression_loss: 884.3228 - val_loss: 240.5170 - val_regression_loss: 102.9462\n","Epoch 48/50\n","537/537 [==============================] - 0s 47us/step - loss: 2038.8509 - regression_loss: 900.4789 - val_loss: 231.1492 - val_regression_loss: 98.3448\n","Epoch 49/50\n","537/537 [==============================] - 0s 47us/step - loss: 1981.7420 - regression_loss: 872.1962 - val_loss: 228.2412 - val_regression_loss: 96.8785\n","Epoch 50/50\n","537/537 [==============================] - 0s 46us/step - loss: 1974.6563 - regression_loss: 866.3931 - val_loss: 229.4882 - val_regression_loss: 97.3985\n","***************************** elapsed_time is:  3.305079698562622\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 829110.4724 - regression_loss: 414104.8459 - val_loss: 95021.6406 - val_regression_loss: 47454.4336\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 781724.9956 - regression_loss: 390439.7428 - val_loss: 88868.0156 - val_regression_loss: 44382.0742\n","Epoch 3/50\n","537/537 [==============================] - 0s 48us/step - loss: 741061.5847 - regression_loss: 370143.5891 - val_loss: 80512.7031 - val_regression_loss: 40210.6250\n","Epoch 4/50\n","537/537 [==============================] - 0s 44us/step - loss: 614907.4110 - regression_loss: 307127.0901 - val_loss: 70004.5234 - val_regression_loss: 34964.1250\n","Epoch 5/50\n","537/537 [==============================] - 0s 52us/step - loss: 577546.8851 - regression_loss: 288489.9506 - val_loss: 57513.6602 - val_regression_loss: 28727.1191\n","Epoch 6/50\n","537/537 [==============================] - 0s 46us/step - loss: 475355.5175 - regression_loss: 237462.3964 - val_loss: 44193.4297 - val_regression_loss: 22074.8516\n","Epoch 7/50\n","537/537 [==============================] - 0s 49us/step - loss: 384747.8799 - regression_loss: 192221.8166 - val_loss: 31587.5059 - val_regression_loss: 15777.6934\n","Epoch 8/50\n","537/537 [==============================] - 0s 43us/step - loss: 280535.8097 - regression_loss: 140159.8608 - val_loss: 22150.4629 - val_regression_loss: 11060.8008\n","Epoch 9/50\n","537/537 [==============================] - 0s 46us/step - loss: 222436.8804 - regression_loss: 111115.6440 - val_loss: 19516.2793 - val_regression_loss: 9736.9766\n","Epoch 10/50\n","537/537 [==============================] - 0s 48us/step - loss: 211524.2902 - regression_loss: 105586.3413 - val_loss: 23343.7090 - val_regression_loss: 11639.9678\n","Epoch 11/50\n","537/537 [==============================] - 0s 55us/step - loss: 250337.2107 - regression_loss: 124875.1628 - val_loss: 23792.7773 - val_regression_loss: 11867.0166\n","Epoch 12/50\n","537/537 [==============================] - 0s 55us/step - loss: 251795.9530 - regression_loss: 125639.1981 - val_loss: 20292.9844 - val_regression_loss: 10124.6768\n","Epoch 13/50\n","537/537 [==============================] - 0s 62us/step - loss: 220798.4702 - regression_loss: 110215.8676 - val_loss: 17410.6406 - val_regression_loss: 8688.0000\n","Epoch 14/50\n","537/537 [==============================] - 0s 48us/step - loss: 187370.9681 - regression_loss: 93546.0144 - val_loss: 16937.1074 - val_regression_loss: 8452.4160\n","Epoch 15/50\n","537/537 [==============================] - 0s 49us/step - loss: 177500.8001 - regression_loss: 88630.9421 - val_loss: 17831.5059 - val_regression_loss: 8899.3516\n","Epoch 16/50\n","537/537 [==============================] - 0s 51us/step - loss: 177561.4394 - regression_loss: 88660.7607 - val_loss: 18743.2324 - val_regression_loss: 9354.5967\n","Epoch 17/50\n","537/537 [==============================] - 0s 60us/step - loss: 186671.7917 - regression_loss: 93211.9553 - val_loss: 18759.7148 - val_regression_loss: 9362.3408\n","Epoch 18/50\n","537/537 [==============================] - 0s 54us/step - loss: 184429.5799 - regression_loss: 92088.8939 - val_loss: 17865.7559 - val_regression_loss: 8915.0859\n","Epoch 19/50\n","537/537 [==============================] - 0s 46us/step - loss: 170018.0557 - regression_loss: 84881.0602 - val_loss: 16480.9414 - val_regression_loss: 8222.5947\n","Epoch 20/50\n","537/537 [==============================] - 0s 48us/step - loss: 161634.3017 - regression_loss: 80688.6662 - val_loss: 15126.9609 - val_regression_loss: 7545.6758\n","Epoch 21/50\n","537/537 [==============================] - 0s 45us/step - loss: 153720.1351 - regression_loss: 76731.6856 - val_loss: 14320.1064 - val_regression_loss: 7142.3926\n","Epoch 22/50\n","537/537 [==============================] - 0s 48us/step - loss: 147853.5861 - regression_loss: 73799.8779 - val_loss: 14104.2920 - val_regression_loss: 7034.5981\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 143939.4604 - regression_loss: 71842.6201 - val_loss: 13976.6143 - val_regression_loss: 6970.7593\n","Epoch 24/50\n","537/537 [==============================] - 0s 44us/step - loss: 140497.3710 - regression_loss: 70121.5848 - val_loss: 13428.3232 - val_regression_loss: 6696.4458\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 133690.7453 - regression_loss: 66718.4724 - val_loss: 12608.2793 - val_regression_loss: 6286.1519\n","Epoch 26/50\n","537/537 [==============================] - 0s 52us/step - loss: 115887.2896 - regression_loss: 57812.9711 - val_loss: 11849.4990 - val_regression_loss: 5906.5015\n","Epoch 27/50\n","537/537 [==============================] - 0s 54us/step - loss: 119707.3991 - regression_loss: 59718.1867 - val_loss: 11207.3125 - val_regression_loss: 5585.3911\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 114837.5313 - regression_loss: 57281.9819 - val_loss: 10544.2236 - val_regression_loss: 5254.1489\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 107441.9893 - regression_loss: 53586.4992 - val_loss: 9926.9902 - val_regression_loss: 4946.1094\n","Epoch 30/50\n","537/537 [==============================] - 0s 45us/step - loss: 94793.9264 - regression_loss: 47267.1430 - val_loss: 9486.8027 - val_regression_loss: 4726.6763\n","Epoch 31/50\n","537/537 [==============================] - 0s 43us/step - loss: 89266.0485 - regression_loss: 44505.5055 - val_loss: 9160.0117 - val_regression_loss: 4563.8394\n","Epoch 32/50\n","537/537 [==============================] - 0s 49us/step - loss: 82603.5482 - regression_loss: 41179.0728 - val_loss: 8558.4053 - val_regression_loss: 4263.2388\n","Epoch 33/50\n","537/537 [==============================] - 0s 52us/step - loss: 71235.3323 - regression_loss: 35493.6649 - val_loss: 7601.0625 - val_regression_loss: 3784.3552\n","Epoch 34/50\n","537/537 [==============================] - 0s 48us/step - loss: 65891.0256 - regression_loss: 32821.2566 - val_loss: 6694.7671 - val_regression_loss: 3330.8552\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 58657.0016 - regression_loss: 29198.5643 - val_loss: 6035.2754 - val_regression_loss: 3001.0015\n","Epoch 36/50\n","537/537 [==============================] - 0s 56us/step - loss: 53688.3068 - regression_loss: 26711.4520 - val_loss: 5663.7461 - val_regression_loss: 2815.4709\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 48258.0738 - regression_loss: 23997.7901 - val_loss: 5336.5337 - val_regression_loss: 2652.0081\n","Epoch 38/50\n","537/537 [==============================] - 0s 57us/step - loss: 43476.9110 - regression_loss: 21610.8401 - val_loss: 4705.0425 - val_regression_loss: 2336.0674\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 39913.3383 - regression_loss: 19827.1830 - val_loss: 3951.7666 - val_regression_loss: 1959.0267\n","Epoch 40/50\n","537/537 [==============================] - 0s 49us/step - loss: 35845.4879 - regression_loss: 17791.2441 - val_loss: 3467.5559 - val_regression_loss: 1716.8119\n","Epoch 41/50\n","537/537 [==============================] - 0s 46us/step - loss: 23274.4440 - regression_loss: 11506.4396 - val_loss: 3128.1365 - val_regression_loss: 1547.1827\n","Epoch 42/50\n","537/537 [==============================] - 0s 50us/step - loss: 29022.0363 - regression_loss: 14380.8477 - val_loss: 3016.0327 - val_regression_loss: 1491.4712\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 20055.0523 - regression_loss: 9904.9541 - val_loss: 2773.1216 - val_regression_loss: 1370.0421\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 26498.6266 - regression_loss: 13124.7525 - val_loss: 2549.4785 - val_regression_loss: 1258.1926\n","Epoch 45/50\n","537/537 [==============================] - 0s 54us/step - loss: 24583.0360 - regression_loss: 12168.2698 - val_loss: 2266.7581 - val_regression_loss: 1116.6305\n","Epoch 46/50\n","537/537 [==============================] - 0s 54us/step - loss: 22735.8658 - regression_loss: 11242.4544 - val_loss: 2110.2000 - val_regression_loss: 1038.3197\n","Epoch 47/50\n","537/537 [==============================] - 0s 52us/step - loss: 21065.0552 - regression_loss: 10409.2385 - val_loss: 2048.6133 - val_regression_loss: 1007.6542\n","Epoch 48/50\n","537/537 [==============================] - 0s 52us/step - loss: 13436.5850 - regression_loss: 6592.7964 - val_loss: 1942.7852 - val_regression_loss: 954.7559\n","Epoch 49/50\n","537/537 [==============================] - 0s 55us/step - loss: 18295.3463 - regression_loss: 9023.7813 - val_loss: 1958.7430 - val_regression_loss: 962.9132\n","Epoch 50/50\n","537/537 [==============================] - 0s 60us/step - loss: 16878.9751 - regression_loss: 8319.4308 - val_loss: 1843.8738 - val_regression_loss: 905.4067\n","***************************** elapsed_time is:  3.06937313079834\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 41345.2882 - regression_loss: 20520.5140 - val_loss: 4473.6294 - val_regression_loss: 2215.7776\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 33807.5536 - regression_loss: 16748.6517 - val_loss: 3397.5569 - val_regression_loss: 1676.9971\n","Epoch 3/50\n","537/537 [==============================] - 0s 44us/step - loss: 25376.4821 - regression_loss: 12527.1850 - val_loss: 2221.5110 - val_regression_loss: 1087.5879\n","Epoch 4/50\n","537/537 [==============================] - 0s 43us/step - loss: 16083.7482 - regression_loss: 7870.4124 - val_loss: 1488.7266 - val_regression_loss: 719.1947\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 10656.1647 - regression_loss: 5140.8865 - val_loss: 1480.1692 - val_regression_loss: 713.9973\n","Epoch 6/50\n","537/537 [==============================] - 0s 48us/step - loss: 11174.8073 - regression_loss: 5393.4957 - val_loss: 1009.6558 - val_regression_loss: 480.1325\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 7483.8131 - regression_loss: 3559.5981 - val_loss: 476.0728 - val_regression_loss: 215.4776\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 3496.9824 - regression_loss: 1584.0547 - val_loss: 352.4673 - val_regression_loss: 155.6365\n","Epoch 9/50\n","537/537 [==============================] - 0s 45us/step - loss: 2800.6064 - regression_loss: 1252.2348 - val_loss: 509.6761 - val_regression_loss: 235.6676\n","Epoch 10/50\n","537/537 [==============================] - 0s 50us/step - loss: 4344.2631 - regression_loss: 2035.7603 - val_loss: 590.5872 - val_regression_loss: 276.9710\n","Epoch 11/50\n","537/537 [==============================] - 0s 61us/step - loss: 5138.1684 - regression_loss: 2439.8223 - val_loss: 501.1081 - val_regression_loss: 232.6583\n","Epoch 12/50\n","537/537 [==============================] - 0s 54us/step - loss: 4345.3370 - regression_loss: 2046.0846 - val_loss: 354.4905 - val_regression_loss: 159.4832\n","Epoch 13/50\n","537/537 [==============================] - 0s 49us/step - loss: 3168.8263 - regression_loss: 1458.5618 - val_loss: 259.8483 - val_regression_loss: 112.0581\n","Epoch 14/50\n","537/537 [==============================] - 0s 52us/step - loss: 2252.8755 - regression_loss: 999.6173 - val_loss: 254.9872 - val_regression_loss: 109.3515\n","Epoch 15/50\n","537/537 [==============================] - 0s 48us/step - loss: 2186.3144 - regression_loss: 962.5840 - val_loss: 293.7650 - val_regression_loss: 128.4265\n","Epoch 16/50\n","537/537 [==============================] - 0s 43us/step - loss: 2284.2976 - regression_loss: 1010.6430 - val_loss: 316.6596 - val_regression_loss: 139.6810\n","Epoch 17/50\n","537/537 [==============================] - 0s 49us/step - loss: 2427.9709 - regression_loss: 1079.9083 - val_loss: 313.8067 - val_regression_loss: 138.2803\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 2316.8464 - regression_loss: 1027.3221 - val_loss: 301.1906 - val_regression_loss: 132.1740\n","Epoch 19/50\n","537/537 [==============================] - 0s 44us/step - loss: 2153.0546 - regression_loss: 945.6348 - val_loss: 278.7906 - val_regression_loss: 121.2722\n","Epoch 20/50\n","537/537 [==============================] - 0s 57us/step - loss: 2043.0760 - regression_loss: 892.1827 - val_loss: 246.1666 - val_regression_loss: 105.2863\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 1831.0123 - regression_loss: 788.5559 - val_loss: 219.3612 - val_regression_loss: 92.1934\n","Epoch 22/50\n","537/537 [==============================] - 0s 41us/step - loss: 1769.4255 - regression_loss: 758.5470 - val_loss: 211.0025 - val_regression_loss: 88.2562\n","Epoch 23/50\n","537/537 [==============================] - 0s 45us/step - loss: 1782.9481 - regression_loss: 771.0444 - val_loss: 212.0013 - val_regression_loss: 88.8623\n","Epoch 24/50\n","537/537 [==============================] - 0s 41us/step - loss: 1852.2363 - regression_loss: 803.5234 - val_loss: 211.2253 - val_regression_loss: 88.4237\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 1796.2783 - regression_loss: 776.6244 - val_loss: 211.8416 - val_regression_loss: 88.5899\n","Epoch 26/50\n","537/537 [==============================] - 0s 48us/step - loss: 1741.4864 - regression_loss: 748.2464 - val_loss: 215.8699 - val_regression_loss: 90.4238\n","Epoch 27/50\n","537/537 [==============================] - 0s 45us/step - loss: 1684.6787 - regression_loss: 720.0203 - val_loss: 219.6190 - val_regression_loss: 92.1193\n","Epoch 28/50\n","537/537 [==============================] - 0s 50us/step - loss: 1665.4586 - regression_loss: 707.8384 - val_loss: 222.0643 - val_regression_loss: 93.1904\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 1644.6486 - regression_loss: 697.1434 - val_loss: 225.3762 - val_regression_loss: 94.7468\n","Epoch 30/50\n","537/537 [==============================] - 0s 47us/step - loss: 1632.4673 - regression_loss: 688.9415 - val_loss: 226.4388 - val_regression_loss: 95.2534\n","Epoch 31/50\n","537/537 [==============================] - 0s 52us/step - loss: 1619.2200 - regression_loss: 682.0247 - val_loss: 222.9963 - val_regression_loss: 93.5878\n","Epoch 32/50\n","537/537 [==============================] - 0s 46us/step - loss: 1582.0230 - regression_loss: 665.7508 - val_loss: 219.3109 - val_regression_loss: 91.8495\n","Epoch 33/50\n","537/537 [==============================] - 0s 52us/step - loss: 1535.0855 - regression_loss: 643.0480 - val_loss: 215.3200 - val_regression_loss: 89.9651\n","Epoch 34/50\n","537/537 [==============================] - 0s 58us/step - loss: 1560.8528 - regression_loss: 655.8766 - val_loss: 210.6381 - val_regression_loss: 87.7211\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 1536.3434 - regression_loss: 645.2387 - val_loss: 207.3772 - val_regression_loss: 86.1405\n","Epoch 36/50\n","537/537 [==============================] - 0s 48us/step - loss: 1527.5785 - regression_loss: 640.8879 - val_loss: 206.7738 - val_regression_loss: 85.8498\n","Epoch 37/50\n","537/537 [==============================] - 0s 54us/step - loss: 1512.7783 - regression_loss: 634.4251 - val_loss: 208.0416 - val_regression_loss: 86.4608\n","Epoch 38/50\n","537/537 [==============================] - 0s 51us/step - loss: 1521.2375 - regression_loss: 637.4466 - val_loss: 209.7315 - val_regression_loss: 87.2690\n","Epoch 39/50\n","537/537 [==============================] - 0s 45us/step - loss: 1491.3762 - regression_loss: 623.0073 - val_loss: 211.6464 - val_regression_loss: 88.1850\n","Epoch 40/50\n","537/537 [==============================] - 0s 47us/step - loss: 1500.4081 - regression_loss: 627.4490 - val_loss: 210.5117 - val_regression_loss: 87.5837\n","Epoch 41/50\n","537/537 [==============================] - 0s 50us/step - loss: 1466.3041 - regression_loss: 609.5433 - val_loss: 208.4271 - val_regression_loss: 86.5313\n","Epoch 42/50\n","537/537 [==============================] - 0s 48us/step - loss: 1457.8182 - regression_loss: 606.6235 - val_loss: 206.2857 - val_regression_loss: 85.4736\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 1416.7070 - regression_loss: 587.1220 - val_loss: 203.7996 - val_regression_loss: 84.2691\n","Epoch 44/50\n","537/537 [==============================] - 0s 47us/step - loss: 1414.8274 - regression_loss: 585.0887 - val_loss: 202.9746 - val_regression_loss: 83.9000\n","Epoch 45/50\n","537/537 [==============================] - 0s 49us/step - loss: 1403.0525 - regression_loss: 580.4033 - val_loss: 202.4443 - val_regression_loss: 83.6553\n","Epoch 46/50\n","537/537 [==============================] - 0s 50us/step - loss: 1401.1380 - regression_loss: 579.7066 - val_loss: 201.8456 - val_regression_loss: 83.3561\n","Epoch 47/50\n","537/537 [==============================] - 0s 47us/step - loss: 1393.7670 - regression_loss: 575.2114 - val_loss: 201.8980 - val_regression_loss: 83.3827\n","Epoch 48/50\n","537/537 [==============================] - 0s 47us/step - loss: 1362.9545 - regression_loss: 560.4660 - val_loss: 200.8055 - val_regression_loss: 82.8419\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 1380.4309 - regression_loss: 570.5335 - val_loss: 200.8277 - val_regression_loss: 82.8420\n","Epoch 50/50\n","537/537 [==============================] - 0s 57us/step - loss: 1389.5583 - regression_loss: 573.9096 - val_loss: 201.2661 - val_regression_loss: 83.0440\n","***************************** elapsed_time is:  3.0508768558502197\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 76945.7825 - regression_loss: 38100.6177 - val_loss: 7232.9160 - val_regression_loss: 3576.5615\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 64379.4118 - regression_loss: 31856.5318 - val_loss: 5656.1426 - val_regression_loss: 2794.0288\n","Epoch 3/50\n","537/537 [==============================] - 0s 50us/step - loss: 49849.2074 - regression_loss: 24638.6982 - val_loss: 3795.9895 - val_regression_loss: 1872.3824\n","Epoch 4/50\n","537/537 [==============================] - 0s 51us/step - loss: 33055.5421 - regression_loss: 16306.2433 - val_loss: 2115.6772 - val_regression_loss: 1043.2734\n","Epoch 5/50\n","537/537 [==============================] - 0s 49us/step - loss: 17117.9236 - regression_loss: 8431.2469 - val_loss: 1746.8436 - val_regression_loss: 870.4659\n","Epoch 6/50\n","537/537 [==============================] - 0s 46us/step - loss: 11556.2538 - regression_loss: 5744.7212 - val_loss: 2271.2671 - val_regression_loss: 1136.5652\n","Epoch 7/50\n","537/537 [==============================] - 0s 50us/step - loss: 14540.6137 - regression_loss: 7264.5749 - val_loss: 1678.8397 - val_regression_loss: 835.1337\n","Epoch 8/50\n","537/537 [==============================] - 0s 45us/step - loss: 11050.5463 - regression_loss: 5473.3842 - val_loss: 808.9600 - val_regression_loss: 392.1756\n","Epoch 9/50\n","537/537 [==============================] - 0s 49us/step - loss: 5971.6327 - regression_loss: 2862.6691 - val_loss: 443.0135 - val_regression_loss: 202.6944\n","Epoch 10/50\n","537/537 [==============================] - 0s 50us/step - loss: 4639.4545 - regression_loss: 2143.8703 - val_loss: 508.7970 - val_regression_loss: 232.0905\n","Epoch 11/50\n","537/537 [==============================] - 0s 52us/step - loss: 6018.6318 - regression_loss: 2807.1928 - val_loss: 628.1132 - val_regression_loss: 291.0708\n","Epoch 12/50\n","537/537 [==============================] - 0s 47us/step - loss: 6972.0809 - regression_loss: 3281.6565 - val_loss: 609.8211 - val_regression_loss: 283.1837\n","Epoch 13/50\n","537/537 [==============================] - 0s 58us/step - loss: 6426.0172 - regression_loss: 3025.5474 - val_loss: 488.8160 - val_regression_loss: 225.0594\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 4928.6422 - regression_loss: 2298.0494 - val_loss: 388.5677 - val_regression_loss: 177.7730\n","Epoch 15/50\n","537/537 [==============================] - 0s 47us/step - loss: 3358.9182 - regression_loss: 1538.4619 - val_loss: 398.5404 - val_regression_loss: 185.4578\n","Epoch 16/50\n","537/537 [==============================] - 0s 52us/step - loss: 2872.5607 - regression_loss: 1319.6884 - val_loss: 495.7329 - val_regression_loss: 236.1063\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 3227.0888 - regression_loss: 1513.9365 - val_loss: 565.3489 - val_regression_loss: 271.9371\n","Epoch 18/50\n","537/537 [==============================] - 0s 49us/step - loss: 3723.3599 - regression_loss: 1774.3691 - val_loss: 534.0988 - val_regression_loss: 256.2063\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 3463.2590 - regression_loss: 1643.8048 - val_loss: 451.4774 - val_regression_loss: 213.8871\n","Epoch 20/50\n","537/537 [==============================] - 0s 47us/step - loss: 2913.4934 - regression_loss: 1362.7102 - val_loss: 390.9677 - val_regression_loss: 182.1441\n","Epoch 21/50\n","537/537 [==============================] - 0s 46us/step - loss: 2496.5936 - regression_loss: 1145.1876 - val_loss: 368.7008 - val_regression_loss: 169.4642\n","Epoch 22/50\n","537/537 [==============================] - 0s 47us/step - loss: 2582.8987 - regression_loss: 1174.4671 - val_loss: 355.6193 - val_regression_loss: 161.6742\n","Epoch 23/50\n","537/537 [==============================] - 0s 49us/step - loss: 2580.2196 - regression_loss: 1162.4198 - val_loss: 333.9109 - val_regression_loss: 149.9764\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 2463.9866 - regression_loss: 1098.5667 - val_loss: 314.4444 - val_regression_loss: 139.8050\n","Epoch 25/50\n","537/537 [==============================] - 0s 53us/step - loss: 2332.6703 - regression_loss: 1027.6273 - val_loss: 310.9963 - val_regression_loss: 138.0894\n","Epoch 26/50\n","537/537 [==============================] - 0s 48us/step - loss: 2205.6537 - regression_loss: 964.8185 - val_loss: 316.5329 - val_regression_loss: 141.3233\n","Epoch 27/50\n","537/537 [==============================] - 0s 45us/step - loss: 2154.0098 - regression_loss: 943.4166 - val_loss: 315.8066 - val_regression_loss: 141.7141\n","Epoch 28/50\n","537/537 [==============================] - 0s 47us/step - loss: 2066.0459 - regression_loss: 905.5019 - val_loss: 306.5395 - val_regression_loss: 137.7995\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 1954.4838 - regression_loss: 855.9006 - val_loss: 297.4854 - val_regression_loss: 133.6968\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1881.9762 - regression_loss: 825.0378 - val_loss: 291.9641 - val_regression_loss: 131.0184\n","Epoch 31/50\n","537/537 [==============================] - 0s 45us/step - loss: 1826.1428 - regression_loss: 798.5306 - val_loss: 284.7430 - val_regression_loss: 127.2582\n","Epoch 32/50\n","537/537 [==============================] - 0s 50us/step - loss: 1836.6495 - regression_loss: 803.8510 - val_loss: 272.5648 - val_regression_loss: 120.9407\n","Epoch 33/50\n","537/537 [==============================] - 0s 53us/step - loss: 1775.8866 - regression_loss: 775.3091 - val_loss: 259.7917 - val_regression_loss: 114.2349\n","Epoch 34/50\n","537/537 [==============================] - 0s 51us/step - loss: 1709.6901 - regression_loss: 735.8612 - val_loss: 252.1437 - val_regression_loss: 110.0473\n","Epoch 35/50\n","537/537 [==============================] - 0s 42us/step - loss: 1643.4045 - regression_loss: 698.5319 - val_loss: 246.5415 - val_regression_loss: 106.8849\n","Epoch 36/50\n","537/537 [==============================] - 0s 49us/step - loss: 1650.7343 - regression_loss: 702.9756 - val_loss: 239.4724 - val_regression_loss: 103.0592\n","Epoch 37/50\n","537/537 [==============================] - 0s 57us/step - loss: 1625.2608 - regression_loss: 687.5110 - val_loss: 231.3239 - val_regression_loss: 98.8620\n","Epoch 38/50\n","537/537 [==============================] - 0s 50us/step - loss: 1613.7265 - regression_loss: 678.1095 - val_loss: 225.9787 - val_regression_loss: 96.2811\n","Epoch 39/50\n","537/537 [==============================] - 0s 45us/step - loss: 1566.6376 - regression_loss: 656.3353 - val_loss: 223.4413 - val_regression_loss: 95.2800\n","Epoch 40/50\n","537/537 [==============================] - 0s 51us/step - loss: 1543.4088 - regression_loss: 648.2242 - val_loss: 222.0942 - val_regression_loss: 94.9626\n","Epoch 41/50\n","537/537 [==============================] - 0s 45us/step - loss: 1520.1334 - regression_loss: 638.9843 - val_loss: 220.8077 - val_regression_loss: 94.6073\n","Epoch 42/50\n","537/537 [==============================] - 0s 41us/step - loss: 1536.3017 - regression_loss: 648.4405 - val_loss: 218.5680 - val_regression_loss: 93.6196\n","Epoch 43/50\n","537/537 [==============================] - 0s 50us/step - loss: 1508.6007 - regression_loss: 636.7029 - val_loss: 213.2315 - val_regression_loss: 90.8352\n","Epoch 44/50\n","537/537 [==============================] - 0s 48us/step - loss: 1495.5425 - regression_loss: 631.1086 - val_loss: 206.9168 - val_regression_loss: 87.4408\n","Epoch 45/50\n","537/537 [==============================] - 0s 50us/step - loss: 1465.2685 - regression_loss: 611.2467 - val_loss: 201.4078 - val_regression_loss: 84.4645\n","Epoch 46/50\n","537/537 [==============================] - 0s 48us/step - loss: 1441.9871 - regression_loss: 597.3149 - val_loss: 197.6123 - val_regression_loss: 82.4429\n","Epoch 47/50\n","537/537 [==============================] - 0s 45us/step - loss: 1427.6864 - regression_loss: 589.5659 - val_loss: 194.8402 - val_regression_loss: 81.0883\n","Epoch 48/50\n","537/537 [==============================] - 0s 51us/step - loss: 1407.1577 - regression_loss: 579.2450 - val_loss: 193.4554 - val_regression_loss: 80.5642\n","Epoch 49/50\n","537/537 [==============================] - 0s 53us/step - loss: 1411.0956 - regression_loss: 583.9673 - val_loss: 192.5494 - val_regression_loss: 80.3032\n","Epoch 50/50\n","537/537 [==============================] - 0s 56us/step - loss: 1405.4525 - regression_loss: 580.9904 - val_loss: 191.0118 - val_regression_loss: 79.6532\n","***************************** elapsed_time is:  3.3480143547058105\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 84002.5808 - regression_loss: 41671.9882 - val_loss: 8266.7275 - val_regression_loss: 4093.7810\n","Epoch 2/50\n","537/537 [==============================] - 0s 47us/step - loss: 71868.3115 - regression_loss: 35621.0784 - val_loss: 6563.7607 - val_regression_loss: 3245.3940\n","Epoch 3/50\n","537/537 [==============================] - 0s 46us/step - loss: 57500.8495 - regression_loss: 28464.5982 - val_loss: 4512.8486 - val_regression_loss: 2224.1470\n","Epoch 4/50\n","537/537 [==============================] - 0s 44us/step - loss: 39736.6497 - regression_loss: 19614.5789 - val_loss: 2550.8792 - val_regression_loss: 1249.1433\n","Epoch 5/50\n","537/537 [==============================] - 0s 47us/step - loss: 21514.6263 - regression_loss: 10550.9432 - val_loss: 1714.4771 - val_regression_loss: 838.7848\n","Epoch 6/50\n","537/537 [==============================] - 0s 45us/step - loss: 12909.4928 - regression_loss: 6314.5115 - val_loss: 2280.9526 - val_regression_loss: 1127.9395\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 16355.6211 - regression_loss: 8085.8140 - val_loss: 1997.6465 - val_regression_loss: 985.7780\n","Epoch 8/50\n","537/537 [==============================] - 0s 58us/step - loss: 14149.9762 - regression_loss: 6979.9004 - val_loss: 1131.7373 - val_regression_loss: 549.2358\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 7720.5175 - regression_loss: 3736.4767 - val_loss: 632.1475 - val_regression_loss: 295.4637\n","Epoch 10/50\n","537/537 [==============================] - 0s 50us/step - loss: 4430.8630 - regression_loss: 2061.3659 - val_loss: 637.9553 - val_regression_loss: 295.4102\n","Epoch 11/50\n","537/537 [==============================] - 0s 48us/step - loss: 5019.2498 - regression_loss: 2333.6224 - val_loss: 815.7870 - val_regression_loss: 382.9659\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 6936.0342 - regression_loss: 3281.4991 - val_loss: 875.6304 - val_regression_loss: 413.0617\n","Epoch 13/50\n","537/537 [==============================] - 0s 47us/step - loss: 7523.8526 - regression_loss: 3576.0191 - val_loss: 772.4011 - val_regression_loss: 362.7045\n","Epoch 14/50\n","537/537 [==============================] - 0s 41us/step - loss: 6561.5438 - regression_loss: 3106.6381 - val_loss: 597.4606 - val_regression_loss: 277.1085\n","Epoch 15/50\n","537/537 [==============================] - 0s 50us/step - loss: 4954.7796 - regression_loss: 2321.2467 - val_loss: 463.9344 - val_regression_loss: 212.4302\n","\n","Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 3617.5891 - regression_loss: 1665.9325 - val_loss: 435.2461 - val_regression_loss: 199.0825\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 3386.7985 - regression_loss: 1558.9926 - val_loss: 431.1017 - val_regression_loss: 197.8948\n","Epoch 18/50\n","537/537 [==============================] - 0s 51us/step - loss: 3200.7137 - regression_loss: 1473.5271 - val_loss: 441.1516 - val_regression_loss: 203.6508\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 3308.7085 - regression_loss: 1533.6631 - val_loss: 451.0939 - val_regression_loss: 209.1481\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 3392.3596 - regression_loss: 1580.6096 - val_loss: 450.0973 - val_regression_loss: 208.9554\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 3352.7496 - regression_loss: 1564.1113 - val_loss: 436.1852 - val_regression_loss: 202.0841\n","Epoch 22/50\n","537/537 [==============================] - 0s 44us/step - loss: 3240.4935 - regression_loss: 1506.5816 - val_loss: 413.8360 - val_regression_loss: 190.8055\n","Epoch 23/50\n","537/537 [==============================] - 0s 52us/step - loss: 3155.1252 - regression_loss: 1463.4139 - val_loss: 390.7420 - val_regression_loss: 179.0130\n","Epoch 24/50\n","537/537 [==============================] - 0s 50us/step - loss: 3051.1050 - regression_loss: 1409.4989 - val_loss: 371.7552 - val_regression_loss: 169.1803\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 2888.9370 - regression_loss: 1327.1218 - val_loss: 358.9068 - val_regression_loss: 162.3872\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 2924.2655 - regression_loss: 1342.8741 - val_loss: 350.6018 - val_regression_loss: 157.8811\n","Epoch 27/50\n","537/537 [==============================] - 0s 51us/step - loss: 2883.4895 - regression_loss: 1317.4894 - val_loss: 344.7304 - val_regression_loss: 154.6397\n","Epoch 28/50\n","537/537 [==============================] - 0s 45us/step - loss: 2821.8852 - regression_loss: 1285.0027 - val_loss: 339.9742 - val_regression_loss: 152.0270\n","Epoch 29/50\n","537/537 [==============================] - 0s 47us/step - loss: 2797.4513 - regression_loss: 1272.5724 - val_loss: 336.2313 - val_regression_loss: 149.9962\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 2532.9108 - regression_loss: 1137.4086 - val_loss: 333.6688 - val_regression_loss: 148.6395\n","Epoch 31/50\n","537/537 [==============================] - 0s 47us/step - loss: 2654.4475 - regression_loss: 1195.0856 - val_loss: 331.7147 - val_regression_loss: 147.6587\n","Epoch 32/50\n","537/537 [==============================] - 0s 42us/step - loss: 2597.4658 - regression_loss: 1170.1422 - val_loss: 329.3283 - val_regression_loss: 146.5147\n","Epoch 33/50\n","537/537 [==============================] - 0s 46us/step - loss: 2589.6394 - regression_loss: 1164.7444 - val_loss: 325.8169 - val_regression_loss: 144.8448\n","Epoch 34/50\n","537/537 [==============================] - 0s 46us/step - loss: 2494.5337 - regression_loss: 1117.9383 - val_loss: 321.0804 - val_regression_loss: 142.5795\n","Epoch 35/50\n","537/537 [==============================] - 0s 50us/step - loss: 2448.0115 - regression_loss: 1095.6312 - val_loss: 316.0432 - val_regression_loss: 140.1617\n","Epoch 36/50\n","537/537 [==============================] - 0s 50us/step - loss: 2468.4754 - regression_loss: 1108.8946 - val_loss: 311.8256 - val_regression_loss: 138.1409\n","Epoch 37/50\n","537/537 [==============================] - 0s 57us/step - loss: 2388.6898 - regression_loss: 1070.4165 - val_loss: 308.8939 - val_regression_loss: 136.7388\n","Epoch 38/50\n","537/537 [==============================] - 0s 58us/step - loss: 2335.9608 - regression_loss: 1042.8997 - val_loss: 307.0448 - val_regression_loss: 135.8657\n","Epoch 39/50\n","537/537 [==============================] - 0s 48us/step - loss: 2325.9623 - regression_loss: 1037.8729 - val_loss: 305.7737 - val_regression_loss: 135.2664\n","Epoch 40/50\n","537/537 [==============================] - 0s 56us/step - loss: 2308.4876 - regression_loss: 1028.9552 - val_loss: 304.7709 - val_regression_loss: 134.7876\n","Epoch 41/50\n","537/537 [==============================] - 0s 47us/step - loss: 2289.7885 - regression_loss: 1020.4071 - val_loss: 303.8513 - val_regression_loss: 134.3409\n","Epoch 42/50\n","537/537 [==============================] - 0s 46us/step - loss: 2259.8338 - regression_loss: 1008.0610 - val_loss: 302.8950 - val_regression_loss: 133.8521\n","Epoch 43/50\n","537/537 [==============================] - 0s 48us/step - loss: 2193.4293 - regression_loss: 972.4338 - val_loss: 302.0614 - val_regression_loss: 133.4093\n","Epoch 44/50\n","537/537 [==============================] - 0s 45us/step - loss: 2187.6438 - regression_loss: 969.6375 - val_loss: 301.2662 - val_regression_loss: 132.9779\n","Epoch 45/50\n","537/537 [==============================] - 0s 50us/step - loss: 2121.8970 - regression_loss: 935.9604 - val_loss: 300.3746 - val_regression_loss: 132.4842\n","Epoch 46/50\n","537/537 [==============================] - 0s 51us/step - loss: 2111.1318 - regression_loss: 928.9937 - val_loss: 299.3864 - val_regression_loss: 131.9370\n","Epoch 47/50\n","537/537 [==============================] - 0s 49us/step - loss: 2116.5513 - regression_loss: 933.3562 - val_loss: 298.6815 - val_regression_loss: 131.5445\n","Epoch 48/50\n","537/537 [==============================] - 0s 48us/step - loss: 1998.2943 - regression_loss: 874.4436 - val_loss: 298.1037 - val_regression_loss: 131.2236\n","Epoch 49/50\n","537/537 [==============================] - 0s 53us/step - loss: 2081.3998 - regression_loss: 916.0033 - val_loss: 297.5452 - val_regression_loss: 130.9541\n","Epoch 50/50\n","537/537 [==============================] - 0s 48us/step - loss: 2038.9260 - regression_loss: 892.0990 - val_loss: 296.9813 - val_regression_loss: 130.7150\n","***************************** elapsed_time is:  3.1092159748077393\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 65389.5217 - regression_loss: 32379.3060 - val_loss: 6527.4502 - val_regression_loss: 3228.3076\n","Epoch 2/50\n","537/537 [==============================] - 0s 49us/step - loss: 55376.9256 - regression_loss: 27416.6706 - val_loss: 4990.9390 - val_regression_loss: 2466.6147\n","Epoch 3/50\n","537/537 [==============================] - 0s 44us/step - loss: 41508.0891 - regression_loss: 20532.8681 - val_loss: 3163.2231 - val_regression_loss: 1560.7432\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 26711.8942 - regression_loss: 13201.5579 - val_loss: 1569.8590 - val_regression_loss: 772.8097\n","Epoch 5/50\n","537/537 [==============================] - 0s 52us/step - loss: 12889.5982 - regression_loss: 6359.3044 - val_loss: 1264.4268 - val_regression_loss: 627.2573\n","Epoch 6/50\n","537/537 [==============================] - 0s 59us/step - loss: 9949.3925 - regression_loss: 4946.1072 - val_loss: 1532.0104 - val_regression_loss: 759.9323\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 11858.4796 - regression_loss: 5883.6187 - val_loss: 1084.7024 - val_regression_loss: 529.0789\n","Epoch 8/50\n","537/537 [==============================] - 0s 49us/step - loss: 8827.4856 - regression_loss: 4301.7888 - val_loss: 596.6730 - val_regression_loss: 278.5582\n","Epoch 9/50\n","537/537 [==============================] - 0s 44us/step - loss: 5586.6958 - regression_loss: 2624.6045 - val_loss: 522.2073 - val_regression_loss: 237.9466\n","Epoch 10/50\n","537/537 [==============================] - 0s 45us/step - loss: 5094.0265 - regression_loss: 2349.6897 - val_loss: 686.7845 - val_regression_loss: 319.3691\n","Epoch 11/50\n","537/537 [==============================] - 0s 44us/step - loss: 6358.5795 - regression_loss: 2981.1576 - val_loss: 742.0634 - val_regression_loss: 347.9173\n","Epoch 12/50\n","537/537 [==============================] - 0s 43us/step - loss: 6634.7189 - regression_loss: 3134.1132 - val_loss: 619.5546 - val_regression_loss: 288.6961\n","Epoch 13/50\n","537/537 [==============================] - 0s 48us/step - loss: 5434.5811 - regression_loss: 2554.4382 - val_loss: 446.9868 - val_regression_loss: 204.9793\n","Epoch 14/50\n","537/537 [==============================] - 0s 55us/step - loss: 3966.1419 - regression_loss: 1841.7895 - val_loss: 363.4786 - val_regression_loss: 165.7431\n","Epoch 15/50\n","537/537 [==============================] - 0s 48us/step - loss: 3249.9133 - regression_loss: 1504.9565 - val_loss: 400.9736 - val_regression_loss: 186.4367\n","Epoch 16/50\n","537/537 [==============================] - 0s 53us/step - loss: 3563.7658 - regression_loss: 1678.0784 - val_loss: 463.1451 - val_regression_loss: 218.4595\n","Epoch 17/50\n","537/537 [==============================] - 0s 52us/step - loss: 3981.6232 - regression_loss: 1896.6051 - val_loss: 451.3454 - val_regression_loss: 212.4020\n","Epoch 18/50\n","537/537 [==============================] - 0s 53us/step - loss: 3840.4838 - regression_loss: 1822.8987 - val_loss: 382.8694 - val_regression_loss: 177.1439\n","Epoch 19/50\n","537/537 [==============================] - 0s 45us/step - loss: 3261.7070 - regression_loss: 1524.7912 - val_loss: 336.2862 - val_regression_loss: 152.3974\n","Epoch 20/50\n","537/537 [==============================] - 0s 60us/step - loss: 2886.6309 - regression_loss: 1326.9767 - val_loss: 337.7686 - val_regression_loss: 151.6922\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 2800.3283 - regression_loss: 1273.3425 - val_loss: 355.1381 - val_regression_loss: 159.2935\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 2975.9313 - regression_loss: 1350.3304 - val_loss: 350.8188 - val_regression_loss: 156.5963\n","Epoch 23/50\n","537/537 [==============================] - 0s 55us/step - loss: 2850.9170 - regression_loss: 1283.3158 - val_loss: 325.1524 - val_regression_loss: 143.7729\n","Epoch 24/50\n","537/537 [==============================] - 0s 52us/step - loss: 2707.2298 - regression_loss: 1214.3222 - val_loss: 301.7630 - val_regression_loss: 132.5206\n","Epoch 25/50\n","537/537 [==============================] - 0s 47us/step - loss: 2526.1559 - regression_loss: 1124.2280 - val_loss: 293.4958 - val_regression_loss: 129.1080\n","Epoch 26/50\n","537/537 [==============================] - 0s 56us/step - loss: 2387.6736 - regression_loss: 1062.6497 - val_loss: 291.2587 - val_regression_loss: 128.7031\n","Epoch 27/50\n","537/537 [==============================] - 0s 52us/step - loss: 2406.9621 - regression_loss: 1076.8433 - val_loss: 283.6663 - val_regression_loss: 125.4135\n","Epoch 28/50\n","537/537 [==============================] - 0s 55us/step - loss: 2356.6957 - regression_loss: 1059.9324 - val_loss: 273.4039 - val_regression_loss: 120.4445\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 2227.6708 - regression_loss: 998.6393 - val_loss: 267.6187 - val_regression_loss: 117.4180\n","Epoch 30/50\n","537/537 [==============================] - 0s 55us/step - loss: 2190.5702 - regression_loss: 976.2573 - val_loss: 265.2996 - val_regression_loss: 115.9644\n","Epoch 31/50\n","537/537 [==============================] - 0s 51us/step - loss: 2183.9674 - regression_loss: 973.1903 - val_loss: 261.1761 - val_regression_loss: 113.6051\n","Epoch 32/50\n","537/537 [==============================] - 0s 50us/step - loss: 1983.5124 - regression_loss: 870.9378 - val_loss: 254.2063 - val_regression_loss: 109.8853\n","Epoch 33/50\n","537/537 [==============================] - 0s 48us/step - loss: 1939.9119 - regression_loss: 845.1875 - val_loss: 250.1344 - val_regression_loss: 107.7136\n","Epoch 34/50\n","537/537 [==============================] - 0s 52us/step - loss: 1983.5984 - regression_loss: 866.5038 - val_loss: 250.4337 - val_regression_loss: 107.7869\n","Epoch 35/50\n","537/537 [==============================] - 0s 49us/step - loss: 1957.5614 - regression_loss: 851.2619 - val_loss: 249.2975 - val_regression_loss: 107.1875\n","Epoch 36/50\n","537/537 [==============================] - 0s 51us/step - loss: 1919.1083 - regression_loss: 832.7131 - val_loss: 243.7698 - val_regression_loss: 104.4158\n","Epoch 37/50\n","537/537 [==============================] - 0s 47us/step - loss: 1905.2528 - regression_loss: 826.1608 - val_loss: 239.1479 - val_regression_loss: 102.1721\n","Epoch 38/50\n","537/537 [==============================] - 0s 42us/step - loss: 1866.4980 - regression_loss: 808.0662 - val_loss: 236.5909 - val_regression_loss: 101.0215\n","Epoch 39/50\n","537/537 [==============================] - 0s 53us/step - loss: 1813.9018 - regression_loss: 782.3957 - val_loss: 235.6408 - val_regression_loss: 100.7406\n","Epoch 40/50\n","537/537 [==============================] - 0s 53us/step - loss: 1803.4420 - regression_loss: 779.2917 - val_loss: 236.1627 - val_regression_loss: 101.2040\n","Epoch 41/50\n","537/537 [==============================] - 0s 47us/step - loss: 1811.7996 - regression_loss: 784.2476 - val_loss: 237.4855 - val_regression_loss: 101.9792\n","Epoch 42/50\n","537/537 [==============================] - 0s 51us/step - loss: 1798.8087 - regression_loss: 776.9066 - val_loss: 237.7833 - val_regression_loss: 102.1009\n","Epoch 43/50\n","537/537 [==============================] - 0s 49us/step - loss: 1744.6117 - regression_loss: 751.0928 - val_loss: 236.2009 - val_regression_loss: 101.1776\n","Epoch 44/50\n","537/537 [==============================] - 0s 46us/step - loss: 1721.7134 - regression_loss: 739.0670 - val_loss: 233.2642 - val_regression_loss: 99.5347\n","Epoch 45/50\n","537/537 [==============================] - 0s 51us/step - loss: 1641.5665 - regression_loss: 695.4260 - val_loss: 230.7557 - val_regression_loss: 98.1828\n","Epoch 46/50\n","537/537 [==============================] - 0s 44us/step - loss: 1739.8608 - regression_loss: 745.1942 - val_loss: 231.2517 - val_regression_loss: 98.5190\n","Epoch 47/50\n","537/537 [==============================] - 0s 50us/step - loss: 1693.0199 - regression_loss: 721.5125 - val_loss: 233.4368 - val_regression_loss: 99.7871\n","Epoch 48/50\n","537/537 [==============================] - 0s 47us/step - loss: 1656.1137 - regression_loss: 704.8675 - val_loss: 234.8670 - val_regression_loss: 100.6777\n","Epoch 49/50\n","537/537 [==============================] - 0s 49us/step - loss: 1698.3086 - regression_loss: 726.7585 - val_loss: 233.9032 - val_regression_loss: 100.2901\n","Epoch 50/50\n","537/537 [==============================] - 0s 51us/step - loss: 1661.9859 - regression_loss: 708.1008 - val_loss: 231.0859 - val_regression_loss: 98.8941\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","***************************** elapsed_time is:  3.12065052986145\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 576402.2103 - regression_loss: 287553.8375 - val_loss: 64532.4922 - val_regression_loss: 32192.3164\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 545578.0170 - regression_loss: 272243.2124 - val_loss: 59336.2266 - val_regression_loss: 29608.8262\n","Epoch 3/50\n","537/537 [==============================] - 0s 44us/step - loss: 493425.5205 - regression_loss: 246280.5597 - val_loss: 52125.2891 - val_regression_loss: 26022.1094\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 443874.8776 - regression_loss: 221660.2909 - val_loss: 42884.6875 - val_regression_loss: 21424.0938\n","Epoch 5/50\n","537/537 [==============================] - 0s 50us/step - loss: 359849.2300 - regression_loss: 179827.8530 - val_loss: 32359.4668 - val_regression_loss: 16187.5107\n","Epoch 6/50\n","537/537 [==============================] - 0s 41us/step - loss: 280076.9604 - regression_loss: 140134.9498 - val_loss: 22065.9688 - val_regression_loss: 11072.6650\n","Epoch 7/50\n","537/537 [==============================] - 0s 45us/step - loss: 197824.8045 - regression_loss: 99289.4331 - val_loss: 14342.4229 - val_regression_loss: 7248.5322\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 131350.7922 - regression_loss: 66375.0892 - val_loss: 12067.7148 - val_regression_loss: 6123.5298\n","Epoch 9/50\n","537/537 [==============================] - 0s 50us/step - loss: 118565.3221 - regression_loss: 60122.5739 - val_loss: 14387.1094 - val_regression_loss: 7188.2280\n","Epoch 10/50\n","537/537 [==============================] - 0s 47us/step - loss: 138354.1822 - regression_loss: 69054.5625 - val_loss: 14088.5352 - val_regression_loss: 6888.3901\n","Epoch 11/50\n","537/537 [==============================] - 0s 48us/step - loss: 137272.0251 - regression_loss: 66996.5115 - val_loss: 10831.5938 - val_regression_loss: 5212.8125\n","Epoch 12/50\n","537/537 [==============================] - 0s 47us/step - loss: 111220.6667 - regression_loss: 53442.7828 - val_loss: 8080.4756 - val_regression_loss: 3880.4790\n","Epoch 13/50\n","537/537 [==============================] - 0s 50us/step - loss: 93375.2848 - regression_loss: 44796.0836 - val_loss: 7229.6699 - val_regression_loss: 3516.9873\n","Epoch 14/50\n","537/537 [==============================] - 0s 44us/step - loss: 90061.2538 - regression_loss: 43853.3128 - val_loss: 7666.7827 - val_regression_loss: 3778.5278\n","Epoch 15/50\n","537/537 [==============================] - 0s 53us/step - loss: 95793.0582 - regression_loss: 47227.0150 - val_loss: 8098.8091 - val_regression_loss: 4018.7131\n","Epoch 16/50\n","537/537 [==============================] - 0s 50us/step - loss: 95257.7148 - regression_loss: 47264.1681 - val_loss: 7923.1172 - val_regression_loss: 3943.3960\n","Epoch 17/50\n","537/537 [==============================] - 0s 54us/step - loss: 90595.9243 - regression_loss: 45097.2795 - val_loss: 7275.5879 - val_regression_loss: 3625.5630\n","Epoch 18/50\n","537/537 [==============================] - 0s 57us/step - loss: 88532.6197 - regression_loss: 44141.4641 - val_loss: 6557.8735 - val_regression_loss: 3268.7571\n","Epoch 19/50\n","537/537 [==============================] - 0s 55us/step - loss: 80111.0758 - regression_loss: 39940.3733 - val_loss: 6208.4873 - val_regression_loss: 3094.2808\n","Epoch 20/50\n","537/537 [==============================] - 0s 50us/step - loss: 72371.8239 - regression_loss: 36083.1314 - val_loss: 6303.9390 - val_regression_loss: 3141.5535\n","Epoch 21/50\n","537/537 [==============================] - 0s 46us/step - loss: 68658.6647 - regression_loss: 34214.6526 - val_loss: 6486.4170 - val_regression_loss: 3232.6150\n","Epoch 22/50\n","537/537 [==============================] - 0s 51us/step - loss: 73341.8387 - regression_loss: 36557.3578 - val_loss: 6345.9399 - val_regression_loss: 3162.5403\n","Epoch 23/50\n","537/537 [==============================] - 0s 54us/step - loss: 70142.8009 - regression_loss: 34953.5302 - val_loss: 5835.3081 - val_regression_loss: 2907.3679\n","Epoch 24/50\n","537/537 [==============================] - 0s 52us/step - loss: 66863.1694 - regression_loss: 33313.1084 - val_loss: 5271.3789 - val_regression_loss: 2625.0564\n","Epoch 25/50\n","537/537 [==============================] - 0s 46us/step - loss: 62111.3100 - regression_loss: 30944.4402 - val_loss: 4924.3418 - val_regression_loss: 2450.6948\n","Epoch 26/50\n","537/537 [==============================] - 0s 51us/step - loss: 56995.3418 - regression_loss: 28375.8394 - val_loss: 4776.2944 - val_regression_loss: 2375.7849\n","Epoch 27/50\n","537/537 [==============================] - 0s 53us/step - loss: 56971.8873 - regression_loss: 28353.5571 - val_loss: 4632.5713 - val_regression_loss: 2303.6196\n","Epoch 28/50\n","537/537 [==============================] - 0s 49us/step - loss: 54105.1846 - regression_loss: 26918.4043 - val_loss: 4426.7109 - val_regression_loss: 2201.2075\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 48122.5242 - regression_loss: 23931.0866 - val_loss: 4281.2124 - val_regression_loss: 2129.7241\n","Epoch 30/50\n","537/537 [==============================] - 0s 48us/step - loss: 46668.0338 - regression_loss: 23208.3617 - val_loss: 4278.6577 - val_regression_loss: 2130.1348\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 43311.5613 - regression_loss: 21545.2606 - val_loss: 4236.7803 - val_regression_loss: 2110.4404\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 41287.1481 - regression_loss: 20546.5287 - val_loss: 3966.7644 - val_regression_loss: 1975.6711\n","Epoch 33/50\n","537/537 [==============================] - 0s 50us/step - loss: 35806.1652 - regression_loss: 17802.3540 - val_loss: 3577.4368 - val_regression_loss: 1780.3503\n","Epoch 34/50\n","537/537 [==============================] - 0s 48us/step - loss: 31374.3214 - regression_loss: 15585.6264 - val_loss: 3297.5464 - val_regression_loss: 1639.7047\n","Epoch 35/50\n","537/537 [==============================] - 0s 64us/step - loss: 28802.6969 - regression_loss: 14291.3306 - val_loss: 3119.4150 - val_regression_loss: 1551.2509\n","Epoch 36/50\n","537/537 [==============================] - 0s 49us/step - loss: 28531.6933 - regression_loss: 14158.2394 - val_loss: 3100.0173 - val_regression_loss: 1543.2244\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 24944.4920 - regression_loss: 12373.7708 - val_loss: 3080.1646 - val_regression_loss: 1534.0648\n","Epoch 38/50\n","537/537 [==============================] - 0s 47us/step - loss: 21995.9940 - regression_loss: 10902.1974 - val_loss: 2744.7041 - val_regression_loss: 1364.9983\n","Epoch 39/50\n","537/537 [==============================] - 0s 49us/step - loss: 21590.7216 - regression_loss: 10683.7030 - val_loss: 2442.9070 - val_regression_loss: 1212.2937\n","Epoch 40/50\n","537/537 [==============================] - 0s 49us/step - loss: 19733.2348 - regression_loss: 9734.9186 - val_loss: 2298.2100 - val_regression_loss: 1139.3549\n","Epoch 41/50\n","537/537 [==============================] - 0s 47us/step - loss: 17759.7377 - regression_loss: 8741.0473 - val_loss: 2273.6987 - val_regression_loss: 1127.5839\n","Epoch 42/50\n","537/537 [==============================] - 0s 45us/step - loss: 16144.6121 - regression_loss: 7931.9258 - val_loss: 2174.5769 - val_regression_loss: 1077.6801\n","Epoch 43/50\n","537/537 [==============================] - 0s 48us/step - loss: 15095.0056 - regression_loss: 7408.7530 - val_loss: 1892.6855 - val_regression_loss: 935.0996\n","Epoch 44/50\n","537/537 [==============================] - 0s 47us/step - loss: 13413.2266 - regression_loss: 6549.6394 - val_loss: 1711.5083 - val_regression_loss: 843.8125\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 11501.6562 - regression_loss: 5582.7914 - val_loss: 1643.0973 - val_regression_loss: 810.4790\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 12074.7042 - regression_loss: 5878.1503 - val_loss: 1600.4287 - val_regression_loss: 790.0832\n","Epoch 47/50\n","537/537 [==============================] - 0s 51us/step - loss: 11544.8219 - regression_loss: 5613.6391 - val_loss: 1394.3684 - val_regression_loss: 686.4007\n","Epoch 48/50\n","537/537 [==============================] - 0s 46us/step - loss: 10518.9165 - regression_loss: 5095.0792 - val_loss: 1302.8870 - val_regression_loss: 640.7468\n","Epoch 49/50\n","537/537 [==============================] - 0s 45us/step - loss: 10229.2792 - regression_loss: 4947.2131 - val_loss: 1331.1669 - val_regression_loss: 656.7466\n","Epoch 50/50\n","537/537 [==============================] - 0s 48us/step - loss: 9652.9696 - regression_loss: 4674.9250 - val_loss: 1290.2783 - val_regression_loss: 636.9479\n","***************************** elapsed_time is:  3.0418906211853027\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 133347.0250 - regression_loss: 66195.7953 - val_loss: 15301.8525 - val_regression_loss: 7590.5039\n","Epoch 2/50\n","537/537 [==============================] - 0s 51us/step - loss: 119038.1179 - regression_loss: 59054.6045 - val_loss: 13093.0850 - val_regression_loss: 6489.1294\n","Epoch 3/50\n","537/537 [==============================] - 0s 44us/step - loss: 100377.6792 - regression_loss: 49753.0266 - val_loss: 10242.2588 - val_regression_loss: 5067.9082\n","Epoch 4/50\n","537/537 [==============================] - 0s 45us/step - loss: 79648.8848 - regression_loss: 39423.0235 - val_loss: 6891.3911 - val_regression_loss: 3398.8618\n","Epoch 5/50\n","537/537 [==============================] - 0s 53us/step - loss: 52966.6379 - regression_loss: 26130.5122 - val_loss: 3749.2708 - val_regression_loss: 1838.0706\n","Epoch 6/50\n","537/537 [==============================] - 0s 50us/step - loss: 29147.3224 - regression_loss: 14301.9379 - val_loss: 2087.6182 - val_regression_loss: 1021.3976\n","Epoch 7/50\n","537/537 [==============================] - 0s 49us/step - loss: 18038.7565 - regression_loss: 8864.6241 - val_loss: 2435.2471 - val_regression_loss: 1205.9670\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 22682.2816 - regression_loss: 11272.8795 - val_loss: 2593.1382 - val_regression_loss: 1284.1333\n","Epoch 9/50\n","537/537 [==============================] - 0s 51us/step - loss: 24583.8721 - regression_loss: 12215.5329 - val_loss: 1990.3529 - val_regression_loss: 976.7029\n","Epoch 10/50\n","537/537 [==============================] - 0s 54us/step - loss: 19361.3701 - regression_loss: 9557.0979 - val_loss: 1444.0579 - val_regression_loss: 697.5808\n","Epoch 11/50\n","537/537 [==============================] - 0s 49us/step - loss: 13338.8231 - regression_loss: 6497.4654 - val_loss: 1457.7507 - val_regression_loss: 700.7000\n","Epoch 12/50\n","537/537 [==============================] - 0s 52us/step - loss: 12603.5721 - regression_loss: 6101.3691 - val_loss: 1767.9803 - val_regression_loss: 854.7984\n","Epoch 13/50\n","537/537 [==============================] - 0s 45us/step - loss: 12527.0796 - regression_loss: 6057.7059 - val_loss: 1928.6926 - val_regression_loss: 936.3876\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 13730.7261 - regression_loss: 6670.0034 - val_loss: 1771.6458 - val_regression_loss: 860.6230\n","Epoch 15/50\n","537/537 [==============================] - 0s 49us/step - loss: 13712.1639 - regression_loss: 6681.2137 - val_loss: 1431.5222 - val_regression_loss: 694.0305\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 10767.2751 - regression_loss: 5235.2436 - val_loss: 1140.1403 - val_regression_loss: 551.7023\n","Epoch 17/50\n","537/537 [==============================] - 0s 48us/step - loss: 9673.0318 - regression_loss: 4713.7538 - val_loss: 1014.6801 - val_regression_loss: 491.6089\n","Epoch 18/50\n","537/537 [==============================] - 0s 46us/step - loss: 9201.2050 - regression_loss: 4498.3193 - val_loss: 1015.5610 - val_regression_loss: 493.5185\n","Epoch 19/50\n","537/537 [==============================] - 0s 42us/step - loss: 9799.5307 - regression_loss: 4808.3057 - val_loss: 1012.7024 - val_regression_loss: 492.3199\n","Epoch 20/50\n","537/537 [==============================] - 0s 46us/step - loss: 10004.0057 - regression_loss: 4909.4386 - val_loss: 945.7156 - val_regression_loss: 458.0968\n","Epoch 21/50\n","537/537 [==============================] - 0s 49us/step - loss: 9342.9516 - regression_loss: 4576.2515 - val_loss: 866.2640 - val_regression_loss: 417.0743\n","Epoch 22/50\n","537/537 [==============================] - 0s 48us/step - loss: 8062.2943 - regression_loss: 3916.8019 - val_loss: 845.0407 - val_regression_loss: 405.0831\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 7446.7023 - regression_loss: 3598.6522 - val_loss: 883.5782 - val_regression_loss: 423.1636\n","Epoch 24/50\n","537/537 [==============================] - 0s 60us/step - loss: 7456.4199 - regression_loss: 3593.6360 - val_loss: 921.9072 - val_regression_loss: 441.6194\n","Epoch 25/50\n","537/537 [==============================] - 0s 47us/step - loss: 7626.1257 - regression_loss: 3677.1749 - val_loss: 902.3700 - val_regression_loss: 431.7492\n","Epoch 26/50\n","537/537 [==============================] - 0s 46us/step - loss: 7398.7116 - regression_loss: 3559.4993 - val_loss: 820.8828 - val_regression_loss: 391.5060\n","Epoch 27/50\n","537/537 [==============================] - 0s 47us/step - loss: 6578.9367 - regression_loss: 3154.6384 - val_loss: 731.1328 - val_regression_loss: 347.5005\n","Epoch 28/50\n","537/537 [==============================] - 0s 47us/step - loss: 6261.3417 - regression_loss: 2999.3211 - val_loss: 667.8968 - val_regression_loss: 316.9064\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 5623.9286 - regression_loss: 2687.3692 - val_loss: 637.6219 - val_regression_loss: 302.5900\n","Epoch 30/50\n","537/537 [==============================] - 0s 50us/step - loss: 5490.9604 - regression_loss: 2626.4613 - val_loss: 619.2604 - val_regression_loss: 293.8135\n","Epoch 31/50\n","537/537 [==============================] - 0s 44us/step - loss: 5543.4034 - regression_loss: 2654.5128 - val_loss: 608.2791 - val_regression_loss: 288.2426\n","Epoch 32/50\n","537/537 [==============================] - 0s 53us/step - loss: 5159.2243 - regression_loss: 2462.6398 - val_loss: 614.0329 - val_regression_loss: 290.6489\n","Epoch 33/50\n","537/537 [==============================] - 0s 50us/step - loss: 4832.9368 - regression_loss: 2295.5526 - val_loss: 628.1470 - val_regression_loss: 297.1645\n","Epoch 34/50\n","537/537 [==============================] - 0s 45us/step - loss: 4513.6415 - regression_loss: 2133.7799 - val_loss: 623.6515 - val_regression_loss: 294.6101\n","Epoch 35/50\n","537/537 [==============================] - 0s 48us/step - loss: 4372.4139 - regression_loss: 2058.2126 - val_loss: 591.7743 - val_regression_loss: 278.7261\n","Epoch 36/50\n","537/537 [==============================] - 0s 49us/step - loss: 4027.6736 - regression_loss: 1883.0149 - val_loss: 557.3157 - val_regression_loss: 261.7676\n","Epoch 37/50\n","537/537 [==============================] - 0s 50us/step - loss: 3748.5342 - regression_loss: 1745.6360 - val_loss: 540.3714 - val_regression_loss: 253.5549\n","Epoch 38/50\n","537/537 [==============================] - 0s 56us/step - loss: 3609.3703 - regression_loss: 1677.4003 - val_loss: 538.7046 - val_regression_loss: 252.7546\n","Epoch 39/50\n","537/537 [==============================] - 0s 48us/step - loss: 3481.3296 - regression_loss: 1614.8751 - val_loss: 544.7120 - val_regression_loss: 255.6239\n","Epoch 40/50\n","537/537 [==============================] - 0s 51us/step - loss: 3147.9365 - regression_loss: 1445.9196 - val_loss: 554.7286 - val_regression_loss: 260.4755\n","Epoch 41/50\n","537/537 [==============================] - 0s 47us/step - loss: 3187.2804 - regression_loss: 1463.1921 - val_loss: 559.5880 - val_regression_loss: 262.9161\n","Epoch 42/50\n","537/537 [==============================] - 0s 42us/step - loss: 3054.3613 - regression_loss: 1395.7717 - val_loss: 550.3871 - val_regression_loss: 258.5593\n","Epoch 43/50\n","537/537 [==============================] - 0s 46us/step - loss: 2864.6760 - regression_loss: 1304.7128 - val_loss: 533.7984 - val_regression_loss: 250.6003\n","Epoch 44/50\n","537/537 [==============================] - 0s 48us/step - loss: 2788.8070 - regression_loss: 1265.6633 - val_loss: 518.5529 - val_regression_loss: 243.2088\n","Epoch 45/50\n","537/537 [==============================] - 0s 52us/step - loss: 2699.2478 - regression_loss: 1223.0999 - val_loss: 503.9279 - val_regression_loss: 235.8714\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 2514.0520 - regression_loss: 1131.1503 - val_loss: 491.1346 - val_regression_loss: 229.2713\n","Epoch 47/50\n","537/537 [==============================] - 0s 54us/step - loss: 2458.0983 - regression_loss: 1098.8996 - val_loss: 477.6223 - val_regression_loss: 222.3754\n","Epoch 48/50\n","537/537 [==============================] - 0s 56us/step - loss: 2355.4719 - regression_loss: 1046.4927 - val_loss: 462.8646 - val_regression_loss: 215.0287\n","Epoch 49/50\n","537/537 [==============================] - 0s 42us/step - loss: 2217.9158 - regression_loss: 978.0499 - val_loss: 447.5686 - val_regression_loss: 207.5403\n","Epoch 50/50\n","537/537 [==============================] - 0s 43us/step - loss: 2215.2587 - regression_loss: 979.7051 - val_loss: 435.5812 - val_regression_loss: 201.6736\n","***************************** elapsed_time is:  3.293840169906616\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 113713.7519 - regression_loss: 56812.0633 - val_loss: 12823.5430 - val_regression_loss: 6404.4194\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 100900.2724 - regression_loss: 50398.3899 - val_loss: 10908.8828 - val_regression_loss: 5445.7598\n","Epoch 3/50\n","537/537 [==============================] - 0s 48us/step - loss: 84017.9408 - regression_loss: 41950.8437 - val_loss: 8335.0693 - val_regression_loss: 4156.4214\n","Epoch 4/50\n","537/537 [==============================] - 0s 49us/step - loss: 63446.5275 - regression_loss: 31642.1669 - val_loss: 5363.7046 - val_regression_loss: 2666.2134\n","Epoch 5/50\n","537/537 [==============================] - 0s 51us/step - loss: 38706.5724 - regression_loss: 19237.0745 - val_loss: 2674.6392 - val_regression_loss: 1313.0300\n","Epoch 6/50\n","537/537 [==============================] - 0s 50us/step - loss: 18722.8646 - regression_loss: 9173.1737 - val_loss: 1633.3004 - val_regression_loss: 778.8287\n","Epoch 7/50\n","537/537 [==============================] - 0s 49us/step - loss: 11870.8576 - regression_loss: 5639.2677 - val_loss: 2331.7273 - val_regression_loss: 1118.8472\n","Epoch 8/50\n","537/537 [==============================] - 0s 43us/step - loss: 18493.6932 - regression_loss: 8878.2043 - val_loss: 2236.5122 - val_regression_loss: 1075.5948\n","Epoch 9/50\n","537/537 [==============================] - 0s 47us/step - loss: 17932.9214 - regression_loss: 8637.4771 - val_loss: 1490.3610 - val_regression_loss: 712.4581\n","Epoch 10/50\n","537/537 [==============================] - 0s 49us/step - loss: 12064.2728 - regression_loss: 5784.6999 - val_loss: 968.6897 - val_regression_loss: 460.5395\n","Epoch 11/50\n","537/537 [==============================] - 0s 47us/step - loss: 7660.9864 - regression_loss: 3653.3922 - val_loss: 960.1102 - val_regression_loss: 462.1161\n","Epoch 12/50\n","537/537 [==============================] - 0s 54us/step - loss: 6906.4010 - regression_loss: 3324.3970 - val_loss: 1194.6729 - val_regression_loss: 582.2965\n","Epoch 13/50\n","537/537 [==============================] - 0s 50us/step - loss: 8487.4124 - regression_loss: 4137.8244 - val_loss: 1341.2948 - val_regression_loss: 656.3802\n","Epoch 14/50\n","537/537 [==============================] - 0s 57us/step - loss: 9295.9475 - regression_loss: 4547.2246 - val_loss: 1288.6827 - val_regression_loss: 629.5848\n","Epoch 15/50\n","537/537 [==============================] - 0s 45us/step - loss: 9082.6812 - regression_loss: 4438.7933 - val_loss: 1093.6110 - val_regression_loss: 530.8993\n","Epoch 16/50\n","537/537 [==============================] - 0s 46us/step - loss: 7509.4304 - regression_loss: 3639.1775 - val_loss: 876.4564 - val_regression_loss: 420.9510\n","Epoch 17/50\n","537/537 [==============================] - 0s 50us/step - loss: 6168.1751 - regression_loss: 2958.2085 - val_loss: 737.0282 - val_regression_loss: 349.9246\n","Epoch 18/50\n","537/537 [==============================] - 0s 55us/step - loss: 5088.5376 - regression_loss: 2406.6895 - val_loss: 710.5826 - val_regression_loss: 335.7100\n","Epoch 19/50\n","537/537 [==============================] - 0s 44us/step - loss: 5244.2956 - regression_loss: 2478.2409 - val_loss: 746.3549 - val_regression_loss: 353.0603\n","Epoch 20/50\n","537/537 [==============================] - 0s 51us/step - loss: 5648.0779 - regression_loss: 2675.2183 - val_loss: 759.9991 - val_regression_loss: 359.8617\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 5587.5324 - regression_loss: 2647.0752 - val_loss: 718.8356 - val_regression_loss: 339.6535\n","Epoch 22/50\n","537/537 [==============================] - 0s 52us/step - loss: 5256.0913 - regression_loss: 2483.4662 - val_loss: 660.2007 - val_regression_loss: 310.9308\n","Epoch 23/50\n","537/537 [==============================] - 0s 48us/step - loss: 4928.4535 - regression_loss: 2324.9011 - val_loss: 628.8214 - val_regression_loss: 295.8913\n","Epoch 24/50\n","537/537 [==============================] - 0s 49us/step - loss: 4480.8933 - regression_loss: 2106.2105 - val_loss: 633.3304 - val_regression_loss: 298.7466\n","Epoch 25/50\n","537/537 [==============================] - 0s 63us/step - loss: 4505.4628 - regression_loss: 2125.3781 - val_loss: 648.4645 - val_regression_loss: 306.7697\n","Epoch 26/50\n","537/537 [==============================] - 0s 49us/step - loss: 4429.3640 - regression_loss: 2091.1689 - val_loss: 647.0270 - val_regression_loss: 306.3371\n","Epoch 27/50\n","537/537 [==============================] - 0s 48us/step - loss: 4520.9009 - regression_loss: 2140.6311 - val_loss: 619.7779 - val_regression_loss: 292.8040\n","Epoch 28/50\n","537/537 [==============================] - 0s 48us/step - loss: 4256.3415 - regression_loss: 2006.7405 - val_loss: 581.0084 - val_regression_loss: 273.3534\n","Epoch 29/50\n","537/537 [==============================] - 0s 49us/step - loss: 4018.3537 - regression_loss: 1888.9872 - val_loss: 547.2590 - val_regression_loss: 256.2518\n","Epoch 30/50\n","537/537 [==============================] - 0s 49us/step - loss: 3705.8411 - regression_loss: 1729.9705 - val_loss: 527.1727 - val_regression_loss: 245.8901\n","Epoch 31/50\n","537/537 [==============================] - 0s 47us/step - loss: 3555.3538 - regression_loss: 1652.9201 - val_loss: 514.4645 - val_regression_loss: 239.1960\n","Epoch 32/50\n","537/537 [==============================] - 0s 48us/step - loss: 3724.4420 - regression_loss: 1736.6979 - val_loss: 500.4369 - val_regression_loss: 231.9141\n","Epoch 33/50\n","537/537 [==============================] - 0s 51us/step - loss: 3460.4132 - regression_loss: 1599.2310 - val_loss: 486.7383 - val_regression_loss: 224.9165\n","Epoch 34/50\n","537/537 [==============================] - 0s 49us/step - loss: 3237.5622 - regression_loss: 1487.4059 - val_loss: 478.6715 - val_regression_loss: 220.9016\n","Epoch 35/50\n","537/537 [==============================] - 0s 52us/step - loss: 3180.0584 - regression_loss: 1459.6870 - val_loss: 474.8773 - val_regression_loss: 219.1352\n","Epoch 36/50\n","537/537 [==============================] - 0s 52us/step - loss: 3003.7535 - regression_loss: 1373.7998 - val_loss: 468.5129 - val_regression_loss: 216.1434\n","Epoch 37/50\n","537/537 [==============================] - 0s 45us/step - loss: 2949.6166 - regression_loss: 1346.5372 - val_loss: 454.0927 - val_regression_loss: 209.1313\n","Epoch 38/50\n","537/537 [==============================] - 0s 52us/step - loss: 2911.4060 - regression_loss: 1329.8121 - val_loss: 438.5869 - val_regression_loss: 201.5096\n","Epoch 39/50\n","537/537 [==============================] - 0s 53us/step - loss: 2783.4383 - regression_loss: 1265.5708 - val_loss: 426.4600 - val_regression_loss: 195.5396\n","Epoch 40/50\n","537/537 [==============================] - 0s 52us/step - loss: 2728.2072 - regression_loss: 1239.9768 - val_loss: 418.3177 - val_regression_loss: 191.5146\n","Epoch 41/50\n","537/537 [==============================] - 0s 49us/step - loss: 2597.3871 - regression_loss: 1176.9188 - val_loss: 410.9243 - val_regression_loss: 187.8433\n","Epoch 42/50\n","537/537 [==============================] - 0s 51us/step - loss: 2366.9455 - regression_loss: 1061.2928 - val_loss: 404.6643 - val_regression_loss: 184.7267\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 2430.8979 - regression_loss: 1094.4256 - val_loss: 399.5229 - val_regression_loss: 182.1305\n","Epoch 44/50\n","537/537 [==============================] - 0s 47us/step - loss: 2364.1252 - regression_loss: 1058.1272 - val_loss: 392.7278 - val_regression_loss: 178.6793\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 2263.9149 - regression_loss: 1009.3867 - val_loss: 383.6842 - val_regression_loss: 174.0420\n","Epoch 46/50\n","537/537 [==============================] - 0s 52us/step - loss: 2203.1904 - regression_loss: 979.4386 - val_loss: 374.0158 - val_regression_loss: 169.0878\n","Epoch 47/50\n","537/537 [==============================] - 0s 51us/step - loss: 2107.8753 - regression_loss: 930.9265 - val_loss: 363.8661 - val_regression_loss: 163.9606\n","Epoch 48/50\n","537/537 [==============================] - 0s 57us/step - loss: 2008.1417 - regression_loss: 879.8966 - val_loss: 353.5723 - val_regression_loss: 158.8484\n","Epoch 49/50\n","537/537 [==============================] - 0s 60us/step - loss: 2019.8493 - regression_loss: 886.5859 - val_loss: 342.3258 - val_regression_loss: 153.2823\n","Epoch 50/50\n","537/537 [==============================] - 0s 49us/step - loss: 2006.2587 - regression_loss: 881.4991 - val_loss: 331.3132 - val_regression_loss: 147.7893\n","***************************** elapsed_time is:  3.0716962814331055\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"32zLJ4yfLOj8","colab_type":"code","outputId":"9b3e7450-b320-402a-d2d5-cf1b1b9ddc71","executionInfo":{"status":"ok","timestamp":1584386392957,"user_tz":-60,"elapsed":678231,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["df_dragonnet=pd.DataFrame([train_result, test_result])\n","df_dragonnet"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.831563</td>\n","      <td>1.17451</td>\n","      <td>4.796549</td>\n","      <td>0.829327</td>\n","      <td>0.346305</td>\n","      <td>1.655209</td>\n","      <td>Dragonnet</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.942670</td>\n","      <td>1.12136</td>\n","      <td>4.701229</td>\n","      <td>0.938807</td>\n","      <td>0.398237</td>\n","      <td>2.062225</td>\n","      <td>Dragonnet</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...     method  train\n","0         2.831563            1.17451  ...  Dragonnet   True\n","1         2.942670            1.12136  ...  Dragonnet  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"imeaA1mMLOkD","colab_type":"text"},"source":["### 1.7.1 Neura Network Visualization"]},{"cell_type":"code","metadata":{"id":"i9IhdaTnLOkF","colab_type":"code","outputId":"cc79b4d7-0dea-438c-8a7d-de737eb3c588","executionInfo":{"status":"ok","timestamp":1584386396340,"user_tz":-60,"elapsed":681604,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = causal_forest(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'Dragonnet', 'train': True})\n","test_result.update({'method': 'Dragonnet', 'train': False})"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 1ms/step - loss: 14463.6517 - regression_loss: 7055.7229 - val_loss: 1608.2212 - val_regression_loss: 780.8147\n","Epoch 2/50\n","537/537 [==============================] - 0s 60us/step - loss: 10493.0339 - regression_loss: 5075.6086 - val_loss: 1046.6842 - val_regression_loss: 500.5428\n","Epoch 3/50\n","537/537 [==============================] - 0s 45us/step - loss: 6544.5100 - regression_loss: 3105.4256 - val_loss: 550.2848 - val_regression_loss: 252.8628\n","Epoch 4/50\n","537/537 [==============================] - 0s 46us/step - loss: 3293.9217 - regression_loss: 1484.7160 - val_loss: 395.8221 - val_regression_loss: 176.1625\n","Epoch 5/50\n","537/537 [==============================] - 0s 43us/step - loss: 3265.9049 - regression_loss: 1475.1438 - val_loss: 375.6133 - val_regression_loss: 166.2253\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 3347.4611 - regression_loss: 1517.2158 - val_loss: 348.0320 - val_regression_loss: 152.4330\n","Epoch 7/50\n","537/537 [==============================] - 0s 46us/step - loss: 2670.6943 - regression_loss: 1178.7741 - val_loss: 403.1710 - val_regression_loss: 180.1818\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 2487.6386 - regression_loss: 1089.0835 - val_loss: 444.1366 - val_regression_loss: 201.3593\n","Epoch 9/50\n","537/537 [==============================] - 0s 53us/step - loss: 2546.0100 - regression_loss: 1123.3932 - val_loss: 403.4810 - val_regression_loss: 182.0386\n","Epoch 10/50\n","537/537 [==============================] - 0s 50us/step - loss: 2295.1182 - regression_loss: 1007.1702 - val_loss: 328.5515 - val_regression_loss: 145.5975\n","Epoch 11/50\n","537/537 [==============================] - 0s 49us/step - loss: 1942.6536 - regression_loss: 837.7286 - val_loss: 280.9663 - val_regression_loss: 122.6022\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 1835.3170 - regression_loss: 789.4314 - val_loss: 275.0294 - val_regression_loss: 120.1052\n","Epoch 13/50\n","537/537 [==============================] - 0s 54us/step - loss: 1989.8231 - regression_loss: 870.6277 - val_loss: 274.6424 - val_regression_loss: 120.0931\n","Epoch 14/50\n","537/537 [==============================] - 0s 51us/step - loss: 2003.7294 - regression_loss: 879.8314 - val_loss: 264.7813 - val_regression_loss: 115.1653\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 1824.5789 - regression_loss: 789.1381 - val_loss: 265.9443 - val_regression_loss: 115.6623\n","Epoch 16/50\n","537/537 [==============================] - 0s 52us/step - loss: 1672.8248 - regression_loss: 713.8499 - val_loss: 284.0953 - val_regression_loss: 124.6298\n","Epoch 17/50\n","537/537 [==============================] - 0s 47us/step - loss: 1660.0853 - regression_loss: 705.3694 - val_loss: 301.1646 - val_regression_loss: 133.0856\n","Epoch 18/50\n","537/537 [==============================] - 0s 52us/step - loss: 1716.8625 - regression_loss: 734.7701 - val_loss: 299.8200 - val_regression_loss: 132.3919\n","Epoch 19/50\n","537/537 [==============================] - 0s 51us/step - loss: 1677.5430 - regression_loss: 715.3431 - val_loss: 280.7529 - val_regression_loss: 122.8930\n","Epoch 20/50\n","537/537 [==============================] - 0s 43us/step - loss: 1610.1427 - regression_loss: 682.4987 - val_loss: 258.0789 - val_regression_loss: 111.6299\n","Epoch 21/50\n","537/537 [==============================] - 0s 50us/step - loss: 1545.2782 - regression_loss: 651.0875 - val_loss: 244.1871 - val_regression_loss: 104.7812\n","Epoch 22/50\n","537/537 [==============================] - 0s 49us/step - loss: 1549.3326 - regression_loss: 654.9351 - val_loss: 239.5683 - val_regression_loss: 102.5707\n","Epoch 23/50\n","537/537 [==============================] - 0s 55us/step - loss: 1531.6809 - regression_loss: 646.4935 - val_loss: 240.5074 - val_regression_loss: 103.1290\n","Epoch 24/50\n","537/537 [==============================] - 0s 57us/step - loss: 1525.2000 - regression_loss: 643.4081 - val_loss: 246.1219 - val_regression_loss: 105.9971\n","Epoch 25/50\n","537/537 [==============================] - 0s 49us/step - loss: 1494.0656 - regression_loss: 630.6145 - val_loss: 253.3521 - val_regression_loss: 109.6427\n","Epoch 26/50\n","537/537 [==============================] - 0s 47us/step - loss: 1478.5153 - regression_loss: 619.4590 - val_loss: 256.2385 - val_regression_loss: 111.0954\n","Epoch 27/50\n","537/537 [==============================] - 0s 49us/step - loss: 1495.8633 - regression_loss: 629.9695 - val_loss: 251.0466 - val_regression_loss: 108.5037\n","Epoch 28/50\n","537/537 [==============================] - 0s 43us/step - loss: 1453.4225 - regression_loss: 609.9681 - val_loss: 241.6800 - val_regression_loss: 103.8312\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 1438.3256 - regression_loss: 602.6302 - val_loss: 233.3821 - val_regression_loss: 99.7082\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 1435.4236 - regression_loss: 600.2444 - val_loss: 229.2235 - val_regression_loss: 97.6696\n","Epoch 31/50\n","537/537 [==============================] - 0s 54us/step - loss: 1427.1457 - regression_loss: 598.0513 - val_loss: 228.6743 - val_regression_loss: 97.4316\n","Epoch 32/50\n","537/537 [==============================] - 0s 51us/step - loss: 1386.6820 - regression_loss: 574.9845 - val_loss: 230.7175 - val_regression_loss: 98.4829\n","Epoch 33/50\n","537/537 [==============================] - 0s 50us/step - loss: 1385.8261 - regression_loss: 578.2017 - val_loss: 231.8330 - val_regression_loss: 99.0674\n","Epoch 34/50\n","537/537 [==============================] - 0s 49us/step - loss: 1386.2012 - regression_loss: 579.6642 - val_loss: 228.2986 - val_regression_loss: 97.3203\n","Epoch 35/50\n","537/537 [==============================] - 0s 47us/step - loss: 1362.0450 - regression_loss: 564.5802 - val_loss: 223.0025 - val_regression_loss: 94.6851\n","Epoch 36/50\n","537/537 [==============================] - 0s 52us/step - loss: 1344.4790 - regression_loss: 557.7671 - val_loss: 219.5616 - val_regression_loss: 92.9694\n","Epoch 37/50\n","537/537 [==============================] - 0s 56us/step - loss: 1302.4880 - regression_loss: 537.4285 - val_loss: 217.6865 - val_regression_loss: 92.0282\n","Epoch 38/50\n","537/537 [==============================] - 0s 47us/step - loss: 1313.0491 - regression_loss: 542.5424 - val_loss: 218.0079 - val_regression_loss: 92.1864\n","Epoch 39/50\n","537/537 [==============================] - 0s 52us/step - loss: 1311.8474 - regression_loss: 541.1238 - val_loss: 217.8799 - val_regression_loss: 92.1236\n","Epoch 40/50\n","537/537 [==============================] - 0s 48us/step - loss: 1298.3201 - regression_loss: 534.5073 - val_loss: 215.1196 - val_regression_loss: 90.7547\n","Epoch 41/50\n","537/537 [==============================] - 0s 50us/step - loss: 1285.8948 - regression_loss: 531.5515 - val_loss: 211.9411 - val_regression_loss: 89.1803\n","Epoch 42/50\n","537/537 [==============================] - 0s 45us/step - loss: 1260.1713 - regression_loss: 518.2275 - val_loss: 209.6083 - val_regression_loss: 88.0200\n","Epoch 43/50\n","537/537 [==============================] - 0s 51us/step - loss: 1223.4202 - regression_loss: 498.7723 - val_loss: 209.0818 - val_regression_loss: 87.7560\n","Epoch 44/50\n","537/537 [==============================] - 0s 46us/step - loss: 1256.6766 - regression_loss: 514.8880 - val_loss: 208.7825 - val_regression_loss: 87.6115\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 1219.0500 - regression_loss: 499.0751 - val_loss: 207.4316 - val_regression_loss: 86.9446\n","Epoch 46/50\n","537/537 [==============================] - 0s 50us/step - loss: 1212.6059 - regression_loss: 493.6050 - val_loss: 205.0575 - val_regression_loss: 85.7713\n","Epoch 47/50\n","537/537 [==============================] - 0s 46us/step - loss: 1195.4554 - regression_loss: 484.9815 - val_loss: 202.8302 - val_regression_loss: 84.6712\n","Epoch 48/50\n","537/537 [==============================] - 0s 54us/step - loss: 1179.2105 - regression_loss: 477.8896 - val_loss: 200.4553 - val_regression_loss: 83.4941\n","Epoch 49/50\n","537/537 [==============================] - 0s 53us/step - loss: 1180.1858 - regression_loss: 476.5443 - val_loss: 198.7447 - val_regression_loss: 82.6505\n","Epoch 50/50\n","537/537 [==============================] - 0s 49us/step - loss: 1154.7922 - regression_loss: 465.8100 - val_loss: 196.9748 - val_regression_loss: 81.7787\n","***************************** elapsed_time is:  3.0875983238220215\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VvttPAc8LOkJ","colab_type":"code","outputId":"b42d897b-fbd5-49a0-b71b-ebee104ee82a","executionInfo":{"status":"ok","timestamp":1584386396652,"user_tz":-60,"elapsed":681907,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":40,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"E6ZW3Us6LOkN","colab_type":"text"},"source":["## QUESTION 6\n","\n","IS THE DRAGONNET NEURAL NETWORK ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"FMGays1ltOL5","colab_type":"text"},"source":["# Answer 6 #\n","\n","The Dragonnet NN proves to be a good estimator for estimating the individual treatment effect as shown by the graph above which shows consistency in prediction for both treatments and non-treatments. There seems to be a good symmetry between high estimated treatment effect having high real treatment effect and vice versa."]},{"cell_type":"markdown","metadata":{"id":"D09tYZt9LOkO","colab_type":"text"},"source":["## 1.8 Comparison of the methods"]},{"cell_type":"code","metadata":{"id":"pti3CRzkLOkO","colab_type":"code","outputId":"50107108-a08a-4eb4-85d8-2f4d770fec72","executionInfo":{"status":"ok","timestamp":1584386396653,"user_tz":-60,"elapsed":681900,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":483}},"source":["pd.concat([df_S_learner_LR, df_PSW_LR, df_S_learner_RF, df_T_learner_LR, df_T_learner_RF, df_causal_forest, df_dragonnet ], ignore_index=True)"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.633660</td>\n","      <td>2.623297</td>\n","      <td>8.362125</td>\n","      <td>0.732443</td>\n","      <td>0.238185</td>\n","      <td>1.493276</td>\n","      <td>S-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.625971</td>\n","      <td>2.635993</td>\n","      <td>8.213626</td>\n","      <td>1.292668</td>\n","      <td>0.396246</td>\n","      <td>2.474603</td>\n","      <td>S-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.595322</td>\n","      <td>2.537818</td>\n","      <td>8.244302</td>\n","      <td>0.412006</td>\n","      <td>0.284332</td>\n","      <td>0.457697</td>\n","      <td>PSW</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.837997</td>\n","      <td>3.484394</td>\n","      <td>8.323623</td>\n","      <td>3.783440</td>\n","      <td>2.649187</td>\n","      <td>3.225824</td>\n","      <td>PSW</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.105397</td>\n","      <td>1.050890</td>\n","      <td>4.766095</td>\n","      <td>0.500324</td>\n","      <td>0.133818</td>\n","      <td>0.928113</td>\n","      <td>S-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3.319565</td>\n","      <td>1.273795</td>\n","      <td>5.171689</td>\n","      <td>0.446731</td>\n","      <td>0.133490</td>\n","      <td>1.027640</td>\n","      <td>S-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.276868</td>\n","      <td>1.055409</td>\n","      <td>3.319402</td>\n","      <td>0.149960</td>\n","      <td>0.133955</td>\n","      <td>0.129121</td>\n","      <td>T-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2.337753</td>\n","      <td>1.122464</td>\n","      <td>3.263915</td>\n","      <td>0.263287</td>\n","      <td>0.195850</td>\n","      <td>0.327846</td>\n","      <td>T-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.779325</td>\n","      <td>0.953937</td>\n","      <td>2.242132</td>\n","      <td>0.123705</td>\n","      <td>0.094111</td>\n","      <td>0.115848</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2.509532</td>\n","      <td>1.112424</td>\n","      <td>3.500381</td>\n","      <td>0.205711</td>\n","      <td>0.118718</td>\n","      <td>0.275564</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.172661</td>\n","      <td>1.921242</td>\n","      <td>6.330031</td>\n","      <td>0.441738</td>\n","      <td>0.198733</td>\n","      <td>0.823171</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4.378083</td>\n","      <td>1.797608</td>\n","      <td>6.606150</td>\n","      <td>0.683703</td>\n","      <td>0.233695</td>\n","      <td>1.354973</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2.831563</td>\n","      <td>1.174510</td>\n","      <td>4.796549</td>\n","      <td>0.829327</td>\n","      <td>0.346305</td>\n","      <td>1.655209</td>\n","      <td>Dragonnet</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2.942670</td>\n","      <td>1.121360</td>\n","      <td>4.701229</td>\n","      <td>0.938807</td>\n","      <td>0.398237</td>\n","      <td>2.062225</td>\n","      <td>Dragonnet</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    pehe_score-mean  pehe_score-median  ...        method  train\n","0          5.633660           2.623297  ...  S-Learner LR   True\n","1          5.625971           2.635993  ...  S-Learner LR  False\n","2          5.595322           2.537818  ...           PSW   True\n","3          6.837997           3.484394  ...           PSW  False\n","4          3.105397           1.050890  ...  S-Learner RF   True\n","5          3.319565           1.273795  ...  S-Learner RF  False\n","6          2.276868           1.055409  ...  T-Learner LR   True\n","7          2.337753           1.122464  ...  T-Learner LR  False\n","8          1.779325           0.953937  ...  T-Learner RF   True\n","9          2.509532           1.112424  ...  T-Learner RF  False\n","10         4.172661           1.921242  ...  T-Learner RF   True\n","11         4.378083           1.797608  ...  T-Learner RF  False\n","12         2.831563           1.174510  ...     Dragonnet   True\n","13         2.942670           1.121360  ...     Dragonnet  False\n","\n","[14 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"5kl0lnoxLOkU","colab_type":"text"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","## QUESTION 7\n","\n","HOW DO THE DIFFERENT MODELS COMPARE IN TERMS OF MEAN PEHE ACCURACY? WHAT ASPECTS DO YOU THINK DETERMINE WHETHER ONE MODEL PERFOMS BETTER THAN ANOTER?"]},{"cell_type":"markdown","metadata":{"id":"G_J96sg0uL9Z","colab_type":"text"},"source":["# Answer 7 #\n","\n","By drawing comparison between all the models tested for S-learners and T-learners, the Random forest trained model for T-learner and Dragonnet NN seem to perform best among the lot. Their pehe-scores seems stable, less or no overfitting between both the classes of Training True and False across the said models which to me is a good measure of determination of a good model performance. Additionally, the fact that S-Learner model algorithms use singel supervised learning technique whereas the T-Learner models use two estimators to execute the same task should also be considered while making a decision. Not to forget, the ability of handling hetrogenity by a model also influences its pehe-accuracy scores and in turn their performance."]},{"cell_type":"code","metadata":{"id":"-fzmpIfvaEbu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}