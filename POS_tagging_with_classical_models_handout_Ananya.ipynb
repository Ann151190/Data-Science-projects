{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"POS_tagging_with_classical_models_handout_Ananya.ipynb","provenance":[{"file_id":"1nSfh7nH1YbsxxtUAo_aWf-UIAjGwP2ua","timestamp":1574113420810},{"file_id":"14tKuuSDCszWD2v1bY97NrR6UdYWYozEy","timestamp":1573908483791}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"no_k76e0NZHI"},"source":["# Task: Part of speech tagging\n","\n","In this task we try to recreate a very rudimentary POS tagger \"from scratch\" using SpaCy and CRF models. \n","\n","(We disregard the fact, that SpaCy has a built in POS tagger for the moment for demonstration purposes.)\n","\n","The input is a tokenized English sentence. The task is to label each word with a part of speech (POS) tag. The tag set, which is identical the [Universal Dependencies project's](https://universaldependencies.org/) basic tag set is the following:\n","\n","- NOUN: noun\n","- VERB: verb\n","- DET: determiner\n","- ADJ: adjective\n","- ADP: adposition (e.g., prepositions)\n","- ADV: adverb\n","- CONJ: conjunction\n","- NUM: numeral\n","- PART: particle (function word that cannot be inflected, has no meaning in\n","  itself and doesn't fit elsewhere, e.g., \"to\")\n","- PRON: pronoun\n","- .: punctuation\n","- X: other\n","\n","The code in this task is an adaptation of the NER code in the sklearn-crfsuite documentation.\n","\n","# The data set\n","\n","__Brown__ corpus: \"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry KuÄera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961\" (Wikpedia: Brown Corpus)\n","\n","Let's download and inspect the data!"]},{"cell_type":"code","metadata":{"id":"qfHhoKYsQVSs","colab_type":"code","colab":{}},"source":["%%capture\n","!pip install nltk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:20.712982Z","start_time":"2019-11-12T09:32:16.093693Z"},"colab_type":"code","id":"KAPwh8mmNZHM","outputId":"46cae560-2c85-4550-97d6-b6b14b236dc3","executionInfo":{"status":"ok","timestamp":1574429586101,"user_tz":-60,"elapsed":6446,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","\n","from nltk.corpus import brown\n","nltk.download('brown')\n","\n","brown.words()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:21.291370Z","start_time":"2019-11-12T09:32:20.723897Z"},"colab_type":"code","id":"3kSgq4e0NZHi","outputId":"4016a36f-19dc-49d9-85e1-e7b603406db5","executionInfo":{"status":"ok","timestamp":1574429586102,"user_tz":-60,"elapsed":6440,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["nltk.download('universal_tagset')\n","brown.tagged_words(tagset='universal')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'), ('Fulton', 'NOUN'), ...]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:21.352042Z","start_time":"2019-11-12T09:32:21.297210Z"},"colab_type":"code","id":"9tT1DBTtNZHu","outputId":"51d47b68-ff24-4377-cd4a-789c18387229","executionInfo":{"status":"ok","timestamp":1574429586103,"user_tz":-60,"elapsed":6434,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["brown.sents()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:25.131849Z","start_time":"2019-11-12T09:32:21.365202Z"},"colab_type":"code","id":"UuwBB-XRNZH6","outputId":"4b37fd63-773c-4c35-983b-fb53925b5166","executionInfo":{"status":"ok","timestamp":1574429588585,"user_tz":-60,"elapsed":8909,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(brown.words())"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1161192"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m8VYZmFCsNTa"},"source":["From the brown the object provided by NLTK we will work with the tagged sentence list:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:25.163453Z","start_time":"2019-11-12T09:32:25.136038Z"},"colab_type":"code","id":"bh0wAJkWNnlV","outputId":"09e02e72-455c-4244-eb61-36ade9426f55","executionInfo":{"status":"ok","timestamp":1574429588586,"user_tz":-60,"elapsed":8901,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["sents = brown.tagged_sents(tagset=\"universal\")\n","\n","sents[:2]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('The', 'DET'),\n","  ('Fulton', 'NOUN'),\n","  ('County', 'NOUN'),\n","  ('Grand', 'ADJ'),\n","  ('Jury', 'NOUN'),\n","  ('said', 'VERB'),\n","  ('Friday', 'NOUN'),\n","  ('an', 'DET'),\n","  ('investigation', 'NOUN'),\n","  ('of', 'ADP'),\n","  (\"Atlanta's\", 'NOUN'),\n","  ('recent', 'ADJ'),\n","  ('primary', 'NOUN'),\n","  ('election', 'NOUN'),\n","  ('produced', 'VERB'),\n","  ('``', '.'),\n","  ('no', 'DET'),\n","  ('evidence', 'NOUN'),\n","  (\"''\", '.'),\n","  ('that', 'ADP'),\n","  ('any', 'DET'),\n","  ('irregularities', 'NOUN'),\n","  ('took', 'VERB'),\n","  ('place', 'NOUN'),\n","  ('.', '.')],\n"," [('The', 'DET'),\n","  ('jury', 'NOUN'),\n","  ('further', 'ADV'),\n","  ('said', 'VERB'),\n","  ('in', 'ADP'),\n","  ('term-end', 'NOUN'),\n","  ('presentments', 'NOUN'),\n","  ('that', 'ADP'),\n","  ('the', 'DET'),\n","  ('City', 'NOUN'),\n","  ('Executive', 'ADJ'),\n","  ('Committee', 'NOUN'),\n","  (',', '.'),\n","  ('which', 'DET'),\n","  ('had', 'VERB'),\n","  ('over-all', 'ADJ'),\n","  ('charge', 'NOUN'),\n","  ('of', 'ADP'),\n","  ('the', 'DET'),\n","  ('election', 'NOUN'),\n","  (',', '.'),\n","  ('``', '.'),\n","  ('deserves', 'VERB'),\n","  ('the', 'DET'),\n","  ('praise', 'NOUN'),\n","  ('and', 'CONJ'),\n","  ('thanks', 'NOUN'),\n","  ('of', 'ADP'),\n","  ('the', 'DET'),\n","  ('City', 'NOUN'),\n","  ('of', 'ADP'),\n","  ('Atlanta', 'NOUN'),\n","  (\"''\", '.'),\n","  ('for', 'ADP'),\n","  ('the', 'DET'),\n","  ('manner', 'NOUN'),\n","  ('in', 'ADP'),\n","  ('which', 'DET'),\n","  ('the', 'DET'),\n","  ('election', 'NOUN'),\n","  ('was', 'VERB'),\n","  ('conducted', 'VERB'),\n","  ('.', '.')]]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:28.899336Z","start_time":"2019-11-12T09:32:25.166107Z"},"colab_type":"code","id":"L2oUMrTpasZY","outputId":"e5969da9-f86c-4f90-d167-3faf18187454","executionInfo":{"status":"ok","timestamp":1574429591740,"user_tz":-60,"elapsed":12048,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(sents)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57340"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tgvacV45sNTz"},"source":["We divide our data set into a train and a valid part:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:28.905030Z","start_time":"2019-11-12T09:32:28.901963Z"},"colab_type":"code","id":"jPvFic-atE6S","colab":{}},"source":["valid_sents = sents[:5734]\n","train_sents = sents[5734:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zO8hXBHHPR4f"},"source":["# Feature template\n","\n","Since the plan is to build a CRF model, we need a __feature template__, which generates features for a word in a sentence (our sequence in the sequence tagging task). We use spaCy for feature extraction."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:36.258620Z","start_time":"2019-11-12T09:32:28.906793Z"},"id":"Y9y85R-DQVTA","colab_type":"code","colab":{}},"source":["#Spacy install, load and such stuff\n","\n","#Import\n","import spacy\n","#By model load, please deactivate unnecessary pipeline elements!\n","\n","nlp = spacy.load(\"en_core_web_sm\") "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TC4f13w9sNUJ"},"source":["We write a function which generates features for a token in a sentence, which is already a spaCy document. The feature vector is represented as a `dict` mapping feature names to their values.\n","\n","The desired **feature set for a token is**:\n","\n","- `bias`: A constant value of 1 as an input\n","- `token.lower`: the lowercased textual form of the token\n","- `token.suffix`: the textual form of the token's suffix as defined by SpaCy,\n","- `token.prefix`: the textual form of the token's prefix as defined by SpaCy,\n","- `token.is_upper`: boolean value indicating if the token is uppercase,\n","- `token.is_title`: boolean value indicating if the token is a title,\n","- `token.is_digit`: boolean value indicating if the token consists of numbers.\n","\n","These are only the `Token`'s own properties, but they represent no context.\n","\n","We would like to include information about  the previous and next words, as well as indicating if the `Token` is the beginning or the end of sentence.\n","\n","The **contextual features** should be:\n"," \n","- `-1:token.lower`: What is the lowercase textual form of the previous token?,\n","- `-1:token.is_title`: Is the previous token a title?,\n","- `-1:token.is_upper`: Is the previous token uppercase?,\n","- `+1:token.lower`: What is the lowercase textual form of the next token?,\n","- `+1:token.is_title`: Is the next token a title?,\n","- `+1:token.is_upper`: Is the next token uppercase?,\n","- `BOS`: Boolean value indicating if the token is the beginning of a sentence,\n","- `EOS`: Boolean value indicating if the token is the end of a sentence"]},{"cell_type":"code","metadata":{"id":"xxamVEmpSyoM","colab_type":"code","colab":{}},"source":["def token2features(sent, i):\n","    \"\"\"Return a feature dict for a token. \n","    sent is a spaCy Doc containing a sentence, i is the token's index in it.\n","    \"\"\"\n","\n","    token = sent[i]\n","\n","    features = {\n","        'bias': 1.0,\n","        'token.lower()': token.lower_,\n","        'token.suffix': token.suffix_,\n","        'token.prefix': token.prefix_,\n","        'token.isupper()': token.is_upper,\n","        'token.istitle()': token.is_title,\n","        'token.isdigit()': token.is_digit,\n","    }\n","    if i > 0:\n","        token1 = sent[i-1]\n","        features.update({\n","            '-1:token.lower()': token1.lower_,\n","            '-1:token.istitle()': token1.is_title,\n","        })\n","    else:\n","        features['BOS'] = True\n","\n","    if i < len(sent)-1:\n","        token1 = sent[i+1]\n","        features.update({\n","            '+1:token.lower()': token1.lower_,\n","            '+1:token.istitle()': token1.is_title,\n","            '+1:token.isupper()': token1.is_upper,\n","        })\n","    else:\n","        features['EOS'] = True\n","\n","    return features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VvwL0hF3sNUS"},"source":["For training, we will also need functions to generate feature dict and label lists for sentences in our training corpus:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:37.958184Z","start_time":"2019-11-12T09:32:37.936592Z"},"colab_type":"code","id":"ZLW80wtksNUU","colab":{}},"source":["  def sent2features(sent):\n","      \"Return a list of feature dicts for a sentence in the data set.\"\n","      # Create a doc by instantiating a Doc class and iterating through the sentence token by token.\n","      # Please bear in mind, that Brown has token-POS pairs, latter one we don't need here...\n","      # Plese use the above defined token2features function on each token to generate the features\n","      # For the whole sentence!\n","      doc1 = []\n","      res = [lis[0] for lis in sent] \n","      doc1 = spacy.tokens.Doc(nlp.vocab, words=res)\n","      return [token2features(doc1, i) for i in range(len(doc1))]\n","\n","  def sent2labels(sent):\n","      \n","      #Please create / filter only the labels for given sentence!\n","      label = []\n","      for token in sent:\n","        label.append(token[1])\n","      return label "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pBoPuzeMsNUa"},"source":["Sanity check: let's see the values for the first 2 tokens in the corpus:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:37.997140Z","start_time":"2019-11-12T09:32:37.966347Z"},"colab_type":"code","id":"UcqvDIJofccv","outputId":"57e2f9e3-42d1-4c21-cc43-6843cb1216f7","executionInfo":{"status":"ok","timestamp":1574431105072,"user_tz":-60,"elapsed":1075,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(sent2features(sents[0])[:2])\n","print(sent2labels(sents[0])[:2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[{'bias': 1.0, 'token.lower()': 'the', 'token.suffix': 'The', 'token.prefix': 'T', 'token.isupper()': False, 'token.istitle()': True, 'token.isdigit()': False, 'BOS': True, '+1:token.lower()': 'fulton', '+1:token.istitle()': True, '+1:token.isupper()': False}, {'bias': 1.0, 'token.lower()': 'fulton', 'token.suffix': 'ton', 'token.prefix': 'F', 'token.isupper()': False, 'token.istitle()': True, 'token.isdigit()': False, '-1:token.lower()': 'the', '-1:token.istitle()': True, '+1:token.lower()': 'county', '+1:token.istitle()': True, '+1:token.isupper()': False}]\n","['DET', 'NOUN']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KsoK0-GNfzt5"},"source":["# Putting the data into final form"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ylfst7VGsNUl"},"source":["Everything is ready to generate the training data in the form which is usable for the CRFsuite. Note that our inputs and labels will be  2-level representations, lists of lists, because we deal with token sequences (sentences)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:02.066506Z","start_time":"2019-11-12T09:32:38.005545Z"},"colab_type":"code","id":"Wfqa0feYgspT","outputId":"514e8e1c-25f0-4f00-f95c-876c744c27b0","executionInfo":{"status":"ok","timestamp":1574429629828,"user_tz":-60,"elapsed":50098,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","X_train = [sent2features(s) for s in train_sents]\n","y_train = [sent2labels(s) for s in train_sents]\n","\n","X_valid = [sent2features(s) for s in valid_sents]\n","y_valid = [sent2labels(s) for s in valid_sents]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 36.3 s, sys: 819 ms, total: 37.1 s\n","Wall time: 37.2 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:02.072026Z","start_time":"2019-11-12T09:33:02.068258Z"},"colab_type":"code","id":"XNFvu0UosNUt","outputId":"52f87a46-51d1-4cca-b855-673f61de15dc","executionInfo":{"status":"ok","timestamp":1574429629829,"user_tz":-60,"elapsed":50093,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["print(\"Feature dict for the first token in the first validation sentence:\")\n","print(X_valid[0][0])\n","print(\"Its label:\")\n","print(y_valid[0][0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Feature dict for the first token in the first validation sentence:\n","{'bias': 1.0, 'token.lower()': 'the', 'token.suffix': 'The', 'token.prefix': 'T', 'token.isupper()': False, 'token.istitle()': True, 'token.isdigit()': False, 'BOS': True, '+1:token.lower()': 'fulton', '+1:token.istitle()': True, '+1:token.isupper()': False}\n","Its label:\n","DET\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f2siQxe9k4ql"},"source":["# Training and evaluation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xwyn356ysNU2"},"source":["We use the super-optimized [CRFsuite](http://www.chokkan.org/software/crfsuite/) via the scikit-learn compatible [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) wrapper to train a CRF model on the data."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:04.830327Z","start_time":"2019-11-12T09:33:02.073675Z"},"colab_type":"code","id":"15POzt86sNSe","colab":{}},"source":["%%capture \n","# only to avoid ugly printouts during install\n","!pip install sklearn-crfsuite"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:04.883741Z","start_time":"2019-11-12T09:33:04.836395Z"},"colab_type":"code","id":"WkX57BFDklEL","colab":{}},"source":["# Please import and train an averaged perceptron model from CRFsuite and use it's custom metrics, \n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","# especially the multiple forms of accuracy score to evaluate the model!\n","crf = sklearn_crfsuite.CRF(\n","    algorithm='ap',\n","    max_iterations=500,\n","    all_possible_transitions=False,\n","    epsilon=1e-5\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jO7JoTjQVTb","colab_type":"code","outputId":"bfd96183-41b5-4a23-c0d9-ff01608ce9ed","executionInfo":{"status":"ok","timestamp":1574430101456,"user_tz":-60,"elapsed":521706,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Please draw some conclusion if this model is \"good enough\" \n","# in your view if you take token level and sentence level metrics into account!\n","#%%time\n","crf.fit(X_train, y_train)\n","\n","labels = list(crf.classes_)\n","\n","y_pred = crf.predict(X_valid)\n","metrics.flat_f1_score(y_valid, y_pred,average='weighted', labels=labels)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9724865413483401"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"OKIxybbgPIw4","colab_type":"code","outputId":"7f85dd28-3bf9-4561-aa32-3892603e8e55","executionInfo":{"status":"ok","timestamp":1574430102482,"user_tz":-60,"elapsed":522724,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["sorted_labels = sorted(\n","    labels,\n","    key=lambda name: (name[1:], name[0])\n",")\n","print(metrics.flat_classification_report(\n","    y_valid, y_pred, labels=sorted_labels, digits=3\n","))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           .      1.000     1.000     1.000     14377\n","           X      0.761     0.540     0.632       100\n","         ADJ      0.904     0.915     0.909      8525\n","         ADP      0.977     0.981     0.979     15138\n","         ADV      0.917     0.934     0.926      4438\n","        VERB      0.969     0.975     0.972     18229\n","         DET      0.995     0.995     0.995     14128\n","        CONJ      0.994     0.996     0.995      3435\n","        NOUN      0.976     0.968     0.972     36341\n","        PRON      0.985     0.989     0.987      3427\n","         PRT      0.938     0.924     0.931      2877\n","         NUM      0.975     0.973     0.974      2386\n","\n","    accuracy                          0.972    123401\n","   macro avg      0.949     0.932     0.939    123401\n","weighted avg      0.973     0.972     0.972    123401\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"63p9RtDhsNU_"},"source":["Let's instantiate and fit our model. CRFsuite implements several learning methods, here we use \"ap\", i.e., averaged perceptron."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"li8CXg67sNVc"},"source":["# Demonstration\n","\n","Just for the fun, we can try out the model."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:35:17.983727Z","start_time":"2019-11-12T09:35:17.965723Z"},"colab_type":"code","id":"JHoYAGHFsNVe","colab":{}},"source":["def predict_tags(sent):\n","    \"\"\"Predict tags for a sentence.\n","    sent is a string.\n","    \"\"\"\n","    doc = nlp(sent)\n","    return crf.predict([[token2features(doc, i) for i in range(len(doc))]])\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:35:37.676093Z","start_time":"2019-11-12T09:35:17.986500Z"},"colab_type":"code","id":"Ya59xso_z7uj","outputId":"09fa731b-2c39-4986-c55f-6b3048f1768e","executionInfo":{"status":"ok","timestamp":1574431781292,"user_tz":-60,"elapsed":44403,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":[" while True:\n","        sent = input(\"\\nEnter a sentence to tag or press return to quit:\\n\")\n","        if sent:\n","            print(predict_tags(sent))\n","        else:\n","            print(\"\\nEmpty input received -- bye!\")\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Enter a sentence to tag or press return to quit:\n","This is an English sentence!\n","[['DET', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN']]\n","\n","Enter a sentence to tag or press return to quit:\n","I am a Data Science enthusiast..\n","[['PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'NOUN', 'NUM']]\n","\n","Enter a sentence to tag or press return to quit:\n","\n","\n","Empty input received -- bye!\n"],"name":"stdout"}]}]}