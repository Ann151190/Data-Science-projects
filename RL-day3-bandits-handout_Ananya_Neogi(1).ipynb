{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"},"colab":{"name":"RL-day3-bandits-handout_Ananya_Neogi.ipynb","provenance":[{"file_id":"1mAKa9lsuPliwt3mqMI6DzqHT9ly2uSQK","timestamp":1583591157409}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ImA7sm_LY1fa","colab_type":"text"},"source":["# Multi-armed bandits"]},{"cell_type":"markdown","metadata":{"id":"xs98OJlPY1fd","colab_type":"text"},"source":["In this task you are going to implement ETC and UCB. Then you will play around the parameters in order to draw some more conclusion.\n","\n","Outline:\n","* Simulator writing\n","* ETC and experiments\n","* UCB and experiments"]},{"cell_type":"markdown","metadata":{"id":"K5_Pc97HY1fe","colab_type":"text"},"source":["## Bulding a simulator for the bandit"]},{"cell_type":"code","metadata":{"id":"DVK4REfxY1ff","colab_type":"code","colab":{}},"source":["from matplotlib import pyplot as plt\n","import numpy as np  # we can use this library for working with matrices"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlzCwhu8Y1fj","colab_type":"code","colab":{}},"source":["class Bandit:\n","    def __init__(self, k, means, rounds):\n","        # we assume Gaussian distributions with sigma=1\n","        self.k = k  # number of arms\n","        self.means = means\n","        self.rounds = rounds  # number of available rounds\n","        \n","        # ----- chose the optimal reward -----\n","        self.optimal_reward = np.amax(means)\n","        self.counter = 0\n","        # gather the empirical regret so far\n","        self.empirical_regret = 0\n","        self.emp_regrets = []\n","        # gather the expected regret so far\n","        self.expected_regret = 0\n","        self.exp_regrets = []\n","    \n","    def play_arm(self, arm):\n","        # ----- sample the appropriate reward -----\n","        \n","        reward = np.random.normal(self.means[:arm],1,_)\n","        # ----- calculate the regret so far and save it -----\n","\n","        self.exp_regrets.append((self.rounds*self.optimal_reward) - np.sum(np.sum(self.expected_regret,reward)/self.rounds))\n","        self.emp_regrets.append((self.rounds*self.optimal_reward) - np.sum(np.sum(self.empirical_regret,reward)/self.rounds))\n","\n","        return reward\n","    \n","    def finished(self):\n","        # ----- return if there is no more round remained -----\n","        if self.counter == self.rounds:\n","          return True \n","    \n","    def plot_regret(self):\n","        plt.plot(list(range(self.counter)), self.emp_regrets, 'b+', list(range(self.counter)), self.exp_regrets, 'ro')\n","        plt.xlabel(\"iteration\")\n","        plt.ylabel(\"regret\")\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThF7kupdY1fm","colab_type":"text"},"source":["## ETC algorithm"]},{"cell_type":"code","metadata":{"id":"TByELZ0jY1fn","colab_type":"code","colab":{}},"source":["class ETCsolver:\n","    def __init__(self, k, m, bandit):\n","        self.k = k  # number of arms\n","        self.m = m  # number of exploration rounds for each arm\n","        self.bandit = bandit\n","        \n","        # ----- create a cache storing the number of trials and the average rewards -----\n","        # for each action\n","        self.cache = np.zeros((k, 2))\n","    \n","    def _exploration_phase(self):\n","        counter = 0\n","        while counter < self.k * self.m:\n","            # ----- implement the exploration part -----\n","            # play the bandit and update cache\n","            cache = self.bandit.play_arm(self.k)\n","            counter+=1\n","    \n","    def _choose_best_action(self):\n","        # ----- we calculate the average reward of each arm -----\n","        # return the best arm\n","        if self.bandit.rounds <= self.k * self.m:\n","          avg_reward = np.mod(self.bandit.rounds,self.k) + 1\n","          best_arm = self.k\n","        else:\n","          avg_reward = np.amax(self.bandit.optimal_reward(self.k * self.m),self.bandit.rounds)\n","          best_arm = self.k\n","        return best_arm\n","    \n","    def run(self):\n","        self._exploration_phase()\n","        # after exploration we choose the best action\n","        optimal_arm = self._choose_best_action()\n","        # ----- play until finished -----\n","        return self.bandit.finished()\n","    \n","    def get_regret(self):\n","        return self.bandit.regrets\n","    \n","    def best_action(self):\n","        return self._choose_best_action() + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mf3eKA18Y1fq","colab_type":"code","colab":{}},"source":["def experiment_etc(k, mu, m, rounds):\n","    bandit = Bandit(k, mu, rounds)\n","    etc = ETCsolver(k, m, bandit)\n","\n","    etc.run()\n","    etc.bandit.plot_regret()\n","    print(\"Optimal action: {}\".format(etc.best_action()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-2A4q3cY1ft","colab_type":"text"},"source":["## UCB algorithm"]},{"cell_type":"code","metadata":{"id":"rlKfqncnY1fu","colab_type":"code","colab":{}},"source":["class UCBsolver:\n","    def __init__(self, k, delta, bandit):\n","        self.k = k  # number of actions\n","        self.delta = delta  # error probability\n","        self.bandit = bandit\n","\n","        self.cache = np.zeros((k, 2))  # stores the number and rewards so far\n","        self.actions = []\n","    \n","    def _init_phase(self):\n","        # at the very beginning each arm is equally good\n","        # unexplored arms are always the best\n","        # ----- pick each arm once to initialize the cache -----\n","        # we want to avoid division by zero\n","        for arm in range(self.k):\n","            # TODO\n","            self.cache = np.minimum(0,self.bandit.play_arm(arm))\n","    def _choose_best_action(self):\n","        # this implements the score for ucb\n","        # ----- first is the average reward term -----\n","        \n","        # ----- second is the exploration term -----\n","        for i in self.actions:\n","          avg_reward = np.amax(i-1, self.delta)\n","          best_arm = self.k\n","        return best_arm\n","    \n","    def run(self):\n","        self._init_phase()\n","        # ----- while not finished -----\n","\n","            # ----- chooe optimal arm -----\n","\n","            # ----- storing the actions so far -----\n","\n","            # ----- playing the chosen arm -----\n","\n","            # ----- update cache -----\n","    \n","    def plot_actions(self):\n","        plt.plot(list(range(len(self.actions))), self.actions, 'r+')\n","        plt.xlabel(\"iteration\")\n","        plt.ylabel(\"chosen action\")\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQrwBHPjY1fx","colab_type":"code","colab":{}},"source":["def experiment_ucb(k, mu, delta, rounds):\n","    bandit = Bandit(k, mu, rounds)\n","    ucb = UCBsolver(k, delta, bandit)\n","\n","    ucb.run()\n","    ucb.bandit.plot_regret()\n","    ucb.plot_actions()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ik35OXTcY1f0","colab_type":"text"},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{"id":"3sUijcYeY1f1","colab_type":"text"},"source":["### ETC use-cases"]},{"cell_type":"code","metadata":{"id":"aVVHOV0XY1f2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":307},"outputId":"8741b057-4480-4d09-aa21-9db7e6bba6f8","executionInfo":{"status":"error","timestamp":1584290579447,"user_tz":-60,"elapsed":538,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTHjYcXXNrOZ6SLog2w_GLXic8EcvHX0vSMTYyeQ=s64","userId":"00462567981712017677"}}},"source":["# Try it with two arms and small difference between the mean values\n","# What are the required number of rounds to find the best action?\n","case1 = experiment_etc(2, 0.2, 2, 5)"],"execution_count":27,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-7bfbf109ccde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcase1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_etc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-4ad22b626953>\u001b[0m in \u001b[0;36mexperiment_etc\u001b[0;34m(k, mu, m, rounds)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0metc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mETCsolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0metc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0metc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_regret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal action: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-570a343bba05>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exploration_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# after exploration we choose the best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimal_arm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-570a343bba05>\u001b[0m in \u001b[0;36m_exploration_phase\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# ----- implement the exploration part -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# play the bandit and update cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-4809050c2f60>\u001b[0m in \u001b[0;36mplay_arm\u001b[0;34m(self, arm)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# ----- sample the appropriate reward -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0marm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# ----- calculate the regret so far and save it -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"]}]},{"cell_type":"code","metadata":{"id":"p53DOJ4vY1f4","colab_type":"code","colab":{}},"source":["# Try it with two arms and big difference between the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwND7dukY1f7","colab_type":"code","colab":{}},"source":["# Try it with five arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU-tTDMQY1f-","colab_type":"code","colab":{}},"source":["# Try it with fifty arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6AsS5KtYY1gA","colab_type":"text"},"source":["### UCB use-cases"]},{"cell_type":"code","metadata":{"id":"Rsoh5JSbY1gB","colab_type":"code","colab":{}},"source":["# Try it with two arms and small difference between the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta? \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjsEa_EyY1gE","colab_type":"code","colab":{}},"source":["# Try it with two arms and big difference between the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta? \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvE55zQ1Y1gG","colab_type":"code","colab":{}},"source":["# Try it with five arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta? \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sidtKTmY1gJ","colab_type":"code","colab":{}},"source":["# Try it with fifty arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta?\n","\n"],"execution_count":0,"outputs":[]}]}