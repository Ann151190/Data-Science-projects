{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"POS_tagging_with_classical_models_handout.ipynb","provenance":[{"file_id":"14tKuuSDCszWD2v1bY97NrR6UdYWYozEy","timestamp":1573908483791}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"no_k76e0NZHI"},"source":["# Task: Part of speech tagging\n","\n","In this task we try to recreate a very rudimentary POS tagger \"from scratch\" using SpaCy and CRF models. \n","\n","(We disregard the fact, that SpaCy has a built in POS tagger for the moment for demonstration purposes.)\n","\n","The input is a tokenized English sentence. The task is to label each word with a part of speech (POS) tag. The tag set, which is identical the [Universal Dependencies project's](https://universaldependencies.org/) basic tag set is the following:\n","\n","- NOUN: noun\n","- VERB: verb\n","- DET: determiner\n","- ADJ: adjective\n","- ADP: adposition (e.g., prepositions)\n","- ADV: adverb\n","- CONJ: conjunction\n","- NUM: numeral\n","- PART: particle (function word that cannot be inflected, has no meaning in\n","  itself and doesn't fit elsewhere, e.g., \"to\")\n","- PRON: pronoun\n","- .: punctuation\n","- X: other\n","\n","The code in this task is an adaptation of the NER code in the sklearn-crfsuite documentation.\n","\n","# The data set\n","\n","__Brown__ corpus: \"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961\" (Wikpedia: Brown Corpus)\n","\n","Let's download and inspect the data!"]},{"cell_type":"code","metadata":{"id":"qfHhoKYsQVSs","colab_type":"code","colab":{}},"source":["%%capture\n","!pip install nltk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:20.712982Z","start_time":"2019-11-12T09:32:16.093693Z"},"colab_type":"code","id":"KAPwh8mmNZHM","outputId":"353789f9-5f43-41d3-c16a-ecd59c5f6777","executionInfo":{"status":"ok","timestamp":1574084476872,"user_tz":-60,"elapsed":8419,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","\n","from nltk.corpus import brown\n","nltk.download('brown')\n","\n","brown.words()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:21.291370Z","start_time":"2019-11-12T09:32:20.723897Z"},"colab_type":"code","id":"3kSgq4e0NZHi","outputId":"42028ce2-f289-46ea-b956-f1d0871d5589","executionInfo":{"status":"ok","timestamp":1574084476874,"user_tz":-60,"elapsed":8398,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["nltk.download('universal_tagset')\n","brown.tagged_words(tagset='universal')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'), ('Fulton', 'NOUN'), ...]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:21.352042Z","start_time":"2019-11-12T09:32:21.297210Z"},"colab_type":"code","id":"9tT1DBTtNZHu","outputId":"77729a8f-8d5e-4880-9428-1ce8cd4980cf","executionInfo":{"status":"ok","timestamp":1574084476876,"user_tz":-60,"elapsed":8366,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["brown.sents()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:25.131849Z","start_time":"2019-11-12T09:32:21.365202Z"},"colab_type":"code","id":"UuwBB-XRNZH6","outputId":"6db38a77-462a-49ff-bb72-46908313d437","executionInfo":{"status":"ok","timestamp":1574084479450,"user_tz":-60,"elapsed":10909,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(brown.words())"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1161192"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m8VYZmFCsNTa"},"source":["From the brown the object provided by NLTK we will work with the tagged sentence list:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:25.163453Z","start_time":"2019-11-12T09:32:25.136038Z"},"colab_type":"code","id":"bh0wAJkWNnlV","outputId":"9d92911b-1627-4bb4-c788-b28fbeec9adf","executionInfo":{"status":"ok","timestamp":1574084479452,"user_tz":-60,"elapsed":10843,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["sents = brown.tagged_sents(tagset=\"universal\")\n","\n","sents[:2]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('The', 'DET'),\n","  ('Fulton', 'NOUN'),\n","  ('County', 'NOUN'),\n","  ('Grand', 'ADJ'),\n","  ('Jury', 'NOUN'),\n","  ('said', 'VERB'),\n","  ('Friday', 'NOUN'),\n","  ('an', 'DET'),\n","  ('investigation', 'NOUN'),\n","  ('of', 'ADP'),\n","  (\"Atlanta's\", 'NOUN'),\n","  ('recent', 'ADJ'),\n","  ('primary', 'NOUN'),\n","  ('election', 'NOUN'),\n","  ('produced', 'VERB'),\n","  ('``', '.'),\n","  ('no', 'DET'),\n","  ('evidence', 'NOUN'),\n","  (\"''\", '.'),\n","  ('that', 'ADP'),\n","  ('any', 'DET'),\n","  ('irregularities', 'NOUN'),\n","  ('took', 'VERB'),\n","  ('place', 'NOUN'),\n","  ('.', '.')],\n"," [('The', 'DET'),\n","  ('jury', 'NOUN'),\n","  ('further', 'ADV'),\n","  ('said', 'VERB'),\n","  ('in', 'ADP'),\n","  ('term-end', 'NOUN'),\n","  ('presentments', 'NOUN'),\n","  ('that', 'ADP'),\n","  ('the', 'DET'),\n","  ('City', 'NOUN'),\n","  ('Executive', 'ADJ'),\n","  ('Committee', 'NOUN'),\n","  (',', '.'),\n","  ('which', 'DET'),\n","  ('had', 'VERB'),\n","  ('over-all', 'ADJ'),\n","  ('charge', 'NOUN'),\n","  ('of', 'ADP'),\n","  ('the', 'DET'),\n","  ('election', 'NOUN'),\n","  (',', '.'),\n","  ('``', '.'),\n","  ('deserves', 'VERB'),\n","  ('the', 'DET'),\n","  ('praise', 'NOUN'),\n","  ('and', 'CONJ'),\n","  ('thanks', 'NOUN'),\n","  ('of', 'ADP'),\n","  ('the', 'DET'),\n","  ('City', 'NOUN'),\n","  ('of', 'ADP'),\n","  ('Atlanta', 'NOUN'),\n","  (\"''\", '.'),\n","  ('for', 'ADP'),\n","  ('the', 'DET'),\n","  ('manner', 'NOUN'),\n","  ('in', 'ADP'),\n","  ('which', 'DET'),\n","  ('the', 'DET'),\n","  ('election', 'NOUN'),\n","  ('was', 'VERB'),\n","  ('conducted', 'VERB'),\n","  ('.', '.')]]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:28.899336Z","start_time":"2019-11-12T09:32:25.166107Z"},"colab_type":"code","id":"L2oUMrTpasZY","outputId":"c47a0215-076f-4841-9909-6fe01279a26d","executionInfo":{"status":"ok","timestamp":1574084482467,"user_tz":-60,"elapsed":13778,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(sents)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57340"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tgvacV45sNTz"},"source":["We divide our data set into a train and a valid part:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:28.905030Z","start_time":"2019-11-12T09:32:28.901963Z"},"colab_type":"code","id":"jPvFic-atE6S","colab":{}},"source":["valid_sents = sents[:5734]\n","train_sents = sents[5734:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zO8hXBHHPR4f"},"source":["# Feature template\n","\n","Since the plan is to build a CRF model, we need a __feature template__, which generates features for a word in a sentence (our sequence in the sequence tagging task). We use spaCy for feature extraction."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:36.258620Z","start_time":"2019-11-12T09:32:28.906793Z"},"id":"Y9y85R-DQVTA","colab_type":"code","colab":{}},"source":["#Spacy install, load and such stuff\n","\n","#Import\n","import spacy\n","#By model load, please deactivate unnecessary pipeline elements!\n","\n","en = spacy.load(\"en\") "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TC4f13w9sNUJ"},"source":["We write a function which generates features for a token in a sentence, which is already a spaCy document. The feature vector is represented as a `dict` mapping feature names to their values.\n","\n","The desired **feature set for a token is**:\n","\n","- `bias`: A constant value of 1 as an input\n","- `token.lower`: the lowercased textual form of the token\n","- `token.suffix`: the textual form of the token's suffix as defined by SpaCy,\n","- `token.prefix`: the textual form of the token's prefix as defined by SpaCy,\n","- `token.is_upper`: boolean value indicating if the token is uppercase,\n","- `token.is_title`: boolean value indicating if the token is a title,\n","- `token.is_digit`: boolean value indicating if the token consists of numbers.\n","\n","These are only the `Token`'s own properties, but they represent no context.\n","\n","We would like to include information about  the previous and next words, as well as indicating if the `Token` is the beginning or the end of sentence.\n","\n","The **contextual features** should be:\n"," \n","- `-1:token.lower`: What is the lowercase textual form of the previous token?,\n","- `-1:token.is_title`: Is the previous token a title?,\n","- `-1:token.is_upper`: Is the previous token uppercase?,\n","- `+1:token.lower`: What is the lowercase textual form of the next token?,\n","- `+1:token.is_title`: Is the next token a title?,\n","- `+1:token.is_upper`: Is the next token uppercase?,\n","- `BOS`: Boolean value indicating if the token is the beginning of a sentence,\n","- `EOS`: Boolean value indicating if the token is the end of a sentence"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:37.930451Z","start_time":"2019-11-12T09:32:37.909065Z"},"colab_type":"code","id":"SKz9zT8bsNUL","colab":{}},"source":["def token2features(sent, i):\n","    \"\"\"Return a feature dict for a token. \n","    sent is a spaCy Doc containing a sentence, i is the token's index in it.\n","    \"\"\"\n","\n","    token = sent[i]\n","    postag = sent[i][1]\n","\n","    features = {\n","        'bias': 1.0,\n","        'token.lower()': token.lower(),\n","        'token.suffix': token[-3:],\n","        'token.prefix': token[-3:-2],\n","        'token.isupper()': token.isupper(),\n","        'token.istitle()': token.istitle(),\n","        'token.isdigit()': token.isdigit(),\n","        'postag': postag,\n","        'postag[:2]': postag[:2],\n","    }\n","    if i > 0:\n","        token1 = sent[i-1][0]\n","        postag1 = sent[i-1][1]\n","        features.update({\n","            '-1:token.lower()': token1.lower(),\n","            '-1:token.istitle()': token1.istitle(),\n","            '-1:token.isupper()': token1.isupper(),\n","            '-1:postag': postag1,\n","            '-1:postag[:2]': postag1[:2],\n","        })\n","    else:\n","        features['BOS'] = True\n","\n","    if i < len(sent)-1:\n","        token1 = sent[i+1][0]\n","        postag1 = sent[i+1][1]\n","        features.update({\n","            '+1:token.lower()': token1.lower(),\n","            '+1:token.istitle()': token1.istitle(),\n","            '+1:token.isupper()': token1.isupper(),\n","            '+1:postag': postag1,\n","            '+1:postag[:2]': postag1[:2],\n","        })\n","    else:\n","        features['EOS'] = True\n","\n","    return features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VvwL0hF3sNUS"},"source":["For training, we will also need functions to generate feature dict and label lists for sentences in our training corpus:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:37.958184Z","start_time":"2019-11-12T09:32:37.936592Z"},"colab_type":"code","id":"ZLW80wtksNUU","colab":{}},"source":["def sent2features(sent):\n","    \"Return a list of feature dicts for a sentence in the data set.\"\n","    # Create a doc by instantiating a Doc class and iterating through the sentence token by token.\n","    # Please bear in mind, that Brown has token-POS pairs, latter one we don't need here...\n","    # Plese use the above defined token2features function on each token to generate the features\n","    # For the whole sentence!\n","    return [token2features(sent, i) for i in range(len(sent))]\n","\n","def sent2labels(sent):\n","    \n","    #Please create / filter only the labels for given sentence!\n","    label = []\n","    for token, postag in sent:\n","      label.append(postag)\n","    return label "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pBoPuzeMsNUa"},"source":["Sanity check: let's see the values for the first 2 tokens in the corpus:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:32:37.997140Z","start_time":"2019-11-12T09:32:37.966347Z"},"colab_type":"code","id":"UcqvDIJofccv","outputId":"e0629f79-06ca-4b0f-f0c8-158f560b8491","executionInfo":{"status":"ok","timestamp":1574092460570,"user_tz":-60,"elapsed":1200,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(sent2features(sents[0])[:2])\n","print(sent2labels(sents[0])[:2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[{'bias': 1.0, 'token.lower()': 'the', 'token.suffix': 'The', 'token.prefix': 'T', 'token.isupper()': False, 'token.istitle()': True, 'token.isdigit()': False, 'postag': 'DET', 'postag[:2]': 'DE', 'BOS': True, '+1:token.lower()': 'fulton', '+1:token.istitle()': True, '+1:token.isupper()': False, '+1:postag': 'NOUN', '+1:postag[:2]': 'NO'}, {'bias': 1.0, 'token.lower()': 'fulton', 'token.suffix': 'ton', 'token.prefix': 't', 'token.isupper()': False, 'token.istitle()': True, 'token.isdigit()': False, 'postag': 'NOUN', 'postag[:2]': 'NO', '-1:token.lower()': 'the', '-1:token.istitle()': True, '-1:token.isupper()': False, '-1:postag': 'DET', '-1:postag[:2]': 'DE', '+1:token.lower()': 'county', '+1:token.istitle()': True, '+1:token.isupper()': False, '+1:postag': 'NOUN', '+1:postag[:2]': 'NO'}]\n","['DET', 'NOUN']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KsoK0-GNfzt5"},"source":["# Putting the data into final form"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ylfst7VGsNUl"},"source":["Everything is ready to generate the training data in the form which is usable for the CRFsuite. Note that our inputs and labels will be  2-level representations, lists of lists, because we deal with token sequences (sentences)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:02.066506Z","start_time":"2019-11-12T09:32:38.005545Z"},"colab_type":"code","id":"Wfqa0feYgspT","outputId":"3c1f6b27-7591-4f1e-f107-ac6c8083a1c4","executionInfo":{"status":"ok","timestamp":1574092490761,"user_tz":-60,"elapsed":12687,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","X_train = [sent2features(s) for s in train_sents]\n","y_train = [sent2labels(s) for s in train_sents]\n","\n","X_valid = [sent2features(s) for s in valid_sents]\n","y_valid = [sent2labels(s) for s in valid_sents]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 10.6 s, sys: 966 ms, total: 11.5 s\n","Wall time: 11.6 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:02.072026Z","start_time":"2019-11-12T09:33:02.068258Z"},"colab_type":"code","id":"XNFvu0UosNUt","outputId":"7909495a-03d7-439f-9609-c4be03ddeac1","executionInfo":{"status":"ok","timestamp":1574092495912,"user_tz":-60,"elapsed":1072,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["print(\"Feature dict for the first token in the first validation sentence:\")\n","print(X_valid[0][0])\n","print(\"Its label:\")\n","print(y_valid[0][0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Feature dict for the first token in the first validation sentence:\n","{'bias': 1.0, 'token.lower()': 'the', 'token.suffix': 'The', 'token.prefix': 'T', 'token.isupper()': False, 'token.istitle()': True, 'token.isdigit()': False, 'postag': 'DET', 'postag[:2]': 'DE', 'BOS': True, '+1:token.lower()': 'fulton', '+1:token.istitle()': True, '+1:token.isupper()': False, '+1:postag': 'NOUN', '+1:postag[:2]': 'NO'}\n","Its label:\n","DET\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f2siQxe9k4ql"},"source":["# Training and evaluation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xwyn356ysNU2"},"source":["We use the super-optimized [CRFsuite](http://www.chokkan.org/software/crfsuite/) via the scikit-learn compatible [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) wrapper to train a CRF model on the data."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:04.830327Z","start_time":"2019-11-12T09:33:02.073675Z"},"colab_type":"code","id":"15POzt86sNSe","outputId":"949505cc-a12d-4df0-9be1-800a671f59ac","executionInfo":{"status":"ok","timestamp":1574090999841,"user_tz":-60,"elapsed":7567,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#%%capture # only to avoid ugly printouts during install\n","!pip install sklearn-crfsuite"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting sklearn-crfsuite\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.28.1)\n","Collecting python-crfsuite>=0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.12.0)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite\n","Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:33:04.883741Z","start_time":"2019-11-12T09:33:04.836395Z"},"colab_type":"code","id":"WkX57BFDklEL","colab":{}},"source":["# Please import and train an averaged perceptron model from CRFsuite and use it's custom metrics, \n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","# especially the multiple forms of accuracy score to evaluate the model!\n","crf = sklearn_crfsuite.CRF(\n","    algorithm='lbfgs',\n","    c1=0.1,\n","    c2=0.1,\n","    max_iterations=100,\n","    all_possible_transitions=True\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jO7JoTjQVTb","colab_type":"code","outputId":"ee442bf7-6cbb-476b-bcc1-ddb18a6508d0","executionInfo":{"status":"ok","timestamp":1574093013361,"user_tz":-60,"elapsed":118010,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Please draw some conclusion if this model is \"good enough\" \n","# in your view if you take token level and sentence level metrics into account!\n","%%time\n","crf.fit(X_train, y_train)\n","\n","labels = list(crf.classes_)\n","#labels.remove('O')\n","\n","y_pred = crf.predict(X_valid)\n","#metrics.flat_f1_score(y_valid, y_pred,average='weighted', labels=labels)\n","metrics.flat_accuracy_score(y_valid, y_pred)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 56s, sys: 111 ms, total: 1min 56s\n","Wall time: 1min 57s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OKIxybbgPIw4","colab_type":"code","outputId":"4f51bd40-4f19-49cd-8943-a42ca4a17a17","executionInfo":{"status":"ok","timestamp":1574093127294,"user_tz":-60,"elapsed":1845,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["sorted_labels = sorted(\n","    labels,\n","    key=lambda name: (name[1:], name[0])\n",")\n","print(metrics.flat_classification_report(\n","    y_valid, y_pred, labels=sorted_labels, digits=3\n","))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           .      1.000     1.000     1.000     14377\n","           X      1.000     1.000     1.000       100\n","         ADJ      1.000     1.000     1.000      8525\n","         ADP      1.000     1.000     1.000     15138\n","         ADV      1.000     1.000     1.000      4438\n","        VERB      1.000     1.000     1.000     18229\n","         DET      1.000     1.000     1.000     14128\n","        CONJ      1.000     1.000     1.000      3435\n","        NOUN      1.000     1.000     1.000     36341\n","        PRON      1.000     1.000     1.000      3427\n","         PRT      1.000     1.000     1.000      2877\n","         NUM      1.000     1.000     1.000      2386\n","\n","    accuracy                          1.000    123401\n","   macro avg      1.000     1.000     1.000    123401\n","weighted avg      1.000     1.000     1.000    123401\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"63p9RtDhsNU_"},"source":["Let's instantiate and fit our model. CRFsuite implements several learning methods, here we use \"ap\", i.e., averaged perceptron."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"li8CXg67sNVc"},"source":["# Demonstration\n","\n","Just for the fun, we can try out the model."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:35:17.983727Z","start_time":"2019-11-12T09:35:17.965723Z"},"colab_type":"code","id":"JHoYAGHFsNVe","colab":{}},"source":["def predict_tags(sent):\n","    \"\"\"Predict tags for a sentence.\n","    sent is a string.\n","    \"\"\"\n","    doc = en(sent)\n","    return crf.predict([[token2features(doc, i) for i in range(len(doc))]])\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-12T09:35:37.676093Z","start_time":"2019-11-12T09:35:17.986500Z"},"colab_type":"code","id":"Ya59xso_z7uj","outputId":"e00d6654-074f-495d-8019-a55f8031f9b6","executionInfo":{"status":"error","timestamp":1574092756888,"user_tz":-60,"elapsed":14278,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"source":[" while True:\n","        sent = input(\"\\nEnter a sentence to tag or press return to quit:\\n\")\n","        if sent:\n","            print(predict_tags(sent))\n","        else:\n","            print(\"\\nEmpty input received -- bye!\")\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Enter a sentence to tag or press return to quit:\n","This is an English sentence\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-6e9683f3a8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m        \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter a sentence to tag or press return to quit:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEmpty input received -- bye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-056eacc46e60>\u001b[0m in \u001b[0;36mpredict_tags\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-056eacc46e60>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-4dc09fb7b36c>\u001b[0m in \u001b[0;36mtoken2features\u001b[0;34m(sent, i)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mspaCy\u001b[0m \u001b[0mDoc\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0ma\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpostag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object does not support indexing"]}]},{"cell_type":"markdown","metadata":{"id":"3lOQav13K7kH","colab_type":"text"},"source":["https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#features"]}]}