{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Copy of Minimal_chat_with_SpaCy_and_WordNet_handout_VictorJohannesHoll.ipynb","provenance":[{"file_id":"1tlfP9DvtphNNsT624ZfLSv1tjWKvDsfP","timestamp":1575339950543}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"d4aTCZPK-1mh","colab_type":"text"},"source":["# Building basic chatbots with rules, syntax and semantic nets\n","\n","It is increasingly often, that companies would like to automate internal or customer facing tasks via a chat interface. Though there are mature frameworks (like [RASA](https://rasa.com/)) or services (like [Microsoft Bot Framework](https://dev.botframework.com/) or [Chatfuel](https://chatfuel.com/)), we will attempt to set up a basic analysis pipeline based on SpaCy and WordNet, that can give us some coverage in a basic banking scenario.   \n","\n","We will use SpaCy for our basic analysis (including syntax), as well as a simple addon, that connects it to WordNet called unsurprisingly [SpaCy-WordNet](https://spacy.io/universe/project/spacy-wordnet).\n","\n","Let's take the following texts as a problem:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:51.229539Z","start_time":"2019-11-11T11:45:51.225013Z"},"id":"Hn7KlMMZ-1mm","colab_type":"code","colab":{}},"source":["test_texts = [\n","    \"I would like to deposit 5000 euros.\",\n","    \"I would like to put in 5000 euros.\",\n","    \"I would like to pay in 5000 euros.\",\n","    \"I would like to pay up 5000 EUR.\",\n","    \"Can I pay in 5000 euros, please?\",\n","    \n","    \n","    \"I would like to deposit money.\",\n","    \n","\n","    \"I am about to take out 5000 euros.\",\n","    \"I am about to get out 5000 euros.\",\n","    \"I am about to withdraw 5000 euros.\",\n","    \"I want to withdraw 5000 USD.\",\n","    \"Can I withdraw $ 5000.\",\n","\n","    \n","    \"Can I check my account, please?\",\n","    \"May I see my balance, please?\",\n","    \"Could I query my account, please?\",\n","    \"I would like to see my account balance.\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RgcSDIkC-1mw","colab_type":"text"},"source":["## Let's try some syntactic analysis!\n","\n","The first goal is to see, if we can filter out, based on some common POS / Dependency structure the \"main message\", the things that people would like to say with the sentences above.\n","\n","The expected output based on our analysis would be something like:"]},{"cell_type":"markdown","metadata":{"id":"wmnc-dbC-1my","colab_type":"text"},"source":["```\n","[deposit, 5000, euros]\n","[put, in, 5000, euros]\n","[pay, in, 5000, euros]\n","[pay, up, 5000EUR]\n","[pay, in, 5000, euros]\n","[deposit, money]\n","[take, out, 5000, euros]\n","----- No success in parsing. Original: I am about to get out 5000 euros.\n","[withdraw, 5000, euros]\n","[withdraw, 5000, USD]\n","[withdraw, $, 5000]\n","[check, my, account]\n","[see, my, balance]\n","[query, my, account]\n","[see, my, account, balance]\n","```"]},{"cell_type":"markdown","metadata":{"id":"DIwZDnow-1m1","colab_type":"text"},"source":["### Preliminaries: install SpaCy and initialize model"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:54.885571Z","start_time":"2019-11-11T11:45:51.232265Z"},"id":"_mtcEZw4-1m5","colab_type":"code","outputId":"7b8c1d98-aeb8-474d-d61a-71aebf388e0c","executionInfo":{"status":"ok","timestamp":1575341215047,"user_tz":-60,"elapsed":8671,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["!python -m spacy download en_core_web_sm"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.737042Z","start_time":"2019-11-11T11:45:54.894640Z"},"id":"pHeys4lK-1m-","colab_type":"code","colab":{}},"source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.759307Z","start_time":"2019-11-11T11:45:55.738655Z"},"id":"FNzGmqm3-1nN","colab_type":"code","colab":{}},"source":["# We create one document out of the array of sentences for convenience.\n","long_text = \" \".join(test_texts)\n","\n","doc = nlp(long_text) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFpJje0v-1nT","colab_type":"text"},"source":["### Try some syntactic matching on the texts!\n","\n","Let us use the syntactic analysis of SpaCy to get to the \"core\" of the sentences!\n","\n","Let us assume, that we are interested in **verbs** and their **minimal subtrees**!\n","\n","Please\n","\n","1. look for the verbs in the sentences, \n","2. get their subtrees,\n","3. delete every token from the \"left\" of the verb\n","4. from the \"right\" subtree, filter interjections and punctuations,\n","5. keep the shortest such subtree from the sentence and print it out!\n","\n","For the visualization of the sentence tree use [DisplaCy](https://spacy.io/usage/visualizers)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.767176Z","start_time":"2019-11-11T11:45:55.760799Z"},"id":"qq0T_x6U-1nV","colab_type":"code","outputId":"5404a200-bf4e-4919-c7d3-bf71aa0b850f","executionInfo":{"status":"ok","timestamp":1575341216740,"user_tz":-60,"elapsed":10350,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["for sentence in doc.sents:\n","    sentence_subtree = []\n","    for token in sentence:\n","      if token.pos_ in ['VERB']:\n","        subtree_list = [leaf for leaf in token.subtree if (leaf.pos_ not in ['INJ', 'PUNCT'])]\n","        #subtree_list = list(token.subtree)\n","        endsubtree_list = subtree_list[token.n_lefts:]\n","        without_list = []\n","       # for token1 in subtree_list:\n","       #   if token1.pos_ not in ['INJ', 'PUNCT']:\n","       #     without_list.append(token1)\n","        if len(endsubtree_list)>1:\n","          sentence_subtree.append(endsubtree_list)\n","    sentence_subtree = [tok for tok in sentence_subtree if len(tok)==min([len(endsubtree_list) for endsubtree_list in sentence_subtree])]\n","    #print(withoutlist)\n","    for sent in sentence_subtree:\n","      sentence_subtree = sent\n","    if sentence_subtree:\n","        print(sentence_subtree)\n","    else:\n","        print(\"----- No success in parsing. Original:\",sentence)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[deposit, 5000, euros]\n","[put, in, 5000, euros]\n","[pay, in, 5000, euros]\n","[pay, up, 5000, EUR]\n","[pay, in, 5000, euros, please]\n","[deposit, money]\n","[take, out, 5000, euros]\n","[get, out, 5000, euros]\n","[withdraw, 5000, euros]\n","[withdraw, 5000, USD]\n","[withdraw, $, 5000]\n","[check, my, account, please]\n","[see, my, balance, please]\n","[query, my, account, please]\n","[see, my, account, balance]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y4NQMVrv-1nd","colab_type":"text"},"source":["As we can see, even in this simple case, some noise remains, that is: with our method we can not achieve success by sentence 8. Please observe, and let's discuss, why!"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.786989Z","start_time":"2019-11-11T11:45:55.769377Z"},"id":"76XVrYqT-1ng","colab_type":"code","outputId":"d5ea1efb-e37f-48d8-e733-46e86bbdcf23","executionInfo":{"status":"ok","timestamp":1575341216741,"user_tz":-60,"elapsed":10342,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":421}},"source":["from spacy import displacy\n","\n","doc=nlp(test_texts[7])\n","\n","displacy.render(doc, style=\"dep\", jupyter = True)\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f273954c3ddb43e5b2c0a8220ac92aa5-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">about</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">get</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">out</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">5000</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">euros.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-f273954c3ddb43e5b2c0a8220ac92aa5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","</svg>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3WY9GL8m-1nn","colab_type":"text"},"source":["It is worth noting, that some addon libraries, like [Textacy](https://spacy.io/universe/project/textacy) have built in functions that can come in handy in these topics.\n","\n","Like:\n","\n","`textacy.spacier.utils.get_main_verbs_of_sent(sent)`\n","Return the main (non-auxiliary) verbs in a sentence.\n","\n","`textacy.spacier.utils.get_subjects_of_verb(verb)`\n","Return all subjects of a verb according to the dependency parse.\n","\n","`textacy.spacier.utils.get_objects_of_verb(verb)`\n","Return all objects of a verb according to the dependency parse, including open clausal complements.\n","\n","`textacy.spacier.utils.get_span_for_compound_noun(noun)`\n","Return document indexes spanning all (adjacent) tokens in a compound noun.\n","\n","`textacy.spacier.utils.get_span_for_verb_auxiliaries(verb)`\n","Return document indexes spanning all (adjacent) tokens around a verb that are auxiliary verbs or negations.\n","\n","None the less, if we want to carry out some definite actions for these sentences, we have to try another route."]},{"cell_type":"markdown","metadata":{"id":"caStnHy1-1nq","colab_type":"text"},"source":["## Second try: detecting \"intents\" and \"entities\" with the help of WordNet\n","\n","In processing chat utterances, the two common tasks are to:\n","\n","1. Detect the overall intent of the given utterance\n","2. Extract some key parameters needed for action.\n","\n","The first is called **\"intent detection\"** the second **\"entity extraction\"**.\n","\n","More on this can be found in the Theory section on chatbots, discussed later.\n","\n","Though the standard practice for the first step is to build up a sentence classifier, and the second is done usually with some token level classifier / matching, now we will utilize the same rule based matching mechanism of SpaCy that we did before, albeit with a twist.\n","\n","One of the main problems, as we saw before is the **variety of utterances**, that is, people tend to formulate the same intent in myriad ways. We will intend to mitigate this by **increasing coverage with WorNet synonyms**.\n","\n","For this we need a connection between our analysis pipeline and WordNet. Luckily, we have it as an extension.\n"]},{"cell_type":"markdown","metadata":{"id":"rwkpqDKs-1nr","colab_type":"text"},"source":["### Install extension and register it to the pipeline"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:56.816094Z","start_time":"2019-11-11T11:45:55.788687Z"},"id":"8aHhTqrx-1nt","colab_type":"code","outputId":"86d73937-36ec-4e57-ad05-eb2209c0f330","executionInfo":{"status":"ok","timestamp":1575341227468,"user_tz":-60,"elapsed":21059,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["!pip install spacy-wordnet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting spacy-wordnet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f2/4d8070df0f7a7a9eeed74eb7e9ce3cf41349eb5e06b1e088de9eeca630e2/spacy-wordnet-0.0.4.tar.gz (648kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 655kB 2.8MB/s \n","\u001b[?25hCollecting nltk<3.4,>=3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 47.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.12.0)\n","Building wheels for collected packages: spacy-wordnet, nltk\n","  Building wheel for spacy-wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spacy-wordnet: filename=spacy_wordnet-0.0.4-py2.py3-none-any.whl size=650293 sha256=12c64d9c9d838bc1ab64091d1ed2f6895e5b72ae5355afbd8b1eaab3d2d4247c\n","  Stored in directory: /root/.cache/pip/wheels/25/93/1d/c86db913cd146fc9ddb26d10f56579c5d58a3e00bc8f96a3a6\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394469 sha256=803b20a201b3846e4cc12f856cbf0b2f54364181b914b8194de3d5e9709760f8\n","  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n","Successfully built spacy-wordnet nltk\n","Installing collected packages: nltk, spacy-wordnet\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.3 spacy-wordnet-0.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:57.312343Z","start_time":"2019-11-11T11:45:56.822682Z"},"id":"eZO3w8qt-1ny","colab_type":"code","outputId":"2adff84b-1a35-467c-b2a8-5841c1b5c22d","executionInfo":{"status":"ok","timestamp":1575341229268,"user_tz":-60,"elapsed":22851,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n","import nltk\n","nltk.download('wordnet')\n","\n","nlp.add_pipe(WordnetAnnotator(nlp.lang)) #Register to the pipeline"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CKJgMBlu-1n5","colab_type":"text"},"source":["### Setting up a custom detector for intents\n","\n","As said, we will hijack the entity detector capability of SpaCy to classify intents.\n","\n","For this, we need to define custom rules with `EntityRuler`, and some patterns that match our intents.\n","\n","We have all in all 3 intents in mind:\n","\n","`INTENTS = [\"TAKEOUT_INTENT\",\"PAYIN_INTENT\",\"BALANCE_INTENT\"]`\n","\n","First define patterns **one for each**, register it, try to run the pipeline, and see the result.\n","\n","After it, you will have to **get back to this cell and iteratively refine the pattern** based on the results of WordNet enrichment below.\n","\n","First make it run through, then refine!\n","All in all 7 patterns are enough in total to detect the three intents in all their forms seen here with the help of WordNet synsets.\n","\n","#### Set up EntityRuler"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.608620Z","start_time":"2019-11-11T11:45:57.315205Z"},"id":"s5lX_BJU-1n6","colab_type":"code","colab":{}},"source":["from spacy.pipeline import EntityRuler\n","\n","INTENTS = [\"TAKEOUT_INTENT\",\"PAYIN_INTENT\",\"BALANCE_INTENT\"]\n","\n","ruler = EntityRuler(nlp, overwrite_ents=True)\n","\n","patterns = [{\"label\": INTENTS[0], \"pattern\": [{\"lower\": \"withdraw\"}]},\n","            {\"label\": INTENTS[0], \"pattern\": [{\"lower\": \"take\"},{\"lower\": \"out\"}]},\n","            {\"label\": INTENTS[1], \"pattern\": [{\"lower\": \"deposit\"}]},\n","            {\"label\": INTENTS[1], \"pattern\": [{\"lower\": \"put\",}, {\"lower\": \"in\"}]},\n","            {\"label\": INTENTS[1], \"pattern\": [{\"lower\": \"pay\"}, {\"lower\": \"in\"}]},\n","            {\"label\": INTENTS[2], \"pattern\": [{\"lower\": \"account\"}]},\n","            {\"label\": INTENTS[2], \"pattern\": [{\"lower\":\"balance\"}]},\n","            ]\n","# Add the patterns to the ruler\n","ruler.add_patterns(patterns)\n","# Add the ruler to the pipeline\n","nlp.add_pipe(ruler, after='tagger')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqBJ0mL4-1n_","colab_type":"text"},"source":["#### Define a `detect_intent` function\n","\n","The function takes in as an input an analysed sentence (`Doc`), a list on intents (eg. `INTENTS`), ad gives back the found intent or `None`."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.613673Z","start_time":"2019-11-11T11:45:58.610722Z"},"id":"S9N021f6-1oA","colab_type":"code","colab":{}},"source":["def detect_intent(analysed_sentence, intents):\n","    # In this case, we do not do proper intent detection,\n","    # which would be a whole sentence classification task, based on it's semantics,\n","    # but we do an intelligent entity matching based on our rules,\n","    # where we treat intents as special entities.\n","    \n","    found=None\n","    \n","    for word in analysed_sentence.ents:\n","      if word.label_ in intents:\n","        found = word.label_ \n","    \n","    return found"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O7Z6mbym-1oE","colab_type":"text"},"source":["### Setitng up a function for detecting \"real\" entities\n","\n","In SpaCy's world, monetary units and numbers are considered to be entities by default, thus the built in Named Entity Recognizer (`ner` in the pipeline) detects and tags those.\n","\n","In our case we are only interested in the monetary entities. **Please bear in mind that MULTIPLE categories can mean money, so some times normal numbers, sometimes formal money, etc. Use multiple numeric categories for detection!**\n","\n","More on this [here](https://spacy.io/usage/linguistic-features/#named-entities) and [here](https://spacy.io/api/annotation#named-entities)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.622131Z","start_time":"2019-11-11T11:45:58.614805Z"},"id":"_bg-GsQ4-1oG","colab_type":"code","colab":{}},"source":["MONEY = [\"MONEY\", \"CARDINAL\", \"QUANTITY\"]\n","\n","def detect_money(analysed_sentence, money):\n","    \n","    found_money = None\n","    only_numbers = None\n","    \n","    for item in analysed_sentence.ents:\n","      if item.label_ in money:\n","        found_money = item.label_\n","        only_numbers = item\n","    \n","    # Please return only the numbers from the money!!!\n","    \n","    return only_numbers "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVyzGYIG-1oM","colab_type":"text"},"source":["### Enriching intent detection with WordNet\n","\n","As we well saw, if we don't want to manually set up the patterns that match all test cases - which is unsustainable for a much bigger corpus than this - we need some semantic help.\n","\n","Let's define a super crude `enrich_sentence` function, that generates sentence variants from the input. It takes in an analysed sentence (`doc`), a set of domains (in our case eg. `ECONOMY_DOMAINS`), and **for each token in the sentence searches for the sysnonyms inside our domains, then replaces the token with it's synonym, and appends the new sentence to a list.**\n","\n","**Finally we expect to get back a set of sentence variants as texts in a list.**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.626815Z","start_time":"2019-11-11T11:45:58.623370Z"},"id":"SEqqgz0f-1oN","colab_type":"code","colab":{}},"source":["ECONOMY_DOMAINS = ['finance', 'banking']\n","\n","\n","def enrich_sentence(analysed_sentence, domains):\n","\n","    enriched_sentences = []\n","    \n","    for syn in analysed_sentence:\n","      synsets = syn._.wordnet.wordnet_synsets_for_domain(domains)\n","      if synsets:\n","        for s in synsets:\n","          for words in s.lemma_names():\n","            enriched_sentences.append(analysed_sentence.text.replace(syn.text,words.replace(\"_\",\" \")))\n","    return enriched_sentences"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5LH03Xp-1oS","colab_type":"text"},"source":["#### Full search for intents\n","\n","Based on the `detect_intent` and `enrich_sentence` functions we set up the full logic that searches for intents.\n","\n","The function has to accept an analysed sentence (`Doc`), the list of intents and the list of domains as above, and then **try to find the intent in the default sentence. If not found, try to enrich the sentence, then search in the enriched ones. Return an intent if found.**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.632646Z","start_time":"2019-11-11T11:45:58.628020Z"},"id":"JdprSvLW-1oU","colab_type":"code","colab":{}},"source":["def search_for_intents(analysed_sentence, intents, domains):\n","\n","    found_intent = None\n","    found_intent = detect_intent(analysed_sentence,intents)   \n","    if found_intent == None:      \n","      for sent in enrich_sentence(analysed_sentence,domains):\n","        found_intent = detect_intent(analysed_sentence,intents)\n","        print(found_intent)\n","    else:\n","      found_intent = found_intent     \n","    return found_intent"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxknQGsj-1oZ","colab_type":"text"},"source":["#### Let's try this out!"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.694647Z","start_time":"2019-11-11T11:45:58.634052Z"},"id":"JXnO6RI3-1oa","colab_type":"code","outputId":"1cfb87f8-2515-47f9-c6be-3b29d3588778","executionInfo":{"status":"ok","timestamp":1575341231076,"user_tz":-60,"elapsed":24637,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sentence = nlp(\"I would like to withdraw 5000 euros.\")\n","\n","found_intent = search_for_intents(sentence,INTENTS,ECONOMY_DOMAINS)\n","\n","print(found_intent)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TAKEOUT_INTENT\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qpyiu0Cp-1oe","colab_type":"text"},"source":["## Finally: parse the full query\n","\n","Refine the original patterns and all the functions until the tests pass at the end of the notebook. Use **the least amount of handmade patterns possible!**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.700317Z","start_time":"2019-11-11T11:45:58.696151Z"},"id":"NMcorgjs-1of","colab_type":"code","colab":{}},"source":["def parse_query(query, intents, domains, money):\n","    print(query)\n","    analysed_sentence = nlp(query)\n","    found_intent = search_for_intents(analysed_sentence,intents, domains)\n","    if found_intent == intents[0] or found_intent == intents[1]:\n","        amount = detect_money(analysed_sentence,money)\n","        if amount:\n","            print(\"Executing\",found_intent,\"with\",amount)\n","            return (found_intent, amount)\n","        else:\n","            print(\"No amount was given, please add one!\")\n","            return (found_intent, None)\n","    elif found_intent == intents[-1]:\n","        print(\"Getting you your account balance, one moment...\")\n","        return (found_intent, None)\n","    else:\n","        print(\"Can't parse what you are asking for, sorry!\")\n","        return (None, None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.757534Z","start_time":"2019-11-11T11:45:58.701701Z"},"id":"GzUN1ZQ3-1ok","colab_type":"code","outputId":"06addbe1-4f39-4186-8e41-b6d3d7b05bdc","executionInfo":{"status":"ok","timestamp":1575341231077,"user_tz":-60,"elapsed":24629,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["parse_query(\"I would like to withdraw 5000.\",INTENTS, ECONOMY_DOMAINS, MONEY)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["I would like to withdraw 5000.\n","Executing TAKEOUT_INTENT with 5000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('TAKEOUT_INTENT', 5000)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.763117Z","start_time":"2019-11-11T11:45:58.758974Z"},"id":"N0oqrhdt-1oq","colab_type":"code","colab":{}},"source":["tests = [\n","    (\"I would like to deposit 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n","    (\"I would like to put in 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n","    (\"I would like to pay in 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n","    (\"I would like to pay up 5000 EUR.\",(\"PAYIN_INTENT\",\"5000\")),\n","    (\"Can I pay in 5000 euros, please?\",(\"PAYIN_INTENT\",\"5000\")),\n","    \n","    \n","    (\"I would like to deposit money.\",(\"PAYIN_INTENT\",None)),\n","    \n","\n","    (\"I am about to take out 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"I am about to get out 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"I am about to withdraw 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"I want to withdraw 5000 USD.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"Can I withdraw $ 5000.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","\n","    \n","    (\"Can I check my account, please?\",(\"BALANCE_INTENT\",None)),\n","    (\"May I see my balance, please?\",(\"BALANCE_INTENT\",None)),\n","    (\"Could I query my account, please?\",(\"BALANCE_INTENT\",None)),\n","    (\"I would like to see my account balance.\",(\"BALANCE_INTENT\",None)),\n","\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJy-3CGvfDbT","colab_type":"code","outputId":"bfe8fed8-2cf9-4b93-b366-6d8b0c2f5288","executionInfo":{"status":"error","timestamp":1575341231283,"user_tz":-60,"elapsed":24824,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":307}},"source":["print(search_for_intents(tests[0],INTENTS,ECONOMY_DOMAINS))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-a94ce967f341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_for_intents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINTENTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mECONOMY_DOMAINS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-8fd0d83bbd85>\u001b[0m in \u001b[0;36msearch_for_intents\u001b[0;34m(analysed_sentence, intents, domains)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfound_intent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfound_intent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_intent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysed_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfound_intent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menrich_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysed_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-30da4464305e>\u001b[0m in \u001b[0;36mdetect_intent\u001b[0;34m(analysed_sentence, intents)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalysed_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ents'"]}]},{"cell_type":"code","metadata":{"id":"YQE6ds1Xg2sF","colab_type":"code","colab":{}},"source":["for test in tests:\n","  parse_query(test,INTENTS,ECONOMY_DOMAINS,MONEY)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"35PtClto3EyC","colab_type":"code","outputId":"44b0026b-4bb5-4beb-a51f-aadab0f1434b","executionInfo":{"status":"error","timestamp":1575341248124,"user_tz":-60,"elapsed":498,"user":{"displayName":"ananya neogi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaXX5hpTJBdR7jrnLKxnXHfnsCyNveb4DMLEkB-A=s64","userId":"00462567981712017677"}},"colab":{"base_uri":"https://localhost:8080/","height":324}},"source":["parse_query(tests[0],INTENTS, ECONOMY_DOMAINS, MONEY) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["('I would like to deposit 5000 euros.', ('PAYIN_INTENT', '5000'))\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-655e5d3d1725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparse_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINTENTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mECONOMY_DOMAINS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMONEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-f0ed78a0ad8e>\u001b[0m in \u001b[0;36mparse_query\u001b[0;34m(query, intents, domains, money)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoney\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0manalysed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfound_intent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_for_intents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysed_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfound_intent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfound_intent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             )\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got tuple)"]}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:59.303171Z","start_time":"2019-11-11T11:45:58.764596Z"},"id":"9A7xSjGf-1ou","colab_type":"code","colab":{}},"source":["for test in tests:\n","    try:\n","        assert parse_query(test[0],INTENTS, ECONOMY_DOMAINS, MONEY) == test[1]\n","    except:\n","        print(\"---ERROR: \",parse_query(test[0],INTENTS, ECONOMY_DOMAINS, MONEY))\n","        raise"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rV-Osftt-1ox","colab_type":"text"},"source":["A more elaborate and very nice example on the power of rule based matching and it's combination with machine learning models can be found [here](https://github.com/pmbaumgartner/binder-notebooks/blob/master/rule-based-matching-with-spacy-matcher.ipynb)"]}]}