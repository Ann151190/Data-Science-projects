{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"},"colab":{"name":"Copy of Sp√∂rer Assignment 4 RL-day3-gridworld-handout.ipynb","provenance":[{"file_id":"1KUxjoiNPWmVw5cC7ykHqDSB87_Unu8c9","timestamp":1584708973198}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"4JW_cx85y57U","colab_type":"code","colab":{}},"source":["# importing the necessary libraries\n","from matplotlib import pyplot as plt\n","from matplotlib import cm\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMCmb24Vy57c","colab_type":"text"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=18q7KL4aV6McMtaid_1Let2aGkw6d4QYn\" width=45%>"]},{"cell_type":"code","metadata":{"id":"ZVhroS5Xy57f","colab_type":"code","colab":{}},"source":["class GridWorld:\n","    \"\"\"\n","    The gridwordls is a frequently used demo environment in reinforcement learning\n","    to try and test ideas.\n","    Today, we will use it to understand the concepts so far.\n","    \n","    The environment: (see image)\n","    * cells: the agent can step on a cell. There is exactly one cell to start from.\n","    This is the top left corner. There is one terminal cell where the walking ends, \n","    the agent can not leave it (blue).\n","    * obstacles: there are cells where the agent can not step. (gray)\n","    * agent: it can move from one cell to an other neighboring cell. \n","    Possible directions: up, down, left, right. Each transition happens with probability 1.\n","    * reward: after each transition the agent receives -1 point. In the terminal cell, no reward\n","    received anymore.\n","    \n","    Implement the environment below!\n","    \"\"\"\n","    def __init__(self, size, start_cell, obstacles, terminating_state):\n","        self.size = size\n","        self.start = start_cell\n","        self.obstacles = obstacles\n","        self.termin = terminating_state\n","        self.current_cell = self.start\n","    \n","    def reset(self):\n","        # ----- reset the current cell to the start cell to start again -----\n","        self.current_cell = self.start\n","\n","    def is_clear_from_obstacle(self, action):\n","        if self.current_cell[0] == 0 and action == 0: # 0 left\n","            return False\n","        if self.current_cell[1] == 0 and action == 1: # 1 up\n","            return False\n","        if self.current_cell[0] == self.size[1] and action == 2: # 2 right\n","            return False\n","        if self.current_cell[1] == self.size[0] and action == 3: # 3 down\n","            return False\n","        return True\n","    \n","    def transition(self, cell, action, boolean_prints=False):\n","        # ----- IMPLEMENT FUNCTION -----\n","        # cell = (row, column) indices\n","        # action: 0 left, 1 up, 2 right, 3 down\n","        if action == 0:\n","            current_cell = [cell[0], cell[1] - 1]\n","        elif action == 1:\n","            current_cell = [cell[0] - 1, cell[1]]\n","        elif action == 2:\n","            current_cell = [cell[0], cell[1] + 1]\n","        elif action == 3:\n","            current_cell = [cell[0] + 1, cell[1]]\n","        else:\n","            raise InputError('Allowed actions are; 0 (left), 1 (up), 2 (right), or 3 (down).')\n","\n","        if current_cell[0] < 0:\n","            if boolean_prints:\n","                print('Not allowed to cross the border to the left!')\n","            current_cell = cell\n","        elif current_cell[1] < 0:\n","            if boolean_prints:\n","                print('Not allowed to cross the border to the top!')\n","            current_cell = cell\n","        elif current_cell[0] > (size[1] - 1):\n","            if boolean_prints:\n","                print('Not allowed to cross the border to the right!')\n","            current_cell = cell\n","        elif current_cell[1] > (size[0] - 1):\n","            if boolean_prints:\n","                print('Not allowed to cross the border to the bottom!')\n","            current_cell = cell\n","        else:\n","            if boolean_prints:\n","                print(f'Moving from {cell} to {current_cell} with {action} is a valid move.')\n","\n","        for index, obstacle in enumerate(self.obstacles):\n","            if obstacle == (cell[0], cell[1]):\n","                if boolean_prints:\n","                    print(f'Not allowed to step on an obstacle! Obstacle index: {index}, obstacle position: {obstacle}')\n","                current_cell = cell\n","\n","        # returns: What will be the next state\n","        # Take care of the borders of the grid!\n","        \n","        r_next = current_cell[0]\n","        c_next = current_cell[1]\n","\n","        self.current_cell = (r_next, c_next)\n","\n","        return (r_next, c_next)\n","\n","    def reward(self, cell, action):\n","        # ----- RETURN REWARD -----\n","        # -1 if not in the terminal state\n","        self.transition(cell=cell, action=action)\n","        if self.in_terminal():\n","            self.current_cell = cell\n","            # print('Terminal state reached!')\n","            return 0\n","        else:\n","            self.current_cell = cell\n","            return -1\n","    \n","    def in_terminal(self):\n","        return self.current_cell == self.termin"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHGVPqK3y57l","colab_type":"code","colab":{}},"source":["class DPsolver:\n","    \"\"\"\n","    This solver is based on the Bellman-equation and it is \n","    solved by iteratively.\n","    The action-value is used to represent the utility of the \n","    actions and states.\n","    \"\"\"\n","    def __init__(self, gridworld, gamma_discount_factor, iterations):\n","        # setting parametes according to the input parameters\n","        self.gridworld = gridworld\n","        self.gamma_discount_factor = gamma_discount_factor\n","        self.iterations = iterations\n","        size = gridworld.size\n","        # initialize accumulaters\n","        self.cntr = 0\n","        self.sum_rewards = []\n","        self.path = []\n","        # ----- initialize the table for Q-function -----\n","        # A Q-table has states in the rows and actions in the columns\n","        self.q_table = np.zeros((4, size[0], size[1]))\n","\n","    def step(self, boolean_print=False):\n","        \"\"\"Bellman equation without action-dependent state-change probability.\n","        \"\"\"\n","        # ----- WRITE THE CODE BELOW -----\n","        # implement one step in the value iteration\n","        rows, columns = self.gridworld.size  # ask for the size of the grid\n","        # ----- cycle over the rows -----\n","        for row in range(0, rows):\n","            # ----- cycle over the columns -----\n","            for column in range(0, columns):\n","                # ----- cycle over the actions -----\n","                for action in range(0, 4):\n","                    # ----- get the reward -----\n","                    reward = self.gridworld.reward(cell=(row, column), action=action)\n","                    # ----- calculate the corresponding next step (what would happen) -----\n","                    cell_next = self.gridworld.transition(cell=(row, column), action=action)\n","                    r2, c2 = cell_next\n","                    self.q_table[action, row, column] = reward + self.gamma_discount_factor * max(\n","                        self.q_table[0, r2, c2],\n","                        self.q_table[1, r2, c2],\n","                        self.q_table[2, r2, c2],\n","                        self.q_table[3, r2, c2]\n","                    )\n","\n","        # Bellman-optimality equation for the Q function (see last equation in the 3rd theory notebook)\n","        # If you look closely that formula, you will see a sum over the possible next states. It is very important, you do not sum over those states.\n","\n","        # increase the counter\n","        self.cntr += 1\n","        # add the return to the sum_rewards list\n","        self.sum_rewards.append(self.trajectory())\n","\n","    def trajectory(self):\n","        # ----- IMPLEMENT THE FUNCTION -----\n","        # reset the gridworld\n","        self.gridworld.reset()\n","        \n","        # calculate the return along a trajectory followed by the current policy\n","        # when started from the start_cell\n","        sum_rewards = 0\n","    \n","        counter_for_trajectory = 0\n","        max_trajectory_iterations = self.iterations\n","        while not self.gridworld.in_terminal():\n","            best_action_index = np.argmax(a=self.q_table[:,self.gridworld.current_cell[0],self.gridworld.current_cell[1]])\n","            reward = self.gridworld.reward(cell=self.gridworld.current_cell, action=best_action_index)\n","            # ----- calculate the corresponding next step (what would happen) -----\n","            self.gridworld.transition(cell=self.gridworld.current_cell, action=best_action_index) \n","\n","            sum_rewards += reward\n","            counter_for_trajectory += 1\n","\n","            if counter_for_trajectory >= max_trajectory_iterations:\n","                break\n","\n","        return sum_rewards\n","\n","    def is_learning_finished(self):\n","        # ----- IMPLEMENT THIS FUNCTION -----\n","        # check whether learning has finished, return it\n","        if self.cntr >= self.iterations:\n","            return True\n","        else:\n","            return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLyiZjfyy57r","colab_type":"code","colab":{}},"source":["def plot_learning_curve(ql):\n","    values = ql.sum_rewards\n","    x = list(range(len(values)))\n","    y = values\n","    plt.plot(x, y, 'ro')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3lrjA48y57w","colab_type":"code","outputId":"faa25322-1fda-4960-f70c-8864624f0eb6","executionInfo":{"status":"ok","timestamp":1584698679598,"user_tz":-60,"elapsed":1051,"user":{"displayName":"Jan Sp√∂rer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRE0bEtV5kJqRFrW5PsBex4yJGWCUGPv7Uvx-PQw=s64","userId":"14656371184625008127"}},"colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["# grid world parameters\n","size = (6, 6)\n","start_cell = (0, 0)\n","obstacles = [(3, 3)]\n","terminating_state = (3, 5)\n","# q learning parameters\n","gamma = 0.9\n","# ----- What is the minimum required number of iterations? -----\n","iterations = 20\n","\n","gw = GridWorld(size, start_cell, obstacles, terminating_state)\n","solver = DPsolver(gw, gamma, iterations)\n","\n","while not solver.is_learning_finished():\n","    solver.step()\n","\n","plot_learning_curve(solver)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhpmyEtSwwx5SWrY/wNkaBAWkCdpnQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"lMV4-JwtOqKK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}