{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment5-Convolutional_Networks_on_CIFAR10_handout.ipynb","version":"0.3.2","provenance":[{"file_id":"1Yqaz07cYLYTb9rdKm8WwoGuD49xlmWxL","timestamp":1556969718410}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eSHhdguTHm0N"},"source":["# Task: CIFAR-10 classification\n","\n","The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n","\n","> \"consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n",">The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n","\n","<img src=\"https://corochann.com/wp-content/uploads/2017/04/cifar10_plot.png\">\n","\n","### Categories:\n","\n","- airplane \t\t\t\t\t\t\t\t\t\t\n","- automobile \t\t\t\t\t\t\t\t\t\t\n","- bird \t\t\t\t\t\t\t\t\t\t\n","- cat \t\t\t\t\t\t\t\t\t\t\n","- deer \t\t\t\t\t\t\t\t\t\t\n","- dog \t\t\t\t\t\t\t\t\t\t\n","- frog \t\t\t\t\t\t\t\t\t\t\n","- horse \t\t\t\t\t\t\t\t\t\t\n","- ship \t\t\t\t\t\t\t\t\t\t\n","- truck"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TIX2ehpiHm0S"},"source":["# Preliminaries"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:39:01.205778Z","start_time":"2019-05-01T08:39:00.058811Z"},"colab_type":"code","id":"sTT_MyHltNm4","colab":{}},"source":["import numpy as np\n","import os\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.regularizers import l1\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.initializers import glorot_normal\n","# from tensorboardcolab import TensorBoardColab \n","\n","# Fix seeds for (hopefully) reproducible results\n","from numpy.random import seed\n","seed(14)\n","from tensorflow import set_random_seed\n","set_random_seed(19)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7d9_yHMEtNnG"},"source":["Download the data if necessary and load it:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:39:36.264800Z","start_time":"2019-05-01T08:39:01.207234Z"},"colab_type":"code","id":"jFmPQL0xM_4p","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"506eb12c-7428-4c8f-c954-44e98539ec03","executionInfo":{"status":"ok","timestamp":1556986997372,"user_tz":-120,"elapsed":1591,"user":{"displayName":"ananya neogi","photoUrl":"https://lh5.googleusercontent.com/-oYos_5uEiio/AAAAAAAAAAI/AAAAAAAAM8U/bI2Bm0P0epw/s64/photo.jpg","userId":"00462567981712017677"}}},"source":["train, test = tf.keras.datasets.cifar10.load_data()\n","\n","train_images, train_labels = train\n","\n","valid_test_images, valid_test_labels = test\n","\n","train_images = train_images / 255.\n","\n","valid_test_images = valid_test_images / 255.\n","\n","valid_images = valid_test_images[:5000]\n","valid_labels = valid_test_labels[:5000]\n","test_images = valid_test_images[5000:]\n","test_labels = valid_test_labels[5000:]\n","\n","print(train_images.shape, valid_images.shape, test_images.shape)\n","print(train_labels.shape, valid_labels.shape, test_labels.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3) (5000, 32, 32, 3) (5000, 32, 32, 3)\n","(50000, 1) (5000, 1) (5000, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QxyeFqibHm05"},"source":["# Model\n","\n","## Parameters"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:39:36.268218Z","start_time":"2019-05-01T08:39:36.266265Z"},"colab_type":"code","id":"S075_4a6tNna","colab":{}},"source":["n_classes = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ACdo3SEitNnl"},"source":["## Network"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:39:36.404889Z","start_time":"2019-05-01T08:39:36.282271Z"},"colab_type":"code","id":"o8OgyCFaHm1H","colab":{"base_uri":"https://localhost:8080/","height":375},"outputId":"8d4ace30-8850-4105-cfe8-25b08c0af3b3","executionInfo":{"status":"error","timestamp":1556989423249,"user_tz":-120,"elapsed":577,"user":{"displayName":"ananya neogi","photoUrl":"https://lh5.googleusercontent.com/-oYos_5uEiio/AAAAAAAAAAI/AAAAAAAAM8U/bI2Bm0P0epw/s64/photo.jpg","userId":"00462567981712017677"}}},"source":["# adapted from https://github.com/jtopor/CUNY-MSDA-661/blob/master/CIFAR-CNN/TF-Layers-CIFAR-GITHUB-v3.py\n","\n","\n","tf.reset_default_graph() # It's good practice to clean and reset everything\n","clear_session            # even using Keras\n","\n","\n","# WE USE FUNCTIONAL API!\n","# (Could be different, but not now...)\n","\n","\n","\n","# Model\n","#######\n","\n","\n","# Define the input!\n","# Remember, we have pictures with 32x32 pixels and 3 color channels\n","# Disregard batch size, Keras will do that for us.\n","x = tf.placeholder(\"float\",[10,32,32,3])\n","\n","# Convolutional Layer #1: (batch_size, 32, 32, 3) -> (batch_size, 32, 32, 64)\n","# Define a \"normal\" convolutional layer for images (not a single sequence, so ?D)\n","# There should be 64 convolutional units\n","# The kernel should be 5 in width and heigth\n","# There should be padding so that the input and output dimensions would be equivalent\n","# The non-linearity should be ReLU\n","conv1 =  tf.keras.layers.Conv2D(\n","      inputs= x,\n","      filters=32,\n","      kernel_size=[5, 5],\n","      padding=\"same\",\n","      activation='relu')\n"," \n","# Pooling Layer #1: (batch_size, 32, 32, 64) -> (batch_size, 16, 16, 64)\n","# Define a maximum based pooling layer with appropriate dimensions\n","# The pooling size should be 2,2 and stride 2\n","pool1 = tf.keras.layers.MaxPool2D(inputs=conv1, pool_size=[2, 2], strides=2)\n","\n","# Define a dropout layer with using the first dropout rate parameter\n","dropout1 = tf.keras.layers.Dropout(inputs=pool1, rate=0.25, training=mode == learn.ModeKeys.TRAIN)\n","\n","# Convolutional Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 16, 16, 64)\n","# Repeat the prior conv layer\n","# Watch for the right input\n","conv2 = tf.keras.layers.Conv2D(\n","      inputs= conv1,\n","      filters=32,\n","      kernel_size=[5, 5],\n","      padding=\"same\",\n","      activation='relu')\n","  \n","# Pooling Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 8, 8, 64)\n","# Repeat the prior pooling layer\n","# Watch for the right input\n","pool2 = tf.keras.layers.MaxPool2D(inputs=conv2, pool_size=[2, 2], strides=2)\n","\n","# Define a dropout layer with using the FIRST dropout rate parameter\n","dropout2 = tf.keras.layers.Dropout(inputs=pool2, rate=0.25, training=mode == learn.ModeKeys.TRAIN)\n","\n","# Convert tensors into vectors: (batch_size, 8, 8, 64) -> (batch_size, 4096)\n","# Use a single KERAS function, NO numpy or reshape magic!\n","# Hint: the result is not 2D but \"flat\"\n","pool2_flat = tf.reshape(dropout2, [-1, 8 * 8 * 64])\n","\n","# Fully connected Layer #1: (batch_size, 4096)-> (batch_size, 512)\n","# Define a fully connected layer with 512 nodes and ReLU\n","dense1 = tf.keras.layers.Dense(inputs=pool2_flat, units=512, activation='relu')\n","\n","# Define a dropout layer with using the SECOND dropout rate parameter\n","dropout3 = tf.keras.layers.Dropout(inputs=dense1, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n","\n","# Dense Layer #1: (batch_size, 512)-> (batch_size, 256)\n","# Define a fully connected layer with 256 nodes and ReLU\n","dense2 = tf.keras.layers.Dense(inputs=dropout3, units=256, activation='relu')\n","\n","# Define a dropout layer with using the SECOND dropout rate parameter\n","dropout4 = tf.keras.layers.Dropout(inputs=dense2, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n","\n","# Logits layer: (batch_size, 256) -> (batch_size, 10)\n","# Define a fully connected layer with ??? nodes\n","# Think about it, what shape should the output be?\n","# What activation?\n","# Think about it: we are in a classification problem!\n","predictions = {\n","      \"classes\": tf.argmax(\n","          input=logits, axis=1),\n","      \"probabilities\": tf.nn.softmax(\n","          logits, name=\"softmax_tensor\")\n","  }\n","\n","# Full model\n","# Instantiate (initialize) the model with inputs and outputs\n","model = Model(inputs=x, outputs=predictions)\n","\n","model.summary()\n"],"execution_count":19,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-86da9ff3c5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       activation='relu')\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Pooling Layer #1: (batch_size, 32, 32, 64) -> (batch_size, 16, 16, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'inputs')"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LGKQ-8bmtNn1"},"source":["## Loss, optimization and compilation"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:39:36.473837Z","start_time":"2019-05-01T08:39:36.407595Z"},"colab_type":"code","id":"j1UmvSzLHm1R","colab":{}},"source":["# Loss \n","\n","loss = sparse_categorical_crossentropy # we use this cross entropy variant as the input is not \n","                                       # one-hot encoded\n","\n","# Optimizer\n","# Choose an optimizer - adaptive ones work well here\n","optimizer = \n"," \n","# Compilation\n","#############\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gkPe16fDtNoJ"},"source":["## Training"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:51:59.029061Z","start_time":"2019-05-01T08:39:36.475186Z"},"colab_type":"code","id":"zZ1d14lFtNoM","colab":{}},"source":["# tb=TensorBoard(log_dir='./Graph')\n","# tbc=TensorBoardColab(graph_path='./Graph')\n","\n","history = model.fit(x=train_images, y=train_labels,\n","                    validation_data=(valid_images, valid_labels),\n","                    epochs=epoch_count,\n","                    batch_size=batch_size) #, callbacks=[tb])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:56:28.958057Z","start_time":"2019-05-01T08:56:28.713031Z"},"colab_type":"code","id":"6GTGeChttNoi","colab":{}},"source":["from matplotlib import pyplot as plt\n","\n","def display_history(history):\n","    \"\"\"Summarize history for accuracy and loss.\n","    \"\"\"\n","    plt.plot(history.history['acc'])\n","    plt.plot(history.history['val_acc'])\n","    plt.title('Model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'valid'], loc='upper left')\n","    plt.show()\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'valid'], loc='upper left')\n","    plt.show()\n","    \n","display_history(history);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:52:07.228831Z","start_time":"2019-05-01T08:52:07.222761Z"},"id":"7ZcPR6BCoCal","colab_type":"code","colab":{}},"source":["assert max(history.history['val_acc']) > 0.75"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O7nD81aQtNo3"},"source":["## Saving the model"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-01T08:52:10.803767Z","start_time":"2019-05-01T08:52:10.663477Z"},"colab_type":"code","id":"XcFR3MkStNo5","colab":{}},"source":["model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"],"execution_count":0,"outputs":[]}]}